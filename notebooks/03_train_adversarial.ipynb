{"cells":[{"cell_type":"code","execution_count":null,"id":"c3ebde29","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c3ebde29","executionInfo":{"status":"ok","timestamp":1770386011603,"user_tz":300,"elapsed":35622,"user":{"displayName":"Zelalem Abahana","userId":"10078571423914522929"}},"outputId":"9d1e2bfc-9251-4170-9f47-c269e758124a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Imports and setup (needed when 02-06 run in separate kernel)\n","import sys\n","from pathlib import Path\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import random\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error, r2_score\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","warnings.filterwarnings('ignore')\n","# Repo root for src imports\n","try:\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=False)\n","except Exception:\n","    pass\n","def _find_repo_root():\n","    cwd = Path.cwd().resolve()\n","    for p in [Path('/content/drive/MyDrive/multihead-attention-robustness'),\n","              Path('/content/drive/My Drive/multihead-attention-robustness'),\n","              Path('/content/repo_run')]:\n","        if (p / 'src').exists():\n","            return p\n","    drive_root = Path('/content/drive')\n","    if drive_root.exists():\n","        for base in [drive_root / 'MyDrive', drive_root / 'My Drive', drive_root]:\n","            p = base / 'multihead-attention-robustness'\n","            if p.exists() and (p / 'src').exists():\n","                return p\n","    p = cwd\n","    for _ in range(10):\n","        if (p / 'src').exists():\n","            return p\n","        if p.parent == p:\n","            break\n","        p = p.parent\n","    return cwd.parent if cwd.name == 'notebooks' else cwd\n","repo_root = _find_repo_root()\n","sys.path.insert(0, str(repo_root))\n","from src.models.feature_token_transformer import FeatureTokenTransformer, SingleHeadTransformer\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","RANDOM_SEED = 42\n","random.seed(RANDOM_SEED)\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","# Preserve models/training_history from notebook 02 when run in pipeline (01→02→03)\n","if 'models' not in globals() or not isinstance(globals().get('models'), dict) or len(globals().get('models', {})) == 0:\n","    models = {}\n","if 'training_history' not in globals() or not isinstance(globals().get('training_history'), dict) or len(globals().get('training_history', {})) == 0:\n","    training_history = {}\n","TRAINING_CONFIG = {\n","    'ols': {}, 'ridge': {'alpha': 1.0},\n","    'mlp': {'hidden_dims': [128, 64], 'learning_rate': 0.001, 'batch_size': 64, 'epochs': 100, 'patience': 10},\n","    'transformer': {'d_model': 72, 'num_heads': 8, 'num_layers': 2, 'd_ff': 512, 'dropout': 0.1,\n","                   'learning_rate': 0.0001, 'batch_size': 32, 'epochs': 100, 'patience': 20}\n","}\n"]},{"cell_type":"code","execution_count":null,"id":"72155dba","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"72155dba","executionInfo":{"status":"ok","timestamp":1770386014708,"user_tz":300,"elapsed":3092,"user":{"displayName":"Zelalem Abahana","userId":"10078571423914522929"}},"outputId":"5a3f1a04-0473-46cc-aaa0-86717403a4ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded fresh data: train 18826, val 3408\n"]}],"source":["# Load fresh data from master_table.csv (standalone: each notebook pulls its own data)\n","data_path = repo_root / 'data' / 'cross_sectional' / 'master_table.csv'\n","df = pd.read_csv(data_path)\n","if 'date' in df.columns:\n","    df['date'] = pd.to_datetime(df['date'])\n","    df = df.set_index('date')\n","class CrossSectionalDataSplitter:\n","    def __init__(self, train_start='2005-01-01', train_end='2017-12-31', val_start='2018-01-01', val_end='2019-12-31'):\n","        self.train_start, self.train_end = train_start, train_end\n","        self.val_start, self.val_end = val_start, val_end\n","    def split(self, master_table):\n","        master_table = master_table.copy()\n","        master_table.index = pd.to_datetime(master_table.index)\n","        return {'train': master_table.loc[self.train_start:self.train_end], 'val': master_table.loc[self.val_start:self.val_end]}\n","    def prepare_features_labels(self, data):\n","        if data.empty:\n","            return pd.DataFrame(), pd.Series()\n","        numeric_data = data.select_dtypes(include=[np.number])\n","        if numeric_data.empty:\n","            return pd.DataFrame(), pd.Series()\n","        exclude_cols = ['mktcap', 'market_cap', 'date', 'year', 'month', 'ticker', 'permno', 'gvkey']\n","        target_cols = ['return', 'returns', 'ret', 'target', 'y', 'next_return', 'forward_return', 'ret_1', 'ret_1m', 'ret_12m', 'future_return', 'returns_1d']\n","        target_col = None\n","        for tc in target_cols:\n","            for col in numeric_data.columns:\n","                if tc.lower() in col.lower() and col.lower() not in [ec.lower() for ec in exclude_cols]:\n","                    target_col = col\n","                    break\n","            if target_col:\n","                break\n","        if target_col is None:\n","            potential = [c for c in numeric_data.columns if c.lower() not in [ec.lower() for ec in exclude_cols]]\n","            target_col = potential[-2] if len(potential) > 1 else (potential[-1] if potential else numeric_data.columns[-1])\n","        feature_cols = [c for c in numeric_data.columns if c != target_col and c.lower() not in [ec.lower() for ec in exclude_cols]]\n","        if not feature_cols:\n","            feature_cols = [c for c in numeric_data.columns if c != target_col]\n","        if not feature_cols:\n","            feature_cols = numeric_data.columns[:-1].tolist()\n","            target_col = numeric_data.columns[-1]\n","        return numeric_data[feature_cols], numeric_data[target_col]\n","splitter = CrossSectionalDataSplitter()\n","data_splits = splitter.split(df)\n","train_df, val_df = data_splits['train'], data_splits['val']\n","X_train_df, y_train = splitter.prepare_features_labels(train_df)\n","X_val_df, y_val = splitter.prepare_features_labels(val_df)\n","X_train = X_train_df.fillna(0).values.astype(np.float32)\n","y_train = y_train.fillna(0).values.astype(np.float32)\n","X_val = X_val_df.fillna(0).values.astype(np.float32)\n","y_val = y_val.fillna(0).values.astype(np.float32)\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_val_scaled = scaler.transform(X_val)\n","print(f'Loaded fresh data: train {X_train_scaled.shape[0]}, val {X_val_scaled.shape[0]}')\n"]},{"cell_type":"code","source":["\"\"\"\n","Train baseline models when models is empty (standalone notebook execution).\n","Run with: %run -i train_baseline_if_needed.py\n","Expects in namespace: X_train_scaled, y_train, X_val_scaled, y_val, device, TRAINING_CONFIG,\n","  RANDOM_SEED, FeatureTokenTransformer, SingleHeadTransformer, nn, torch, np,\n","  mean_squared_error, r2_score.\n","Populates: models, training_history.\n","\"\"\"\n","from sklearn.linear_model import LinearRegression, Ridge\n","\n","def train_baseline_models():\n","    global models, training_history\n","    models = {}\n","    training_history = {}\n","    _device = globals().get('device', 'cpu')\n","    _cfg = globals().get('TRAINING_CONFIG', {})\n","    _seed = globals().get('RANDOM_SEED', 42)\n","    tr_cfg = _cfg.get('transformer', {'d_model': 72, 'num_heads': 8, 'num_layers': 2, 'd_ff': 512,\n","        'dropout': 0.1, 'learning_rate': 0.0001, 'batch_size': 32, 'epochs': 100, 'patience': 20})\n","\n","    def _train_transformer(model, X_train, y_train, X_val, y_val):\n","        model = model.to(_device)\n","        criterion = nn.MSELoss()\n","        optimizer = torch.optim.Adam(model.parameters(), lr=tr_cfg['learning_rate'])\n","        X_t = torch.FloatTensor(X_train).to(_device)\n","        y_t = torch.FloatTensor(y_train).to(_device)\n","        X_v = torch.FloatTensor(X_val).to(_device)\n","        y_v = torch.FloatTensor(y_val).to(_device)\n","        nf = model.num_features if hasattr(model, 'num_features') else getattr(model.model, 'num_features', X_train.shape[1])\n","        if X_train.shape[1] != nf:\n","            if X_train.shape[1] < nf:\n","                pad_t = np.zeros((X_train.shape[0], nf - X_train.shape[1]))\n","                pad_v = np.zeros((X_val.shape[0], nf - X_val.shape[1]))\n","                X_t = torch.FloatTensor(np.hstack([X_train, pad_t])).to(_device)\n","                X_v = torch.FloatTensor(np.hstack([X_val, pad_v])).to(_device)\n","            else:\n","                X_t = torch.FloatTensor(X_train[:, :nf]).to(_device)\n","                X_v = torch.FloatTensor(X_val[:, :nf]).to(_device)\n","        bs = tr_cfg['batch_size']\n","        best = float('inf')\n","        pc = 0\n","        for ep in range(tr_cfg['epochs']):\n","            model.train()\n","            for i in range(0, len(X_t), bs):\n","                optimizer.zero_grad()\n","                out = model(X_t[i:i+bs])\n","                out = out[0] if isinstance(out, tuple) else out\n","                criterion(out.squeeze(), y_t[i:i+bs]).backward()\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","                optimizer.step()\n","            model.eval()\n","            with torch.no_grad():\n","                vp = model(X_v)\n","                vp = vp[0] if isinstance(vp, tuple) else vp\n","                vl = criterion(vp.squeeze(), y_v).item()\n","            if vl < best:\n","                best, pc = vl, 0\n","            else:\n","                pc += 1\n","                if pc >= tr_cfg['patience']:\n","                    break\n","        model.eval()\n","        with torch.no_grad():\n","            pred = model(X_v)\n","            pred = (pred[0] if isinstance(pred, tuple) else pred).squeeze().cpu().numpy()\n","        return pred\n","\n","    # OLS\n","    m = LinearRegression()\n","    m.fit(X_train_scaled, y_train)\n","    p = m.predict(X_val_scaled)\n","    models['OLS'] = m\n","    training_history['OLS'] = {'rmse': np.sqrt(mean_squared_error(y_val, p)), 'r2': r2_score(y_val, p)}\n","\n","    # Ridge\n","    m = Ridge(alpha=_cfg.get('ridge', {}).get('alpha', 1.0), random_state=_seed)\n","    m.fit(X_train_scaled, y_train)\n","    p = m.predict(X_val_scaled)\n","    models['Ridge'] = m\n","    training_history['Ridge'] = {'rmse': np.sqrt(mean_squared_error(y_val, p)), 'r2': r2_score(y_val, p)}\n","\n","    # XGBoost\n","    try:\n","        import xgboost as xgb\n","        m = xgb.XGBRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, subsample=0.8,\n","            colsample_bytree=0.8, random_state=_seed, objective='reg:squarederror', eval_metric='rmse')\n","        m.fit(X_train_scaled, y_train, eval_set=[(X_val_scaled, y_val)], verbose=False)\n","        p = m.predict(X_val_scaled)\n","        models['XGBoost'] = m\n","        training_history['XGBoost'] = {'rmse': np.sqrt(mean_squared_error(y_val, p)), 'r2': r2_score(y_val, p)}\n","    except ImportError:\n","        pass\n","\n","    # MLP\n","    class MLP(nn.Module):\n","        def __init__(self):\n","            super().__init__()\n","            h = _cfg.get('mlp', {}).get('hidden_dims', [128, 64])\n","            layers = []\n","            prev = X_train_scaled.shape[1]\n","            for d in h:\n","                layers += [nn.Linear(prev, d), nn.ReLU(), nn.Dropout(0.1)]\n","                prev = d\n","            layers.append(nn.Linear(prev, 1))\n","            self.net = nn.Sequential(*layers)\n","        def forward(self, x):\n","            return self.net(x).squeeze(-1)\n","    m = MLP().to(_device)\n","    opt = torch.optim.Adam(m.parameters(), lr=_cfg.get('mlp', {}).get('learning_rate', 0.001))\n","    Xt = torch.FloatTensor(X_train_scaled).to(_device)\n","    yt = torch.FloatTensor(y_train).to(_device)\n","    Xv = torch.FloatTensor(X_val_scaled).to(_device)\n","    for _ in range(min(_cfg.get('mlp', {}).get('epochs', 100), 50)):\n","        m.train()\n","        opt.zero_grad()\n","        torch.nn.functional.mse_loss(m(Xt), yt).backward()\n","        opt.step()\n","    m.eval()\n","    with torch.no_grad():\n","        p = m(Xv).cpu().numpy()\n","    models['MLP'] = m\n","    training_history['MLP'] = {'rmse': np.sqrt(mean_squared_error(y_val, p)), 'r2': r2_score(y_val, p)}\n","\n","    # Transformers\n","    nf = X_train_scaled.shape[1]\n","    for name, cls, kw in [\n","        ('Single-Head', SingleHeadTransformer, {'num_features': nf, 'd_model': tr_cfg['d_model'], 'num_layers': tr_cfg['num_layers']}),\n","        ('Multi-Head', FeatureTokenTransformer, {'num_features': nf, 'd_model': tr_cfg['d_model'], 'num_heads': tr_cfg['num_heads'],\n","            'num_layers': tr_cfg['num_layers'], 'd_ff': tr_cfg['d_ff'], 'dropout': tr_cfg['dropout'], 'use_head_diversity': False}),\n","        ('Multi-Head Diversity', FeatureTokenTransformer, {'num_features': nf, 'd_model': tr_cfg['d_model'], 'num_heads': tr_cfg['num_heads'],\n","            'num_layers': tr_cfg['num_layers'], 'd_ff': tr_cfg['d_ff'], 'dropout': tr_cfg['dropout'],\n","            'use_head_diversity': True, 'diversity_weight': 0.01}),\n","    ]:\n","        mdl = cls(**kw)\n","        pred = _train_transformer(mdl, X_train_scaled, y_train, X_val_scaled, y_val)\n","        models[name] = mdl\n","        training_history[name] = {'rmse': np.sqrt(mean_squared_error(y_val, pred)), 'r2': r2_score(y_val, pred)}\n","\n","    print(f\"✓ Trained {len(models)} baseline models (standalone mode)\")\n","\n","if __name__ == '__main__':\n","    g = globals()\n","    if 'models' not in g or not isinstance(g.get('models'), dict) or len(g.get('models', {})) == 0:\n","        train_baseline_models()\n","    else:\n","        print(\"✓ Baseline models already loaded, skipping training\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8uLXTeL90E-w","executionInfo":{"status":"ok","timestamp":1770386980753,"user_tz":300,"elapsed":966044,"user":{"displayName":"Zelalem Abahana","userId":"10078571423914522929"}},"outputId":"aef77600-a044-4259-efb0-9c5fe66410ff"},"id":"8uLXTeL90E-w","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✓ Trained 7 baseline models (standalone mode)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"P2qCwIdt0EvZ"},"id":"P2qCwIdt0EvZ","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2UY0ayIiyJdS","executionInfo":{"status":"ok","timestamp":1770386980758,"user_tz":300,"elapsed":2,"user":{"displayName":"Zelalem Abahana","userId":"10078571423914522929"}},"outputId":"b4a39183-85a6-4097-8197-c23c18abb386"},"outputs":[{"output_type":"stream","name":"stdout","text":[" Using 7 baseline models from prior run\n"]}],"source":["# Standalone: train baseline models if not yet loaded (run 02 or pipeline first)\n","if len(models) == 0:\n","    try:\n","        get_ipython().run_line_magic('run', '-i train_baseline_if_needed.py')\n","    except Exception as e:\n","        print(f\" Could not run train_baseline_if_needed: {e}. Run notebook 02 first.\")\n","else:\n","    print(f\" Using {len(models)} baseline models from prior run\")"],"id":"2UY0ayIiyJdS"},{"cell_type":"code","execution_count":null,"id":"bec831d4","metadata":{"id":"bec831d4"},"outputs":[],"source":["epochs = n_epochs = num_epochs = 100  # full training (standalone mode)\n"]},{"cell_type":"code","execution_count":null,"id":"3f3c3c03","metadata":{"execution":{"iopub.execute_input":"2026-01-23T05:33:30.417903Z","iopub.status.busy":"2026-01-23T05:33:30.417713Z","iopub.status.idle":"2026-01-23T05:33:30.422633Z","shell.execute_reply":"2026-01-23T05:33:30.422291Z"},"papermill":{"duration":0.016146,"end_time":"2026-01-23T05:33:30.423532","exception":false,"start_time":"2026-01-23T05:33:30.407386","status":"completed"},"tags":[],"id":"3f3c3c03"},"outputs":[],"source":["# Adversarial Attack Implementations (A1-A4)\n","def apply_a1_attack(X, epsilon=0.01):\n","    \"\"\"A1: Measurement Error - bounded perturbations.\"\"\"\n","    noise = np.random.normal(0, epsilon, X.shape)\n","    # Scale noise by feature standard deviation\n","    feature_std = np.std(X, axis=0, keepdims=True) + 1e-8\n","    noise = noise * feature_std\n","    return X + noise\n","\n","\n","def apply_a2_attack(X, missing_rate=0.1):\n","    \"\"\"A2: Missingness/Staleness - set random features to zero.\"\"\"\n","    X_adv = X.copy()\n","    n_samples, n_features = X.shape\n","    n_missing = int(n_features * missing_rate)\n","\n","    for i in range(n_samples):\n","        missing_indices = np.random.choice(n_features, n_missing, replace=False)\n","        X_adv[i, missing_indices] = 0.0\n","\n","    return X_adv\n","\n","\n","def apply_a3_attack(X, epsilon=0.01):\n","    \"\"\"A3: Rank Manipulation - cross-sectional perturbation preserving ranks.\"\"\"\n","    X_adv = X.copy()\n","    n_samples = X.shape[0]\n","\n","    # Add small random perturbation that preserves relative ordering\n","    for i in range(n_samples):\n","        perturbation = np.random.normal(0, epsilon, X.shape[1])\n","        # Scale by feature std to maintain relative magnitudes\n","        feature_std = np.std(X[i], axis=0) + 1e-8\n","        perturbation = perturbation * feature_std\n","        X_adv[i] = X[i] + perturbation\n","\n","    return X_adv\n","\n","\n","def apply_a4_attack(X, epsilon=1.0):\n","    \"\"\"A4: Regime Shift - distribution shift attack.\"\"\"\n","    # A4 simulates regime shift by scaling volatility\n","    # epsilon acts as volatility multiplier\n","    X_adv = X.copy()\n","    feature_std = np.std(X, axis=0, keepdims=True) + 1e-8\n","    # Generate noise with std = epsilon, then scale by feature std\n","    noise = np.random.normal(0, epsilon, X.shape) * feature_std\n","    X_adv = X + noise\n","    return X_adv\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"4ec15b76","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ec15b76","executionInfo":{"status":"ok","timestamp":1770386980773,"user_tz":300,"elapsed":6,"user":{"displayName":"Zelalem Abahana","userId":"10078571423914522929"}},"outputId":"96a4c992-073d-432e-a62d-33a2c6ff33cc"},"outputs":[{"output_type":"stream","name":"stdout","text":[" Bootstrap confidence interval functions loaded\n"]}],"source":["# Bootstrap Confidence Intervals for R² and Robustness Metrics\n","from scipy import stats\n","\n","def bootstrap_confidence_interval(data, n_bootstrap=1000, confidence=0.95, method='percentile'):\n","    \"\"\"\n","    Compute bootstrap confidence interval for a metric.\n","\n","    Parameters:\n","    -----------\n","    data : array-like\n","        Sample data\n","    n_bootstrap : int\n","        Number of bootstrap samples\n","    confidence : float\n","        Confidence level (e.g., 0.95 for 95% CI)\n","    method : str\n","        'percentile' or 'bca' (bias-corrected and accelerated)\n","\n","    Returns:\n","    --------\n","    mean : float\n","        Mean of the data\n","    std : float\n","        Standard error (standard deviation)\n","    ci_lower : float\n","        Lower bound of confidence interval\n","    ci_upper : float\n","        Upper bound of confidence interval\n","    \"\"\"\n","    data = np.array(data)\n","    n = len(data)\n","    alpha = 1 - confidence\n","\n","    # Bootstrap samples\n","    bootstrap_samples = []\n","    for _ in range(n_bootstrap):\n","        # Resample with replacement\n","        indices = np.random.choice(n, size=n, replace=True)\n","        bootstrap_samples.append(np.mean(data[indices]))\n","\n","    bootstrap_samples = np.array(bootstrap_samples)\n","\n","    # Compute statistics\n","    mean = np.mean(data)\n","    std = np.std(bootstrap_samples)  # Standard error\n","\n","    if method == 'percentile':\n","        ci_lower = np.percentile(bootstrap_samples, 100 * alpha / 2)\n","        ci_upper = np.percentile(bootstrap_samples, 100 * (1 - alpha / 2))\n","    else:  # bca method\n","        # Bias-corrected and accelerated bootstrap\n","        z0 = stats.norm.ppf(np.mean(bootstrap_samples < mean))\n","        # Acceleration (simplified - using jackknife)\n","        jackknife_means = []\n","        for i in range(n):\n","            jackknife_data = np.delete(data, i)\n","            jackknife_means.append(np.mean(jackknife_data))\n","        jackknife_means = np.array(jackknife_means)\n","        a = np.sum((np.mean(jackknife_means) - jackknife_means)**3) / (6 * np.sum((np.mean(jackknife_means) - jackknife_means)**2)**1.5)\n","\n","        # BCa adjustment\n","        z_alpha_lower = stats.norm.ppf(alpha / 2)\n","        z_alpha_upper = stats.norm.ppf(1 - alpha / 2)\n","        z_lower = z0 + (z0 + z_alpha_lower) / (1 - a * (z0 + z_alpha_lower))\n","        z_upper = z0 + (z0 + z_alpha_upper) / (1 - a * (z0 + z_alpha_upper))\n","\n","        ci_lower = np.percentile(bootstrap_samples, 100 * stats.norm.cdf(z_lower))\n","        ci_upper = np.percentile(bootstrap_samples, 100 * stats.norm.cdf(z_upper))\n","\n","    return {\n","        'mean': mean,\n","        'std': std,\n","        'ci_lower': ci_lower,\n","        'ci_upper': ci_upper,\n","        'confidence': confidence\n","    }\n","\n","def compute_r2_with_ci(y_true, y_pred, n_bootstrap=1000, confidence=0.95):\n","    \"\"\"\n","    Compute R² with bootstrap confidence interval.\n","\n","    Parameters:\n","    -----------\n","    y_true : array-like\n","        True target values\n","    y_pred : array-like\n","        Predicted values\n","    n_bootstrap : int\n","        Number of bootstrap samples\n","    confidence : float\n","        Confidence level\n","\n","    Returns:\n","    --------\n","    dict with r2, std, ci_lower, ci_upper\n","    \"\"\"\n","    y_true = np.array(y_true)\n","    y_pred = np.array(y_pred)\n","    n = len(y_true)\n","\n","    # Original R²\n","    r2_original = r2_score(y_true, y_pred)\n","\n","    # Bootstrap R² values\n","    r2_bootstrap = []\n","    for _ in range(n_bootstrap):\n","        indices = np.random.choice(n, size=n, replace=True)\n","        y_true_boot = y_true[indices]\n","        y_pred_boot = y_pred[indices]\n","        r2_boot = r2_score(y_true_boot, y_pred_boot)\n","        r2_bootstrap.append(r2_boot)\n","\n","    r2_bootstrap = np.array(r2_bootstrap)\n","\n","    # Compute statistics\n","    std = np.std(r2_bootstrap)\n","    alpha = 1 - confidence\n","    ci_lower = np.percentile(r2_bootstrap, 100 * alpha / 2)\n","    ci_upper = np.percentile(r2_bootstrap, 100 * (1 - alpha / 2))\n","\n","    return {\n","        'r2': r2_original,\n","        'std': std,\n","        'ci_lower': ci_lower,\n","        'ci_upper': ci_upper,\n","        'confidence': confidence\n","    }\n","\n","print(\" Bootstrap confidence interval functions loaded\")"]},{"cell_type":"code","execution_count":null,"id":"6022c36b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6022c36b","executionInfo":{"status":"ok","timestamp":1770386980785,"user_tz":300,"elapsed":11,"user":{"displayName":"Zelalem Abahana","userId":"10078571423914522929"}},"outputId":"7b83a51d-e2ba-4043-8a66-a968a364cbe8"},"outputs":[{"output_type":"stream","name":"stdout","text":[" Enhanced evaluation function with confidence intervals loaded\n"]}],"source":["# Enhanced evaluation function with confidence intervals\n","def evaluate_model_under_attack_with_ci(model, model_name, X_val, y_val, attack_type, epsilon,\n","                                        device='cpu', is_sklearn=False, num_runs=5, n_bootstrap=1000):\n","    \"\"\"\n","    Evaluate a model under a specific attack with bootstrap confidence intervals.\n","\n","    Returns:\n","    --------\n","    dict with metrics including confidence intervals for robustness\n","    \"\"\"\n","    # Set model to eval mode\n","    if not is_sklearn:\n","        model.eval()\n","        for module in model.modules():\n","            if isinstance(module, nn.Dropout):\n","                module.eval()\n","\n","    # Make clean predictions\n","    if is_sklearn:\n","        y_pred_clean = model.predict(X_val)\n","    else:\n","        with torch.no_grad():\n","            X_tensor = torch.FloatTensor(X_val).to(device)\n","            output = model(X_tensor)\n","            if isinstance(output, tuple):\n","                y_pred_tensor = output[0]\n","            else:\n","                y_pred_tensor = output\n","            y_pred_clean = y_pred_tensor.cpu().numpy().flatten()\n","\n","    # Calculate clean RMSE and R² with CI\n","    clean_rmse = np.sqrt(mean_squared_error(y_val, y_pred_clean))\n","    r2_stats = compute_r2_with_ci(y_val, y_pred_clean, n_bootstrap=n_bootstrap)\n","\n","    # Run attack multiple times and collect robustness values\n","    robustness_values = []\n","    adv_rmses = []\n","\n","    for run in range(num_runs):\n","        # Apply attack\n","        if attack_type == 'a1':\n","            X_adv = apply_a1_attack(X_val, epsilon=epsilon)\n","        elif attack_type == 'a2':\n","            missing_rate = min(epsilon / 10.0, 0.8)\n","            X_adv = apply_a2_attack(X_val, missing_rate=missing_rate)\n","        elif attack_type == 'a3':\n","            X_adv = apply_a3_attack(X_val, epsilon=epsilon)\n","        elif attack_type == 'a4':\n","            X_adv = apply_a4_attack(X_val, epsilon=epsilon)\n","        else:\n","            X_adv = X_val.copy()\n","\n","        # Make adversarial predictions\n","        if is_sklearn:\n","            y_pred_adv = model.predict(X_adv)\n","        else:\n","            with torch.no_grad():\n","                X_adv_tensor = torch.FloatTensor(X_adv).to(device)\n","                output_adv = model(X_adv_tensor)\n","                if isinstance(output_adv, tuple):\n","                    y_pred_adv_tensor = output_adv[0]\n","                else:\n","                    y_pred_adv_tensor = output_adv\n","                y_pred_adv = y_pred_adv_tensor.cpu().numpy().flatten()\n","\n","        # Calculate adversarial RMSE\n","        adv_rmse = np.sqrt(mean_squared_error(y_val, y_pred_adv))\n","        adv_rmses.append(adv_rmse)\n","\n","        # Calculate robustness for this run\n","        delta_rmse = adv_rmse - clean_rmse\n","        if clean_rmse > 0:\n","            robustness = min(1.0, 1.0 - (delta_rmse / clean_rmse))\n","        else:\n","            robustness = 1.0\n","        robustness_values.append(robustness)\n","\n","    # Average across runs\n","    avg_adv_rmse = np.mean(adv_rmses)\n","    delta_rmse = avg_adv_rmse - clean_rmse\n","    avg_robustness = np.mean(robustness_values)\n","\n","    # Compute robustness confidence interval\n","    robustness_ci = bootstrap_confidence_interval(\n","        robustness_values, n_bootstrap=n_bootstrap, confidence=0.95\n","    )\n","\n","    return {\n","        'clean_rmse': clean_rmse,\n","        'adv_rmse': avg_adv_rmse,\n","        'delta_rmse': delta_rmse,\n","        'robustness': avg_robustness,\n","        'robustness_std': robustness_ci['std'],\n","        'robustness_ci_lower': robustness_ci['ci_lower'],\n","        'robustness_ci_upper': robustness_ci['ci_upper'],\n","        'r2': r2_stats['r2'],\n","        'r2_std': r2_stats['std'],\n","        'r2_ci_lower': r2_stats['ci_lower'],\n","        'r2_ci_upper': r2_stats['ci_upper']\n","    }\n","\n","print(\" Enhanced evaluation function with confidence intervals loaded\")"]},{"cell_type":"code","execution_count":null,"id":"e8b25cd7","metadata":{"execution":{"iopub.execute_input":"2026-01-23T05:33:30.444111Z","iopub.status.busy":"2026-01-23T05:33:30.443940Z","iopub.status.idle":"2026-01-23T05:33:30.446664Z","shell.execute_reply":"2026-01-23T05:33:30.446312Z"},"papermill":{"duration":0.014163,"end_time":"2026-01-23T05:33:30.447477","exception":false,"start_time":"2026-01-23T05:33:30.433314","status":"completed"},"tags":[],"id":"e8b25cd7"},"outputs":[],"source":["# Adversarial Training Configuration\n","ADVERSARIAL_CONFIG = {\n","    'epsilons': [0.25, 0.5, 1.0],  # Attack strengths\n","    'attacks': ['a1', 'a2', 'a3', 'a4'],  # Attack types\n","    'robust_weight': 0.3,  # Weight for adversarial loss (0.3 = 30% adversarial, 70% clean)\n","    'learning_rate': 0.0001,\n","    'batch_size': 32,\n","    'epochs': 100,\n","    'patience': 20,\n","    'warmup_epochs': 5  # Gradually increase adversarial weight\n","}\n","\n","# Store adversarially trained models\n","adversarial_models = {}\n","adversarial_training_history = {}"]},{"cell_type":"code","execution_count":null,"id":"903542c1","metadata":{"execution":{"iopub.execute_input":"2026-01-23T05:33:30.476286Z","iopub.status.busy":"2026-01-23T05:33:30.476125Z","iopub.status.idle":"2026-01-23T05:33:30.482515Z","shell.execute_reply":"2026-01-23T05:33:30.482162Z"},"papermill":{"duration":0.018077,"end_time":"2026-01-23T05:33:30.483669","exception":false,"start_time":"2026-01-23T05:33:30.465592","status":"completed"},"tags":[],"id":"903542c1"},"outputs":[],"source":["def adversarial_training_step(model, X_batch, y_batch, attack_type, epsilon,\n","                             optimizer, device='cpu', robust_weight=0.3):\n","    \"\"\"\n","    Perform one adversarial training step.\n","\n","    Args:\n","        model: The model to train\n","        X_batch: Input batch (numpy array)\n","        y_batch: Target batch (numpy array)\n","        attack_type: 'a1', 'a2', 'a3', or 'a4'\n","        epsilon: Attack strength\n","        optimizer: Optimizer\n","        device: Device to use\n","        robust_weight: Weight for adversarial loss\n","\n","    Returns:\n","        Dictionary with loss values or None if batch is invalid\n","    \"\"\"\n","    model.train()\n","    optimizer.zero_grad()\n","\n","    # Convert to tensors\n","    X_tensor = torch.FloatTensor(X_batch).to(device)\n","    y_tensor = torch.FloatTensor(y_batch).to(device)\n","\n","    # Clean forward pass\n","    output_clean = model(X_tensor)\n","    if isinstance(output_clean, tuple):\n","        y_pred_clean = output_clean[0]\n","    else:\n","        y_pred_clean = output_clean\n","\n","    # Check for NaN/Inf in predictions\n","    if torch.any(torch.isnan(y_pred_clean)) or torch.any(torch.isinf(y_pred_clean)):\n","        return None\n","\n","    clean_loss = nn.MSELoss()(y_pred_clean.squeeze(), y_tensor)\n","\n","    # Check if clean_loss is valid\n","    if torch.isnan(clean_loss) or torch.isinf(clean_loss):\n","        return None\n","\n","    # Generate adversarial examples\n","    if attack_type == 'a1':\n","        X_adv = apply_a1_attack(X_batch, epsilon=epsilon)\n","    elif attack_type == 'a2':\n","        # For A2, epsilon controls missing rate\n","        missing_rate = min(epsilon / 10.0, 0.8)  # Convert epsilon to missing rate\n","        X_adv = apply_a2_attack(X_batch, missing_rate=missing_rate)\n","    elif attack_type == 'a3':\n","        X_adv = apply_a3_attack(X_batch, epsilon=epsilon)\n","    elif attack_type == 'a4':\n","        X_adv = apply_a4_attack(X_batch, epsilon=epsilon)\n","    else:\n","        raise ValueError(f\"Unknown attack type: {attack_type}\")\n","\n","    # Adversarial forward pass\n","    X_adv_tensor = torch.FloatTensor(X_adv).to(device)\n","    output_adv = model(X_adv_tensor)\n","    if isinstance(output_adv, tuple):\n","        y_pred_adv = output_adv[0]\n","    else:\n","        y_pred_adv = output_adv\n","\n","    # Check for NaN/Inf in adversarial predictions\n","    if torch.any(torch.isnan(y_pred_adv)) or torch.any(torch.isinf(y_pred_adv)):\n","        return None\n","\n","    adv_loss = nn.MSELoss()(y_pred_adv.squeeze(), y_tensor)\n","\n","    # Check if adv_loss is valid\n","    if torch.isnan(adv_loss) or torch.isinf(adv_loss):\n","        return None\n","\n","    # Combined loss\n","    total_loss = (1 - robust_weight) * clean_loss + robust_weight * adv_loss\n","\n","    # Check if total_loss is valid before backward pass\n","    if torch.isnan(total_loss) or torch.isinf(total_loss):\n","        return None\n","\n","    # Ensure total_loss requires gradients\n","    if not total_loss.requires_grad:\n","        return None\n","\n","    # Backward pass with error handling\n","    try:\n","        total_loss.backward()\n","    except RuntimeError as e:\n","        if \"does not require grad\" in str(e) or \"does not have a grad_fn\" in str(e):\n","            optimizer.zero_grad()\n","            return None\n","        else:\n","            raise\n","\n","    # Gradient clipping to prevent exploding gradients\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","    optimizer.step()\n","\n","    return {\n","        'clean_loss': clean_loss.item(),\n","        'adversarial_loss': adv_loss.item(),\n","        'total_loss': total_loss.item()\n","    }"]},{"cell_type":"code","execution_count":null,"id":"c4b3e578","metadata":{"execution":{"iopub.execute_input":"2026-01-23T05:33:30.505878Z","iopub.status.busy":"2026-01-23T05:33:30.505718Z","iopub.status.idle":"2026-01-23T05:33:30.517488Z","shell.execute_reply":"2026-01-23T05:33:30.517151Z"},"papermill":{"duration":0.024209,"end_time":"2026-01-23T05:33:30.518558","exception":false,"start_time":"2026-01-23T05:33:30.494349","status":"completed"},"tags":[],"id":"c4b3e578"},"outputs":[],"source":["def train_adversarial_model(model, model_name, X_train, y_train, X_val, y_val,\n","                           attack_type, epsilon, config, device='cpu'):\n","    \"\"\"\n","    Train model with adversarial training.\n","\n","    Args:\n","        model: Model to train (will be copied)\n","        model_name: Name of the model\n","        X_train: Training features\n","        y_train: Training targets\n","        X_val: Validation features\n","        y_val: Validation targets\n","        attack_type: 'a1', 'a2', 'a3', or 'a4'\n","        epsilon: Attack strength\n","        config: Training configuration\n","        device: Device to use\n","\n","    Returns:\n","        Trained model, predictions, and training history\n","    \"\"\"\n","    # Create a fresh copy of the model for adversarial training\n","    import copy\n","    model = copy.deepcopy(model)\n","    model = model.to(device)\n","    model.train()\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","        optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-6\n","    )\n","\n","    # Convert to tensors\n","    X_train_tensor = torch.FloatTensor(X_train).to(device)\n","    y_train_tensor = torch.FloatTensor(y_train).to(device)\n","    X_val_tensor = torch.FloatTensor(X_val).to(device)\n","    y_val_tensor = torch.FloatTensor(y_val).to(device)\n","\n","    # Handle feature dimension mismatch\n","    num_features = model.num_features if hasattr(model, 'num_features') else model.model.num_features\n","\n","    if X_train.shape[1] != num_features:\n","        if X_train.shape[1] < num_features:\n","            # Pad\n","            padding_train = np.zeros((X_train.shape[0], num_features - X_train.shape[1]))\n","            padding_val = np.zeros((X_val.shape[0], num_features - X_val.shape[1]))\n","            X_train_tensor = torch.FloatTensor(np.hstack([X_train, padding_train])).to(device)\n","            X_val_tensor = torch.FloatTensor(np.hstack([X_val, padding_val])).to(device)\n","        else:\n","            # Truncate\n","            X_train_tensor = torch.FloatTensor(X_train[:, :num_features]).to(device)\n","            X_val_tensor = torch.FloatTensor(X_val[:, :num_features]).to(device)\n","\n","    history = {\n","        'train_loss': [],\n","        'val_loss': [],\n","        'train_clean_loss': [],\n","        'train_adv_loss': []\n","    }\n","\n","    best_val_loss = float('inf')\n","    patience_counter = 0\n","    warmup_epochs = config.get('warmup_epochs', 5)\n","\n","    batch_size = config['batch_size']\n","    n_batches = (len(X_train_tensor) + batch_size - 1) // batch_size\n","\n","    for epoch in range(config['epochs']):\n","        # Gradual warmup: increase robust_weight from 0.1 to target value\n","        if epoch < warmup_epochs:\n","            current_robust_weight = 0.1 + (config['robust_weight'] - 0.1) * (epoch / warmup_epochs)\n","        else:\n","            current_robust_weight = config['robust_weight']\n","\n","        epoch_losses = {'clean': [], 'adv': [], 'total': []}\n","\n","        # Training\n","        model.train()\n","        for i in range(0, len(X_train_tensor), batch_size):\n","            batch_X = X_train_tensor[i:i+batch_size].cpu().numpy()\n","            batch_y = y_train_tensor[i:i+batch_size].cpu().numpy()\n","\n","            losses = adversarial_training_step(\n","                model, batch_X, batch_y, attack_type, epsilon,\n","                optimizer, device, current_robust_weight\n","            )\n","\n","            # Skip batch if None (invalid batch)\n","            if losses is None:\n","                continue\n","\n","            # Check for NaN/Inf in losses\n","            if (np.isnan(losses['total_loss']) or np.isinf(losses['total_loss']) or\n","                np.isnan(losses['clean_loss']) or np.isinf(losses['clean_loss']) or\n","                np.isnan(losses['adversarial_loss']) or np.isinf(losses['adversarial_loss'])):\n","                continue\n","\n","            epoch_losses['clean'].append(losses['clean_loss'])\n","            epoch_losses['adv'].append(losses['adversarial_loss'])\n","            epoch_losses['total'].append(losses['total_loss'])\n","\n","        # Skip epoch if all losses are invalid\n","        if len(epoch_losses['total']) == 0:\n","            continue\n","\n","        # Validation\n","        model.eval()\n","        with torch.no_grad():\n","            output_val = model(X_val_tensor)\n","            if isinstance(output_val, tuple):\n","                y_pred_val = output_val[0]\n","            else:\n","                y_pred_val = output_val\n","\n","            # Check for constant predictions (model collapse detection)\n","            y_pred_np = y_pred_val.squeeze().cpu().numpy()\n","            pred_std = np.std(y_pred_np)\n","\n","            if pred_std < 1e-8:\n","                print(f\"   MODEL COLLAPSE DETECTED at epoch {epoch+1}!\")\n","                break\n","\n","            val_loss = nn.MSELoss()(y_pred_val.squeeze(), y_val_tensor).item()\n","\n","            # Check for NaN/Inf in validation loss\n","            if np.isnan(val_loss) or np.isinf(val_loss):\n","                val_loss = float('inf')\n","\n","        # Record history\n","        avg_train_loss = np.mean(epoch_losses['total']) if epoch_losses['total'] else float('inf')\n","        avg_clean_loss = np.mean(epoch_losses['clean']) if epoch_losses['clean'] else 0.0\n","        avg_adv_loss = np.mean(epoch_losses['adv']) if epoch_losses['adv'] else 0.0\n","\n","        history['train_loss'].append(avg_train_loss)\n","        history['val_loss'].append(val_loss)\n","        history['train_clean_loss'].append(avg_clean_loss)\n","        history['train_adv_loss'].append(avg_adv_loss)\n","\n","        # Learning rate scheduling\n","        scheduler.step(val_loss)\n","\n","        # Early stopping\n","        if not (np.isnan(val_loss) or np.isinf(val_loss)):\n","            if val_loss < best_val_loss:\n","                best_val_loss = val_loss\n","                patience_counter = 0\n","            else:\n","                patience_counter += 1\n","                if patience_counter >= config['patience']:\n","                    print(f\"  {model_name} ({attack_type.upper()}, ε={epsilon}): Early stopping at epoch {epoch+1}\")\n","                    break\n","\n","        if (epoch + 1) % 10 == 0:\n","            print(f\"  {model_name} ({attack_type.upper()}, ε={epsilon}) - Epoch {epoch+1}/{config['epochs']}: \"\n","                  f\"Train Loss={avg_train_loss:.6f}, Val Loss={val_loss:.6f}, \"\n","                  f\"Robust Weight={current_robust_weight:.3f}\")\n","\n","    # Final evaluation\n","    model.eval()\n","    with torch.no_grad():\n","        final_pred = model(X_val_tensor)\n","        if isinstance(final_pred, tuple):\n","            final_pred = final_pred[0]\n","        final_pred = final_pred.squeeze().cpu().numpy()\n","\n","    return model, final_pred, history"]},{"cell_type":"code","execution_count":null,"id":"1148cf2c","metadata":{"execution":{"iopub.execute_input":"2026-01-23T05:33:30.539172Z","iopub.status.busy":"2026-01-23T05:33:30.538998Z","iopub.status.idle":"2026-01-23T08:18:23.366050Z","shell.execute_reply":"2026-01-23T08:18:23.365641Z"},"papermill":{"duration":9892.855388,"end_time":"2026-01-23T08:18:23.384034","exception":false,"start_time":"2026-01-23T05:33:30.528646","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"1148cf2c","executionInfo":{"status":"ok","timestamp":1770395723090,"user_tz":300,"elapsed":8742261,"user":{"displayName":"Zelalem Abahana","userId":"10078571423914522929"}},"outputId":"6185c656-ee58-4cab-a95a-6c09aec6e9ea"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","ADVERSARIAL TRAINING FOR TRANSFORMER MODELS\n","================================================================================\n","Training on attacks: ['a1', 'a2', 'a3', 'a4']\n","Epsilons: [0.25, 0.5, 1.0]\n","Robust weight: 0.3\n","\n","\n","================================================================================\n","Training Single-Head with Adversarial Training\n","================================================================================\n","\n","Training Single-Head (A1, ε=0.25)...\n","  Single-Head (A1, ε=0.25) - Epoch 10/100: Train Loss=0.000420, Val Loss=0.000268, Robust Weight=0.300\n","  Single-Head (A1, ε=0.25) - Epoch 20/100: Train Loss=0.000381, Val Loss=0.000274, Robust Weight=0.300\n","  Single-Head (A1, ε=0.25): Early stopping at epoch 26\n","  Single-Head (A1, ε=0.25) trained - RMSE: 0.016570, R²: 0.098650\n","\n","Training Single-Head (A1, ε=0.5)...\n","  Single-Head (A1, ε=0.5) - Epoch 10/100: Train Loss=0.000432, Val Loss=0.000270, Robust Weight=0.300\n","  Single-Head (A1, ε=0.5) - Epoch 20/100: Train Loss=0.000402, Val Loss=0.000271, Robust Weight=0.300\n","  Single-Head (A1, ε=0.5): Early stopping at epoch 24\n","  Single-Head (A1, ε=0.5) trained - RMSE: 0.016513, R²: 0.104793\n","\n","Training Single-Head (A1, ε=1.0)...\n","  Single-Head (A1, ε=1.0) - Epoch 10/100: Train Loss=0.000456, Val Loss=0.000268, Robust Weight=0.300\n","  Single-Head (A1, ε=1.0) - Epoch 20/100: Train Loss=0.000438, Val Loss=0.000272, Robust Weight=0.300\n","  Single-Head (A1, ε=1.0) - Epoch 30/100: Train Loss=0.000418, Val Loss=0.000272, Robust Weight=0.300\n","  Single-Head (A1, ε=1.0): Early stopping at epoch 33\n","  Single-Head (A1, ε=1.0) trained - RMSE: 0.016596, R²: 0.095763\n","\n","Training Single-Head (A2, ε=0.25)...\n","  Single-Head (A2, ε=0.25) - Epoch 10/100: Train Loss=0.000393, Val Loss=0.000269, Robust Weight=0.300\n","  Single-Head (A2, ε=0.25) - Epoch 20/100: Train Loss=0.000391, Val Loss=0.000275, Robust Weight=0.300\n","  Single-Head (A2, ε=0.25) - Epoch 30/100: Train Loss=0.000359, Val Loss=0.000283, Robust Weight=0.300\n","  Single-Head (A2, ε=0.25): Early stopping at epoch 34\n","  Single-Head (A2, ε=0.25) trained - RMSE: 0.016894, R²: 0.062988\n","\n","Training Single-Head (A2, ε=0.5)...\n","  Single-Head (A2, ε=0.5) - Epoch 10/100: Train Loss=0.000403, Val Loss=0.000283, Robust Weight=0.300\n","  Single-Head (A2, ε=0.5) - Epoch 20/100: Train Loss=0.000388, Val Loss=0.000282, Robust Weight=0.300\n","  Single-Head (A2, ε=0.5): Early stopping at epoch 22\n","  Single-Head (A2, ε=0.5) trained - RMSE: 0.016863, R²: 0.066455\n","\n","Training Single-Head (A2, ε=1.0)...\n","  Single-Head (A2, ε=1.0) - Epoch 10/100: Train Loss=0.000430, Val Loss=0.000280, Robust Weight=0.300\n","  Single-Head (A2, ε=1.0) - Epoch 20/100: Train Loss=0.000393, Val Loss=0.000279, Robust Weight=0.300\n","  Single-Head (A2, ε=1.0): Early stopping at epoch 25\n","  Single-Head (A2, ε=1.0) trained - RMSE: 0.016717, R²: 0.082535\n","\n","Training Single-Head (A3, ε=0.25)...\n","  Single-Head (A3, ε=0.25) - Epoch 10/100: Train Loss=0.000426, Val Loss=0.000272, Robust Weight=0.300\n","  Single-Head (A3, ε=0.25) - Epoch 20/100: Train Loss=0.000406, Val Loss=0.000276, Robust Weight=0.300\n","  Single-Head (A3, ε=0.25): Early stopping at epoch 25\n","  Single-Head (A3, ε=0.25) trained - RMSE: 0.016630, R²: 0.092145\n","\n","Training Single-Head (A3, ε=0.5)...\n","  Single-Head (A3, ε=0.5) - Epoch 10/100: Train Loss=0.000447, Val Loss=0.000272, Robust Weight=0.300\n","  Single-Head (A3, ε=0.5) - Epoch 20/100: Train Loss=0.000419, Val Loss=0.000277, Robust Weight=0.300\n","  Single-Head (A3, ε=0.5): Early stopping at epoch 21\n","  Single-Head (A3, ε=0.5) trained - RMSE: 0.016624, R²: 0.092769\n","\n","Training Single-Head (A3, ε=1.0)...\n","  Single-Head (A3, ε=1.0) - Epoch 10/100: Train Loss=0.000477, Val Loss=0.000270, Robust Weight=0.300\n","  Single-Head (A3, ε=1.0) - Epoch 20/100: Train Loss=0.000446, Val Loss=0.000273, Robust Weight=0.300\n","  Single-Head (A3, ε=1.0): Early stopping at epoch 29\n","  Single-Head (A3, ε=1.0) trained - RMSE: 0.016575, R²: 0.098135\n","\n","Training Single-Head (A4, ε=0.25)...\n","  Single-Head (A4, ε=0.25) - Epoch 10/100: Train Loss=0.000399, Val Loss=0.000271, Robust Weight=0.300\n","  Single-Head (A4, ε=0.25) - Epoch 20/100: Train Loss=0.000381, Val Loss=0.000278, Robust Weight=0.300\n","  Single-Head (A4, ε=0.25): Early stopping at epoch 23\n","  Single-Head (A4, ε=0.25) trained - RMSE: 0.016773, R²: 0.076393\n","\n","Training Single-Head (A4, ε=0.5)...\n","  Single-Head (A4, ε=0.5) - Epoch 10/100: Train Loss=0.000430, Val Loss=0.000264, Robust Weight=0.300\n","  Single-Head (A4, ε=0.5) - Epoch 20/100: Train Loss=0.000416, Val Loss=0.000262, Robust Weight=0.300\n","  Single-Head (A4, ε=0.5) - Epoch 30/100: Train Loss=0.000397, Val Loss=0.000268, Robust Weight=0.300\n","  Single-Head (A4, ε=0.5): Early stopping at epoch 37\n","  Single-Head (A4, ε=0.5) trained - RMSE: 0.016447, R²: 0.111984\n","\n","Training Single-Head (A4, ε=1.0)...\n","  Single-Head (A4, ε=1.0) - Epoch 10/100: Train Loss=0.000451, Val Loss=0.000271, Robust Weight=0.300\n","  Single-Head (A4, ε=1.0) - Epoch 20/100: Train Loss=0.000452, Val Loss=0.000274, Robust Weight=0.300\n","  Single-Head (A4, ε=1.0) - Epoch 30/100: Train Loss=0.000417, Val Loss=0.000273, Robust Weight=0.300\n","  Single-Head (A4, ε=1.0): Early stopping at epoch 34\n","  Single-Head (A4, ε=1.0) trained - RMSE: 0.016549, R²: 0.100923\n","\n","================================================================================\n","Training Multi-Head with Adversarial Training\n","================================================================================\n","\n","Training Multi-Head (A1, ε=0.25)...\n","  Multi-Head (A1, ε=0.25) - Epoch 10/100: Train Loss=0.000412, Val Loss=0.000284, Robust Weight=0.300\n","  Multi-Head (A1, ε=0.25) - Epoch 20/100: Train Loss=0.000401, Val Loss=0.000277, Robust Weight=0.300\n","  Multi-Head (A1, ε=0.25) - Epoch 30/100: Train Loss=0.000355, Val Loss=0.000282, Robust Weight=0.300\n","  Multi-Head (A1, ε=0.25): Early stopping at epoch 35\n","  Multi-Head (A1, ε=0.25) trained - RMSE: 0.016935, R²: 0.058529\n","\n","Training Multi-Head (A1, ε=0.5)...\n","  Multi-Head (A1, ε=0.5) - Epoch 10/100: Train Loss=0.000422, Val Loss=0.000277, Robust Weight=0.300\n","  Multi-Head (A1, ε=0.5) - Epoch 20/100: Train Loss=0.000396, Val Loss=0.000276, Robust Weight=0.300\n","  Multi-Head (A1, ε=0.5): Early stopping at epoch 25\n","  Multi-Head (A1, ε=0.5) trained - RMSE: 0.016712, R²: 0.083156\n","\n","Training Multi-Head (A1, ε=1.0)...\n","  Multi-Head (A1, ε=1.0) - Epoch 10/100: Train Loss=0.000465, Val Loss=0.000282, Robust Weight=0.300\n","  Multi-Head (A1, ε=1.0) - Epoch 20/100: Train Loss=0.000430, Val Loss=0.000279, Robust Weight=0.300\n","  Multi-Head (A1, ε=1.0) - Epoch 30/100: Train Loss=0.000413, Val Loss=0.000276, Robust Weight=0.300\n","  Multi-Head (A1, ε=1.0) - Epoch 40/100: Train Loss=0.000401, Val Loss=0.000279, Robust Weight=0.300\n","  Multi-Head (A1, ε=1.0): Early stopping at epoch 43\n","  Multi-Head (A1, ε=1.0) trained - RMSE: 0.016674, R²: 0.087253\n","\n","Training Multi-Head (A2, ε=0.25)...\n","  Multi-Head (A2, ε=0.25) - Epoch 10/100: Train Loss=0.000403, Val Loss=0.000287, Robust Weight=0.300\n","  Multi-Head (A2, ε=0.25) - Epoch 20/100: Train Loss=0.000383, Val Loss=0.000282, Robust Weight=0.300\n","  Multi-Head (A2, ε=0.25) - Epoch 30/100: Train Loss=0.000357, Val Loss=0.000281, Robust Weight=0.300\n","  Multi-Head (A2, ε=0.25) - Epoch 40/100: Train Loss=0.000321, Val Loss=0.000287, Robust Weight=0.300\n","  Multi-Head (A2, ε=0.25): Early stopping at epoch 43\n","  Multi-Head (A2, ε=0.25) trained - RMSE: 0.017008, R²: 0.050314\n","\n","Training Multi-Head (A2, ε=0.5)...\n","  Multi-Head (A2, ε=0.5) - Epoch 10/100: Train Loss=0.000412, Val Loss=0.000278, Robust Weight=0.300\n","  Multi-Head (A2, ε=0.5) - Epoch 20/100: Train Loss=0.000379, Val Loss=0.000291, Robust Weight=0.300\n","  Multi-Head (A2, ε=0.5): Early stopping at epoch 30\n","  Multi-Head (A2, ε=0.5) trained - RMSE: 0.017076, R²: 0.042743\n","\n","Training Multi-Head (A2, ε=1.0)...\n","  Multi-Head (A2, ε=1.0) - Epoch 10/100: Train Loss=0.000410, Val Loss=0.000289, Robust Weight=0.300\n","  Multi-Head (A2, ε=1.0) - Epoch 20/100: Train Loss=0.000385, Val Loss=0.000287, Robust Weight=0.300\n","  Multi-Head (A2, ε=1.0): Early stopping at epoch 25\n","  Multi-Head (A2, ε=1.0) trained - RMSE: 0.016968, R²: 0.054831\n","\n","Training Multi-Head (A3, ε=0.25)...\n","  Multi-Head (A3, ε=0.25) - Epoch 10/100: Train Loss=0.000432, Val Loss=0.000283, Robust Weight=0.300\n","  Multi-Head (A3, ε=0.25) - Epoch 20/100: Train Loss=0.000412, Val Loss=0.000284, Robust Weight=0.300\n","  Multi-Head (A3, ε=0.25) - Epoch 30/100: Train Loss=0.000380, Val Loss=0.000279, Robust Weight=0.300\n","  Multi-Head (A3, ε=0.25) - Epoch 40/100: Train Loss=0.000372, Val Loss=0.000279, Robust Weight=0.300\n","  Multi-Head (A3, ε=0.25): Early stopping at epoch 49\n","  Multi-Head (A3, ε=0.25) trained - RMSE: 0.016713, R²: 0.083025\n","\n","Training Multi-Head (A3, ε=0.5)...\n","  Multi-Head (A3, ε=0.5) - Epoch 10/100: Train Loss=0.000461, Val Loss=0.000277, Robust Weight=0.300\n","  Multi-Head (A3, ε=0.5) - Epoch 20/100: Train Loss=0.000415, Val Loss=0.000278, Robust Weight=0.300\n","  Multi-Head (A3, ε=0.5): Early stopping at epoch 26\n","  Multi-Head (A3, ε=0.5) trained - RMSE: 0.016728, R²: 0.081413\n","\n","Training Multi-Head (A3, ε=1.0)...\n","  Multi-Head (A3, ε=1.0) - Epoch 10/100: Train Loss=0.000460, Val Loss=0.000279, Robust Weight=0.300\n","  Multi-Head (A3, ε=1.0) - Epoch 20/100: Train Loss=0.000445, Val Loss=0.000283, Robust Weight=0.300\n","  Multi-Head (A3, ε=1.0): Early stopping at epoch 29\n","  Multi-Head (A3, ε=1.0) trained - RMSE: 0.016723, R²: 0.081908\n","\n","Training Multi-Head (A4, ε=0.25)...\n","  Multi-Head (A4, ε=0.25) - Epoch 10/100: Train Loss=0.000408, Val Loss=0.000284, Robust Weight=0.300\n","  Multi-Head (A4, ε=0.25) - Epoch 20/100: Train Loss=0.000382, Val Loss=0.000284, Robust Weight=0.300\n","  Multi-Head (A4, ε=0.25) - Epoch 30/100: Train Loss=0.000359, Val Loss=0.000284, Robust Weight=0.300\n","  Multi-Head (A4, ε=0.25): Early stopping at epoch 31\n","  Multi-Head (A4, ε=0.25) trained - RMSE: 0.016886, R²: 0.063948\n","\n","Training Multi-Head (A4, ε=0.5)...\n","  Multi-Head (A4, ε=0.5) - Epoch 10/100: Train Loss=0.000437, Val Loss=0.000278, Robust Weight=0.300\n","  Multi-Head (A4, ε=0.5) - Epoch 20/100: Train Loss=0.000400, Val Loss=0.000278, Robust Weight=0.300\n","  Multi-Head (A4, ε=0.5) - Epoch 30/100: Train Loss=0.000383, Val Loss=0.000279, Robust Weight=0.300\n","  Multi-Head (A4, ε=0.5): Early stopping at epoch 32\n","  Multi-Head (A4, ε=0.5) trained - RMSE: 0.016716, R²: 0.082715\n","\n","Training Multi-Head (A4, ε=1.0)...\n","  Multi-Head (A4, ε=1.0) - Epoch 10/100: Train Loss=0.000455, Val Loss=0.000282, Robust Weight=0.300\n","  Multi-Head (A4, ε=1.0) - Epoch 20/100: Train Loss=0.000426, Val Loss=0.000284, Robust Weight=0.300\n","  Multi-Head (A4, ε=1.0) - Epoch 30/100: Train Loss=0.000413, Val Loss=0.000280, Robust Weight=0.300\n","  Multi-Head (A4, ε=1.0) - Epoch 40/100: Train Loss=0.000407, Val Loss=0.000278, Robust Weight=0.300\n","  Multi-Head (A4, ε=1.0) - Epoch 50/100: Train Loss=0.000405, Val Loss=0.000278, Robust Weight=0.300\n","  Multi-Head (A4, ε=1.0) - Epoch 60/100: Train Loss=0.000399, Val Loss=0.000279, Robust Weight=0.300\n","  Multi-Head (A4, ε=1.0): Early stopping at epoch 61\n","  Multi-Head (A4, ε=1.0) trained - RMSE: 0.016683, R²: 0.086326\n","\n","================================================================================\n","Training Multi-Head Diversity with Adversarial Training\n","================================================================================\n","\n","Training Multi-Head Diversity (A1, ε=0.25)...\n","  Multi-Head Diversity (A1, ε=0.25) - Epoch 10/100: Train Loss=0.000401, Val Loss=0.000273, Robust Weight=0.300\n","  Multi-Head Diversity (A1, ε=0.25) - Epoch 20/100: Train Loss=0.000385, Val Loss=0.000274, Robust Weight=0.300\n","  Multi-Head Diversity (A1, ε=0.25) - Epoch 30/100: Train Loss=0.000353, Val Loss=0.000277, Robust Weight=0.300\n","  Multi-Head Diversity (A1, ε=0.25): Early stopping at epoch 34\n","  Multi-Head Diversity (A1, ε=0.25) trained - RMSE: 0.016696, R²: 0.084886\n","\n","Training Multi-Head Diversity (A1, ε=0.5)...\n","  Multi-Head Diversity (A1, ε=0.5) - Epoch 10/100: Train Loss=0.000412, Val Loss=0.000270, Robust Weight=0.300\n","  Multi-Head Diversity (A1, ε=0.5) - Epoch 20/100: Train Loss=0.000406, Val Loss=0.000271, Robust Weight=0.300\n","  Multi-Head Diversity (A1, ε=0.5) - Epoch 30/100: Train Loss=0.000375, Val Loss=0.000273, Robust Weight=0.300\n","  Multi-Head Diversity (A1, ε=0.5): Early stopping at epoch 35\n","  Multi-Head Diversity (A1, ε=0.5) trained - RMSE: 0.016553, R²: 0.100437\n","\n","Training Multi-Head Diversity (A1, ε=1.0)...\n","  Multi-Head Diversity (A1, ε=1.0) - Epoch 10/100: Train Loss=0.000453, Val Loss=0.000272, Robust Weight=0.300\n","  Multi-Head Diversity (A1, ε=1.0) - Epoch 20/100: Train Loss=0.000419, Val Loss=0.000270, Robust Weight=0.300\n","  Multi-Head Diversity (A1, ε=1.0): Early stopping at epoch 29\n","  Multi-Head Diversity (A1, ε=1.0) trained - RMSE: 0.016399, R²: 0.117188\n","\n","Training Multi-Head Diversity (A2, ε=0.25)...\n","  Multi-Head Diversity (A2, ε=0.25) - Epoch 10/100: Train Loss=0.000383, Val Loss=0.000276, Robust Weight=0.300\n","  Multi-Head Diversity (A2, ε=0.25) - Epoch 20/100: Train Loss=0.000359, Val Loss=0.000276, Robust Weight=0.300\n","  Multi-Head Diversity (A2, ε=0.25): Early stopping at epoch 22\n","  Multi-Head Diversity (A2, ε=0.25) trained - RMSE: 0.016663, R²: 0.088525\n","\n","Training Multi-Head Diversity (A2, ε=0.5)...\n","  Multi-Head Diversity (A2, ε=0.5) - Epoch 10/100: Train Loss=0.000390, Val Loss=0.000278, Robust Weight=0.300\n","  Multi-Head Diversity (A2, ε=0.5) - Epoch 20/100: Train Loss=0.000364, Val Loss=0.000278, Robust Weight=0.300\n","  Multi-Head Diversity (A2, ε=0.5): Early stopping at epoch 22\n","  Multi-Head Diversity (A2, ε=0.5) trained - RMSE: 0.016711, R²: 0.083277\n","\n","Training Multi-Head Diversity (A2, ε=1.0)...\n","  Multi-Head Diversity (A2, ε=1.0) - Epoch 10/100: Train Loss=0.000411, Val Loss=0.000275, Robust Weight=0.300\n","  Multi-Head Diversity (A2, ε=1.0) - Epoch 20/100: Train Loss=0.000376, Val Loss=0.000274, Robust Weight=0.300\n","  Multi-Head Diversity (A2, ε=1.0): Early stopping at epoch 26\n","  Multi-Head Diversity (A2, ε=1.0) trained - RMSE: 0.016616, R²: 0.093621\n","\n","Training Multi-Head Diversity (A3, ε=0.25)...\n","  Multi-Head Diversity (A3, ε=0.25) - Epoch 10/100: Train Loss=0.000434, Val Loss=0.000278, Robust Weight=0.300\n","  Multi-Head Diversity (A3, ε=0.25) - Epoch 20/100: Train Loss=0.000386, Val Loss=0.000276, Robust Weight=0.300\n","  Multi-Head Diversity (A3, ε=0.25): Early stopping at epoch 26\n","  Multi-Head Diversity (A3, ε=0.25) trained - RMSE: 0.016580, R²: 0.097522\n","\n","Training Multi-Head Diversity (A3, ε=0.5)...\n","  Multi-Head Diversity (A3, ε=0.5) - Epoch 10/100: Train Loss=0.000444, Val Loss=0.000268, Robust Weight=0.300\n","  Multi-Head Diversity (A3, ε=0.5) - Epoch 20/100: Train Loss=0.000413, Val Loss=0.000273, Robust Weight=0.300\n","  Multi-Head Diversity (A3, ε=0.5): Early stopping at epoch 26\n","  Multi-Head Diversity (A3, ε=0.5) trained - RMSE: 0.016512, R²: 0.104915\n","\n","Training Multi-Head Diversity (A3, ε=1.0)...\n","  Multi-Head Diversity (A3, ε=1.0) - Epoch 10/100: Train Loss=0.000454, Val Loss=0.000268, Robust Weight=0.300\n","  Multi-Head Diversity (A3, ε=1.0) - Epoch 20/100: Train Loss=0.000428, Val Loss=0.000268, Robust Weight=0.300\n","  Multi-Head Diversity (A3, ε=1.0): Early stopping at epoch 28\n","  Multi-Head Diversity (A3, ε=1.0) trained - RMSE: 0.016461, R²: 0.110508\n","\n","Training Multi-Head Diversity (A4, ε=0.25)...\n","  Multi-Head Diversity (A4, ε=0.25) - Epoch 10/100: Train Loss=0.000408, Val Loss=0.000275, Robust Weight=0.300\n","  Multi-Head Diversity (A4, ε=0.25) - Epoch 20/100: Train Loss=0.000365, Val Loss=0.000273, Robust Weight=0.300\n","  Multi-Head Diversity (A4, ε=0.25): Early stopping at epoch 26\n","  Multi-Head Diversity (A4, ε=0.25) trained - RMSE: 0.016629, R²: 0.092170\n","\n","Training Multi-Head Diversity (A4, ε=0.5)...\n","  Multi-Head Diversity (A4, ε=0.5) - Epoch 10/100: Train Loss=0.000417, Val Loss=0.000271, Robust Weight=0.300\n","  Multi-Head Diversity (A4, ε=0.5) - Epoch 20/100: Train Loss=0.000387, Val Loss=0.000268, Robust Weight=0.300\n","  Multi-Head Diversity (A4, ε=0.5) - Epoch 30/100: Train Loss=0.000374, Val Loss=0.000272, Robust Weight=0.300\n","  Multi-Head Diversity (A4, ε=0.5): Early stopping at epoch 33\n","  Multi-Head Diversity (A4, ε=0.5) trained - RMSE: 0.016480, R²: 0.108436\n","\n","Training Multi-Head Diversity (A4, ε=1.0)...\n","  Multi-Head Diversity (A4, ε=1.0) - Epoch 10/100: Train Loss=0.000434, Val Loss=0.000272, Robust Weight=0.300\n","  Multi-Head Diversity (A4, ε=1.0) - Epoch 20/100: Train Loss=0.000435, Val Loss=0.000270, Robust Weight=0.300\n","  Multi-Head Diversity (A4, ε=1.0) - Epoch 30/100: Train Loss=0.000401, Val Loss=0.000271, Robust Weight=0.300\n","  Multi-Head Diversity (A4, ε=1.0): Early stopping at epoch 34\n","  Multi-Head Diversity (A4, ε=1.0) trained - RMSE: 0.016469, R²: 0.109587\n","\n","================================================================================\n","ADVERSARIAL TRAINING COMPLETE\n","================================================================================\n","Total adversarially trained models: 36\n"]}],"source":["# Train adversarially trained models\n","print(\"=\" * 80)\n","print(\"ADVERSARIAL TRAINING FOR TRANSFORMER MODELS\")\n","print(\"=\" * 80)\n","print(f\"Training on attacks: {ADVERSARIAL_CONFIG['attacks']}\")\n","print(f\"Epsilons: {ADVERSARIAL_CONFIG['epsilons']}\")\n","print(f\"Robust weight: {ADVERSARIAL_CONFIG['robust_weight']}\")\n","print()\n","\n","# Models to train adversarially\n","transformer_model_names = ['Single-Head', 'Multi-Head', 'Multi-Head Diversity']\n","base_models = {\n","    'Single-Head': models['Single-Head'],\n","    'Multi-Head': models['Multi-Head'],\n","    'Multi-Head Diversity': models['Multi-Head Diversity']\n","}\n","\n","# Train each model with each attack at each epsilon\n","for model_name in transformer_model_names:\n","    print(f\"\\n{'='*80}\")\n","    print(f\"Training {model_name} with Adversarial Training\")\n","    print(f\"{'='*80}\")\n","\n","    base_model = base_models[model_name]\n","\n","    for attack_type in ADVERSARIAL_CONFIG['attacks']:\n","        for epsilon in ADVERSARIAL_CONFIG['epsilons']:\n","            model_key = f\"{model_name} ({attack_type.upper()}, ε={epsilon})\"\n","            print(f\"\\nTraining {model_key}...\")\n","\n","            try:\n","                adv_model, adv_pred, adv_history = train_adversarial_model(\n","                    base_model, model_name, X_train_scaled, y_train,\n","                    X_val_scaled, y_val, attack_type, epsilon,\n","                    ADVERSARIAL_CONFIG, device\n","                )\n","\n","                # Evaluate\n","                adv_rmse = np.sqrt(mean_squared_error(y_val, adv_pred))\n","                adv_r2 = r2_score(y_val, adv_pred)\n","\n","                adversarial_models[model_key] = adv_model\n","                adversarial_training_history[model_key] = {\n","                    'rmse': adv_rmse,\n","                    'r2': adv_r2,\n","                    'history': adv_history\n","                }\n","\n","                print(f\"  {model_key} trained - RMSE: {adv_rmse:.6f}, R²: {adv_r2:.6f}\")\n","\n","            except Exception as e:\n","                print(f\" Error training {model_key}: {e}\")\n","                import traceback\n","                traceback.print_exc()\n","\n","print(\"\\n\" + \"=\" * 80)\n","print(\"ADVERSARIAL TRAINING COMPLETE\")\n","print(\"=\" * 80)\n","print(f\"Total adversarially trained models: {len(adversarial_models)}\")"]},{"cell_type":"code","source":[],"metadata":{"id":"4zvWVCdy27Iw"},"id":"4zvWVCdy27Iw","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"papermill":{"default_parameters":{},"duration":10661.980152,"end_time":"2026-01-23T08:19:09.986181","environment_variables":{},"exception":null,"input_path":"notebooks/Final_Models_Demo_6heads.ipynb","output_path":"Final_Models_Demo_6heads_executed.ipynb","parameters":{},"start_time":"2026-01-23T05:21:28.006029","version":"2.6.0"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}