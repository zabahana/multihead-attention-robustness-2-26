{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Head Diversity: PGD/FGSM & Certified Robustness\n",
        "\n",
        "**Worst-case adversarial attacks** (FGSM, PGD) and **certified robustness** evaluation.\n",
        "\n",
        "**Model**: Multi-Head Diversity only.\n",
        "\n",
        "**Attacks**:\n",
        "- **FGSM**: Fast Gradient Sign Method – one-step perturbation to maximize MSE\n",
        "- **PGD**: Projected Gradient Descent – iterative worst-case attack\n",
        "\n",
        "**Certified**: Gradient-norm-based upper bound on worst-case prediction change."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "USE_SMALL_DATA = True\n",
        "N_SAMPLES = 5000\n",
        "N_EPOCHS = 30\n",
        "PGD_STEPS = 10\n",
        "PGD_ALPHA = 0.1  # Step size per PGD iteration"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    print('Google Drive mounted.')\n",
        "except Exception:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys, os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "def _find_repo_root():\n",
        "    cwd = Path.cwd().resolve()\n",
        "    for p in [Path('/content/drive/MyDrive/multihead-attention-robustness'), Path('/content/drive/My Drive/multihead-attention-robustness'), Path('/content/repo_run')]:\n",
        "        if p.exists() and (p / 'src').exists():\n",
        "            return p\n",
        "    drive_root = Path('/content/drive')\n",
        "    if drive_root.exists():\n",
        "        for base in [drive_root / 'MyDrive', drive_root / 'My Drive', drive_root]:\n",
        "            if base.exists():\n",
        "                for sub in base.iterdir():\n",
        "                    if sub.is_dir() and 'multihead-attention' in sub.name.lower() and (sub / 'src').exists():\n",
        "                        return sub\n",
        "    p = cwd\n",
        "    for _ in range(10):\n",
        "        if (p / 'src').exists():\n",
        "            return p\n",
        "        if p.parent == p:\n",
        "            break\n",
        "        p = p.parent\n",
        "    return cwd.parent if cwd.name == 'notebooks' else cwd\n",
        "\n",
        "repo_root = _find_repo_root()\n",
        "if not (repo_root / 'src').exists():\n",
        "    raise FileNotFoundError(f\"Repo root not found. Run Drive mount first.\")\n",
        "sys.path.insert(0, str(repo_root))\n",
        "os.chdir(repo_root)\n",
        "from src.models.feature_token_transformer import FeatureTokenTransformer\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device: {device}, Repo: {repo_root}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data_path = repo_root / 'data' / 'cross_sectional' / 'master_table.csv'\n",
        "if not data_path.exists():\n",
        "    data_path = repo_root / 'data' / 'master_table.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "if 'date' in df.columns:\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df = df.set_index('date')\n",
        "\n",
        "class CrossSectionalDataSplitter:\n",
        "    def __init__(self, train_start='2005-01-01', train_end='2017-12-31', val_start='2018-01-01', val_end='2019-12-31'):\n",
        "        self.train_start, self.train_end = train_start, train_end\n",
        "        self.val_start, self.val_end = val_start, val_end\n",
        "    def split(self, master_table):\n",
        "        master_table = master_table.copy()\n",
        "        master_table.index = pd.to_datetime(master_table.index)\n",
        "        return {'train': master_table.loc[self.train_start:self.train_end], 'val': master_table.loc[self.val_start:self.val_end]}\n",
        "    def prepare_features_labels(self, data):\n",
        "        if data.empty:\n",
        "            return pd.DataFrame(), pd.Series()\n",
        "        numeric_data = data.select_dtypes(include=[np.number])\n",
        "        if numeric_data.empty:\n",
        "            return pd.DataFrame(), pd.Series()\n",
        "        exclude_cols = ['mktcap', 'market_cap', 'date', 'year', 'month', 'ticker', 'permno', 'gvkey']\n",
        "        target_cols = ['return', 'returns', 'ret', 'target', 'y', 'next_return', 'forward_return', 'ret_1', 'ret_1m', 'ret_12m', 'future_return', 'returns_1d']\n",
        "        target_col = None\n",
        "        for tc in target_cols:\n",
        "            for col in numeric_data.columns:\n",
        "                if tc.lower() in col.lower() and col.lower() not in [ec.lower() for ec in exclude_cols]:\n",
        "                    target_col = col\n",
        "                    break\n",
        "            if target_col:\n",
        "                break\n",
        "        if target_col is None:\n",
        "            potential = [c for c in numeric_data.columns if c.lower() not in [ec.lower() for ec in exclude_cols]]\n",
        "            target_col = potential[-2] if len(potential) > 1 else (potential[-1] if potential else numeric_data.columns[-1])\n",
        "        feature_cols = [c for c in numeric_data.columns if c != target_col and c.lower() not in [ec.lower() for ec in exclude_cols]]\n",
        "        if not feature_cols:\n",
        "            feature_cols = [c for c in numeric_data.columns if c != target_col]\n",
        "        if not feature_cols:\n",
        "            feature_cols = numeric_data.columns[:-1].tolist()\n",
        "            target_col = numeric_data.columns[-1]\n",
        "        return numeric_data[feature_cols], numeric_data[target_col]\n",
        "\n",
        "splitter = CrossSectionalDataSplitter()\n",
        "data_splits = splitter.split(df)\n",
        "train_df, val_df = data_splits['train'], data_splits['val']\n",
        "X_train_df, y_train = splitter.prepare_features_labels(train_df)\n",
        "X_val_df, y_val = splitter.prepare_features_labels(val_df)\n",
        "X_train = X_train_df.fillna(0).values.astype(np.float32)\n",
        "y_train = y_train.fillna(0).values.astype(np.float32)\n",
        "X_val = X_val_df.fillna(0).values.astype(np.float32)\n",
        "y_val = y_val.fillna(0).values.astype(np.float32)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "if USE_SMALL_DATA and N_SAMPLES < len(X_train):\n",
        "    idx = np.random.RandomState(RANDOM_SEED).choice(len(X_train), N_SAMPLES, replace=False)\n",
        "    X_train_scaled, y_train = X_train_scaled[idx], y_train[idx]\n",
        "\n",
        "sigma_train = np.std(X_train_scaled, axis=0) + 1e-8\n",
        "print(f'Data: train {X_train_scaled.shape[0]}, val {X_val_scaled.shape[0]}, features {X_train_scaled.shape[1]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. FGSM & PGD Attacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def fgsm_attack(model, X, y, epsilon, sigma, device, targeted=False, batch_size=512):\n",
        "    \"\"\"FGSM: x_adv = x + epsilon * sigma * sign(grad_x MSE).\"\"\"\n",
        "    model.eval()\n",
        "    X_adv_list = []\n",
        "    sigma_t = torch.FloatTensor(np.array(sigma, dtype=np.float32)).to(device)\n",
        "    for i in range(0, len(X), batch_size):\n",
        "        X_b = torch.FloatTensor(X[i:i+batch_size]).to(device).requires_grad_(True)\n",
        "        y_b = torch.FloatTensor(y[i:i+batch_size]).to(device)\n",
        "        pred, _ = model(X_b)\n",
        "        loss = nn.MSELoss()(pred.squeeze(), y_b)\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        grad = X_b.grad.detach()\n",
        "        sign_grad = torch.sign(grad)\n",
        "        if targeted:\n",
        "            sign_grad = -sign_grad\n",
        "        delta = epsilon * sigma_t * sign_grad\n",
        "        X_adv_list.append((X_b.detach() + delta).cpu().numpy())\n",
        "    return np.vstack(X_adv_list)\n",
        "\n",
        "def pgd_attack(model, X, y, epsilon, sigma, steps, alpha, device, targeted=False, batch_size=512):\n",
        "    \"\"\"PGD: iterative FGSM with L_inf projection to epsilon*sigma ball.\"\"\"\n",
        "    X_adv = X.copy().astype(np.float32)\n",
        "    sigma_arr = np.array(sigma, dtype=np.float32)\n",
        "    for _ in range(steps):\n",
        "        for i in range(0, len(X_adv), batch_size):\n",
        "            X_t = torch.FloatTensor(X_adv[i:i+batch_size]).to(device).requires_grad_(True)\n",
        "            y_b = torch.FloatTensor(y[i:i+batch_size]).to(device)\n",
        "            model.eval()\n",
        "            pred, _ = model(X_t)\n",
        "            loss = nn.MSELoss()(pred.squeeze(), y_b)\n",
        "            model.zero_grad()\n",
        "            loss.backward()\n",
        "            grad = X_t.grad.detach().cpu().numpy()\n",
        "            sign_grad = np.sign(grad)\n",
        "            if targeted:\n",
        "                sign_grad = -sign_grad\n",
        "            delta = X_adv[i:i+batch_size] - X[i:i+batch_size].astype(np.float32)\n",
        "            delta = delta + alpha * sigma_arr * sign_grad\n",
        "            delta = np.clip(delta, -epsilon * sigma_arr, epsilon * sigma_arr)\n",
        "            X_adv[i:i+batch_size] = X[i:i+batch_size].astype(np.float32) + delta\n",
        "    return X_adv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train Multi-Head Diversity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train_mhd(model, X_train, y_train, X_val, y_val, config, device):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
        "    X_t = torch.FloatTensor(X_train).to(device)\n",
        "    y_t = torch.FloatTensor(y_train).to(device)\n",
        "    X_v = torch.FloatTensor(X_val).to(device)\n",
        "    batch_size = config['batch_size']\n",
        "    for epoch in range(config['epochs']):\n",
        "        model.train()\n",
        "        for i in range(0, len(X_t), batch_size):\n",
        "            bx, by = X_t[i:i+batch_size], y_t[i:i+batch_size]\n",
        "            opt.zero_grad()\n",
        "            pred, attn = model(bx)\n",
        "            loss = criterion(pred.squeeze(), by)\n",
        "            if model.use_head_diversity and attn:\n",
        "                loss = loss + model.compute_diversity_loss([attn[f'layer_{j}'] for j in range(len(attn))])\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                p, _ = model(X_v)\n",
        "                vloss = criterion(p.squeeze(), torch.FloatTensor(y_val).to(device))\n",
        "            print(f'  Epoch {epoch+1}: val_loss={vloss.item():.6f}')\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        pred, _ = model(X_v)\n",
        "        pred = pred.squeeze().cpu().numpy()\n",
        "    return model, pred\n",
        "\n",
        "tr_cfg = {'d_model': 72, 'num_layers': 2, 'd_ff': 512, 'dropout': 0.1, 'learning_rate': 0.0001, 'batch_size': 32, 'epochs': N_EPOCHS if USE_SMALL_DATA else 100}\n",
        "model = FeatureTokenTransformer(num_features=X_train_scaled.shape[1], d_model=tr_cfg['d_model'], num_heads=8, num_layers=tr_cfg['num_layers'], d_ff=tr_cfg['d_ff'], dropout=tr_cfg['dropout'], use_head_diversity=True, diversity_weight=0.01)\n",
        "model, pred_clean = train_mhd(model, X_train_scaled, y_train, X_val_scaled, y_val, tr_cfg, device)\n",
        "\n",
        "clean_rmse = np.sqrt(mean_squared_error(y_val, pred_clean))\n",
        "clean_r2 = r2_score(y_val, pred_clean)\n",
        "print(f'Clean: RMSE={clean_rmse:.6f}, R²={clean_r2:.6f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Evaluate Under FGSM & PGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "EPSILONS = [0.25, 0.5, 1.0]\n",
        "results = []\n",
        "\n",
        "for eps in EPSILONS:\n",
        "    X_fgsm = fgsm_attack(model, X_val_scaled, y_val, eps, sigma_train, device)\n",
        "    X_pgd = pgd_attack(model, X_val_scaled, y_val, eps, sigma_train, PGD_STEPS, PGD_ALPHA, device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        pred_fgsm = model(torch.FloatTensor(X_fgsm).to(device))[0].squeeze().cpu().numpy()\n",
        "        pred_pgd = model(torch.FloatTensor(X_pgd).to(device))[0].squeeze().cpu().numpy()\n",
        "    \n",
        "    for name, pred in [('FGSM', pred_fgsm), ('PGD', pred_pgd)]:\n",
        "        rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
        "        r2 = r2_score(y_val, pred)\n",
        "        delta_rmse = rmse - clean_rmse\n",
        "        rob_rmse = min(1.0, 1.0 - delta_rmse / clean_rmse) if clean_rmse > 0 else 1.0\n",
        "        rob_r2 = min(1.0, r2 / clean_r2) if clean_r2 > 0 else 1.0\n",
        "        results.append({'attack': name, 'epsilon': eps, 'rmse': rmse, 'r2': r2, 'delta_rmse': delta_rmse, 'rob_rmse': rob_rmse, 'rob_r2': rob_r2})\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df.to_string())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Certified Robustness (Gradient-Norm Bound)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def certified_bound_gradient_norm(model, X, y, epsilon, sigma, device, batch_size=256):\n",
        "    \"\"\"Upper bound on |f(x+δ)-f(x)|: |δ|≤ε*σ implies |f(x+δ)-f(x)| ≤ ε * ||σ|| * ||∇f(x)|| (L2).\"\"\"\n",
        "    model.eval()\n",
        "    grad_norms = []\n",
        "    pred_clean_list = []\n",
        "    n = len(X)\n",
        "    for i in range(0, n, batch_size):\n",
        "        X_b = torch.FloatTensor(X[i:i+batch_size]).to(device).requires_grad_(True)\n",
        "        y_b = torch.FloatTensor(y[i:i+batch_size]).to(device)\n",
        "        pred, _ = model(X_b)\n",
        "        loss = nn.MSELoss()(pred.squeeze(), y_b)\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        grad = X_b.grad.detach().cpu().numpy()\n",
        "        sigma_b = sigma if isinstance(sigma, np.ndarray) else np.array(sigma)\n",
        "        gn = np.sqrt(np.sum((grad * sigma_b) ** 2, axis=1))\n",
        "        grad_norms.extend(gn)\n",
        "        pred_clean_list.extend(pred.detach().squeeze().cpu().numpy().tolist())\n",
        "    grad_norms = np.array(grad_norms)\n",
        "    pred_clean_arr = np.array(pred_clean_list)\n",
        "    cert_bound = epsilon * grad_norms\n",
        "    return cert_bound, pred_clean_arr, np.mean(grad_norms), np.median(grad_norms)\n",
        "\n",
        "cert_bounds, pred_cert, mean_gn, median_gn = certified_bound_gradient_norm(model, X_val_scaled, y_val, 1.0, sigma_train, device)\n",
        "print(f'Certified bound (ε=1.0): mean |Δf| ≤ {np.mean(cert_bounds):.6f}, median ≤ {np.median(cert_bounds):.6f}')\n",
        "print(f'Mean gradient norm (scaled by σ): {mean_gn:.6f}')\n",
        "\n",
        "cert_results = []\n",
        "for eps in EPSILONS:\n",
        "    cb, _, _, _ = certified_bound_gradient_norm(model, X_val_scaled, y_val, eps, sigma_train, device)\n",
        "    cert_results.append({'epsilon': eps, 'cert_bound_mean': np.mean(cb), 'cert_bound_median': np.median(cb)})\n",
        "cert_df = pd.DataFrame(cert_results)\n",
        "print('\\nCertified bounds by ε:')\n",
        "print(cert_df.to_string(index=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# 1. RMSE-based robustness vs epsilon (FGSM vs PGD)\n",
        "ax = axes[0, 0]\n",
        "for attack in ['FGSM', 'PGD']:\n",
        "    d = results_df[results_df['attack'] == attack]\n",
        "    ax.plot(d['epsilon'], d['rob_rmse'], 'o-', label=attack)\n",
        "ax.axhline(1.0, color='gray', linestyle='--', alpha=0.5)\n",
        "ax.set_xlabel('ε (perturbation budget)')\n",
        "ax.set_ylabel('Robustness (RMSE-based)')\n",
        "ax.set_title('FGSM vs PGD: RMSE-based Robustness')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. R² degradation vs epsilon\n",
        "ax = axes[0, 1]\n",
        "for attack in ['FGSM', 'PGD']:\n",
        "    d = results_df[results_df['attack'] == attack]\n",
        "    ax.plot(d['epsilon'], d['r2'], 's-', label=attack)\n",
        "ax.axhline(clean_r2, color='gray', linestyle='--', alpha=0.5, label='Clean')\n",
        "ax.set_xlabel('ε')\n",
        "ax.set_ylabel('R² under attack')\n",
        "ax.set_title('R² Degradation Under FGSM/PGD')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Heatmap: Robustness\n",
        "ax = axes[1, 0]\n",
        "pivot = results_df.pivot(index='attack', columns='epsilon', values='rob_rmse')\n",
        "sns.heatmap(pivot, annot=True, fmt='.3f', cmap='RdYlGn', vmin=0.3, vmax=1.0, ax=ax)\n",
        "ax.set_title('Robustness Heatmap (FGSM vs PGD)')\n",
        "\n",
        "# 4. Certified bound vs epsilon\n",
        "ax = axes[1, 1]\n",
        "ax.plot(cert_df['epsilon'], cert_df['cert_bound_mean'], 'o-', label='Mean bound')\n",
        "ax.plot(cert_df['epsilon'], cert_df['cert_bound_median'], 's-', label='Median bound')\n",
        "ax.set_xlabel('ε')\n",
        "ax.set_ylabel('Certified |Δf| bound')\n",
        "ax.set_title('Certified Robustness: |f(x+δ)-f(x)| ≤ ε·||σ⊙∇f||')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Multi-Head Diversity: PGD/FGSM & Certified Robustness', fontsize=12, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Clean vs Adversarial predictions (FGSM and PGD at ε=1.0)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "X_fgsm = fgsm_attack(model, X_val_scaled, y_val, 1.0, sigma_train, device)\n",
        "X_pgd = pgd_attack(model, X_val_scaled, y_val, 1.0, sigma_train, PGD_STEPS, PGD_ALPHA, device)\n",
        "with torch.no_grad():\n",
        "    pred_fgsm = model(torch.FloatTensor(X_fgsm).to(device))[0].squeeze().cpu().numpy()\n",
        "    pred_pgd = model(torch.FloatTensor(X_pgd).to(device))[0].squeeze().cpu().numpy()\n",
        "\n",
        "axes[0].scatter(pred_clean, pred_fgsm, alpha=0.3, s=5, label='FGSM')\n",
        "axes[0].scatter(pred_clean, pred_pgd, alpha=0.3, s=5, label='PGD')\n",
        "axes[0].plot([pred_clean.min(), pred_clean.max()], [pred_clean.min(), pred_clean.max()], 'r--', label='y=x')\n",
        "axes[0].set_xlabel('Clean prediction')\n",
        "axes[0].set_ylabel('Adversarial prediction (ε=1.0)')\n",
        "axes[0].set_title('Clean vs FGSM/PGD Predictions')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].scatter(y_val, pred_clean, alpha=0.3, s=5, label='Clean', c='blue')\n",
        "axes[1].scatter(y_val, pred_fgsm, alpha=0.3, s=5, label='FGSM', c='orange')\n",
        "axes[1].scatter(y_val, pred_pgd, alpha=0.3, s=5, label='PGD', c='green')\n",
        "axes[1].set_xlabel('True return')\n",
        "axes[1].set_ylabel('Prediction')\n",
        "axes[1].set_title('True vs Predicted (Clean vs Adversarial)')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print('='*60)\n",
        "print('SUMMARY: Multi-Head Diversity (PGD/FGSM & Certified)')\n",
        "print('='*60)\n",
        "print(f'Clean: RMSE={clean_rmse:.6f}, R²={clean_r2:.6f}')\n",
        "print('\\nRobustness (RMSE-based) by attack (mean over ε):')\n",
        "for a in ['FGSM', 'PGD']:\n",
        "    m = results_df[results_df['attack']==a]['rob_rmse'].mean()\n",
        "    print(f'  {a}: {m:.4f}')\n",
        "print('\\nCertified bound (mean |Δf| ≤ ε·||σ⊙∇f||):')\n",
        "print(cert_df.to_string(index=False))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}