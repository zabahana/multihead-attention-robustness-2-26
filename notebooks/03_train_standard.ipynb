{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e0ee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small-data turn-on/off for all 6 notebook runs (01→06). Set once; applies to full pipeline.\n",
    "USE_SMALL_DATA = False  # True = small data (N_SAMPLES); False = full data\n",
    "N_SAMPLES = 10       # Max observations when USE_SMALL_DATA (e.g. 10 for quick test)\n",
    "N_EPOCHS = 1       # Max training epochs when USE_SMALL_DATA (02, 03, 04)\n",
    "# 01: applied automatically below. 02-04: epochs/n_epochs/num_epochs set automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ebde29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and setup (needed when 02-06 run in separate kernel)\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Repo root for src imports\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "except Exception:\n",
    "    pass\n",
    "def _find_repo_root():\n",
    "    cwd = Path.cwd().resolve()\n",
    "    for p in [Path('/content/drive/MyDrive/multihead-attention-robustness'),\n",
    "              Path('/content/drive/My Drive/multihead-attention-robustness'),\n",
    "              Path('/content/repo_run')]:\n",
    "        if (p / 'src').exists():\n",
    "            return p\n",
    "    drive_root = Path('/content/drive')\n",
    "    if drive_root.exists():\n",
    "        for base in [drive_root / 'MyDrive', drive_root / 'My Drive', drive_root]:\n",
    "            p = base / 'multihead-attention-robustness'\n",
    "            if p.exists() and (p / 'src').exists():\n",
    "                return p\n",
    "    p = cwd\n",
    "    for _ in range(10):\n",
    "        if (p / 'src').exists():\n",
    "            return p\n",
    "        if p.parent == p:\n",
    "            break\n",
    "        p = p.parent\n",
    "    return cwd.parent if cwd.name == 'notebooks' else cwd\n",
    "repo_root = _find_repo_root()\n",
    "sys.path.insert(0, str(repo_root))\n",
    "from src.models.feature_token_transformer import FeatureTokenTransformer, SingleHeadTransformer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "models = {}\n",
    "training_history = {}\n",
    "TRAINING_CONFIG = {\n",
    "    'ols': {}, 'ridge': {'alpha': 1.0},\n",
    "    'mlp': {'hidden_dims': [128, 64], 'learning_rate': 0.001, 'batch_size': 64, 'epochs': 100, 'patience': 10},\n",
    "    'transformer': {'d_model': 72, 'num_heads': 8, 'num_layers': 2, 'd_ff': 512, 'dropout': 0.1,\n",
    "                   'learning_rate': 0.0001, 'batch_size': 32, 'epochs': 100, 'patience': 20}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72155dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fresh data from master_table.csv (standalone: each notebook pulls its own data)\n",
    "data_path = repo_root / 'data' / 'cross_sectional' / 'master_table.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "if 'date' in df.columns:\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.set_index('date')\n",
    "class CrossSectionalDataSplitter:\n",
    "    def __init__(self, train_start='2005-01-01', train_end='2017-12-31', val_start='2018-01-01', val_end='2019-12-31'):\n",
    "        self.train_start, self.train_end = train_start, train_end\n",
    "        self.val_start, self.val_end = val_start, val_end\n",
    "    def split(self, master_table):\n",
    "        master_table = master_table.copy()\n",
    "        master_table.index = pd.to_datetime(master_table.index)\n",
    "        return {'train': master_table.loc[self.train_start:self.train_end], 'val': master_table.loc[self.val_start:self.val_end]}\n",
    "    def prepare_features_labels(self, data):\n",
    "        if data.empty:\n",
    "            return pd.DataFrame(), pd.Series()\n",
    "        numeric_data = data.select_dtypes(include=[np.number])\n",
    "        if numeric_data.empty:\n",
    "            return pd.DataFrame(), pd.Series()\n",
    "        exclude_cols = ['mktcap', 'market_cap', 'date', 'year', 'month', 'ticker', 'permno', 'gvkey']\n",
    "        target_cols = ['return', 'returns', 'ret', 'target', 'y', 'next_return', 'forward_return', 'ret_1', 'ret_1m', 'ret_12m', 'future_return', 'returns_1d']\n",
    "        target_col = None\n",
    "        for tc in target_cols:\n",
    "            for col in numeric_data.columns:\n",
    "                if tc.lower() in col.lower() and col.lower() not in [ec.lower() for ec in exclude_cols]:\n",
    "                    target_col = col\n",
    "                    break\n",
    "            if target_col:\n",
    "                break\n",
    "        if target_col is None:\n",
    "            potential = [c for c in numeric_data.columns if c.lower() not in [ec.lower() for ec in exclude_cols]]\n",
    "            target_col = potential[-2] if len(potential) > 1 else (potential[-1] if potential else numeric_data.columns[-1])\n",
    "        feature_cols = [c for c in numeric_data.columns if c != target_col and c.lower() not in [ec.lower() for ec in exclude_cols]]\n",
    "        if not feature_cols:\n",
    "            feature_cols = [c for c in numeric_data.columns if c != target_col]\n",
    "        if not feature_cols:\n",
    "            feature_cols = numeric_data.columns[:-1].tolist()\n",
    "            target_col = numeric_data.columns[-1]\n",
    "        return numeric_data[feature_cols], numeric_data[target_col]\n",
    "splitter = CrossSectionalDataSplitter()\n",
    "data_splits = splitter.split(df)\n",
    "train_df, val_df = data_splits['train'], data_splits['val']\n",
    "X_train_df, y_train = splitter.prepare_features_labels(train_df)\n",
    "X_val_df, y_val = splitter.prepare_features_labels(val_df)\n",
    "X_train = X_train_df.fillna(0).values.astype(np.float32)\n",
    "y_train = y_train.fillna(0).values.astype(np.float32)\n",
    "X_val = X_val_df.fillna(0).values.astype(np.float32)\n",
    "y_val = y_val.fillna(0).values.astype(np.float32)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "print(f'Loaded fresh data: train {X_train_scaled.shape[0]}, val {X_val_scaled.shape[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec831d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = n_epochs = num_epochs = 100  # full training (standalone mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f3c3c03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T05:33:30.417903Z",
     "iopub.status.busy": "2026-01-23T05:33:30.417713Z",
     "iopub.status.idle": "2026-01-23T05:33:30.422633Z",
     "shell.execute_reply": "2026-01-23T05:33:30.422291Z"
    },
    "papermill": {
     "duration": 0.016146,
     "end_time": "2026-01-23T05:33:30.423532",
     "exception": false,
     "start_time": "2026-01-23T05:33:30.407386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adversarial Attack Implementations (A1-A4)\n",
    "def apply_a1_attack(X, epsilon=0.01):\n",
    "    \"\"\"A1: Measurement Error - bounded perturbations.\"\"\"\n",
    "    noise = np.random.normal(0, epsilon, X.shape)\n",
    "    # Scale noise by feature standard deviation\n",
    "    feature_std = np.std(X, axis=0, keepdims=True) + 1e-8\n",
    "    noise = noise * feature_std\n",
    "    return X + noise\n",
    "\n",
    "\n",
    "def apply_a2_attack(X, missing_rate=0.1):\n",
    "    \"\"\"A2: Missingness/Staleness - set random features to zero.\"\"\"\n",
    "    X_adv = X.copy()\n",
    "    n_samples, n_features = X.shape\n",
    "    n_missing = int(n_features * missing_rate)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        missing_indices = np.random.choice(n_features, n_missing, replace=False)\n",
    "        X_adv[i, missing_indices] = 0.0\n",
    "    \n",
    "    return X_adv\n",
    "\n",
    "\n",
    "def apply_a3_attack(X, epsilon=0.01):\n",
    "    \"\"\"A3: Rank Manipulation - cross-sectional perturbation preserving ranks.\"\"\"\n",
    "    X_adv = X.copy()\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # Add small random perturbation that preserves relative ordering\n",
    "    for i in range(n_samples):\n",
    "        perturbation = np.random.normal(0, epsilon, X.shape[1])\n",
    "        # Scale by feature std to maintain relative magnitudes\n",
    "        feature_std = np.std(X[i], axis=0) + 1e-8\n",
    "        perturbation = perturbation * feature_std\n",
    "        X_adv[i] = X[i] + perturbation\n",
    "    \n",
    "    return X_adv\n",
    "\n",
    "\n",
    "def apply_a4_attack(X, epsilon=1.0):\n",
    "    \"\"\"A4: Regime Shift - distribution shift attack.\"\"\"\n",
    "    # A4 simulates regime shift by scaling volatility\n",
    "    # epsilon acts as volatility multiplier\n",
    "    X_adv = X.copy()\n",
    "    feature_std = np.std(X, axis=0, keepdims=True) + 1e-8\n",
    "    # Generate noise with std = epsilon, then scale by feature std\n",
    "    noise = np.random.normal(0, epsilon, X.shape) * feature_std\n",
    "    X_adv = X + noise\n",
    "    return X_adv\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec15b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap Confidence Intervals for R² and Robustness Metrics\n",
    "from scipy import stats\n",
    "\n",
    "def bootstrap_confidence_interval(data, n_bootstrap=1000, confidence=0.95, method='percentile'):\n",
    "    \"\"\"\n",
    "    Compute bootstrap confidence interval for a metric.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        Sample data\n",
    "    n_bootstrap : int\n",
    "        Number of bootstrap samples\n",
    "    confidence : float\n",
    "        Confidence level (e.g., 0.95 for 95% CI)\n",
    "    method : str\n",
    "        'percentile' or 'bca' (bias-corrected and accelerated)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    mean : float\n",
    "        Mean of the data\n",
    "    std : float\n",
    "        Standard error (standard deviation)\n",
    "    ci_lower : float\n",
    "        Lower bound of confidence interval\n",
    "    ci_upper : float\n",
    "        Upper bound of confidence interval\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    n = len(data)\n",
    "    alpha = 1 - confidence\n",
    "    \n",
    "    # Bootstrap samples\n",
    "    bootstrap_samples = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        # Resample with replacement\n",
    "        indices = np.random.choice(n, size=n, replace=True)\n",
    "        bootstrap_samples.append(np.mean(data[indices]))\n",
    "    \n",
    "    bootstrap_samples = np.array(bootstrap_samples)\n",
    "    \n",
    "    # Compute statistics\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(bootstrap_samples)  # Standard error\n",
    "    \n",
    "    if method == 'percentile':\n",
    "        ci_lower = np.percentile(bootstrap_samples, 100 * alpha / 2)\n",
    "        ci_upper = np.percentile(bootstrap_samples, 100 * (1 - alpha / 2))\n",
    "    else:  # bca method\n",
    "        # Bias-corrected and accelerated bootstrap\n",
    "        z0 = stats.norm.ppf(np.mean(bootstrap_samples < mean))\n",
    "        # Acceleration (simplified - using jackknife)\n",
    "        jackknife_means = []\n",
    "        for i in range(n):\n",
    "            jackknife_data = np.delete(data, i)\n",
    "            jackknife_means.append(np.mean(jackknife_data))\n",
    "        jackknife_means = np.array(jackknife_means)\n",
    "        a = np.sum((np.mean(jackknife_means) - jackknife_means)**3) / (6 * np.sum((np.mean(jackknife_means) - jackknife_means)**2)**1.5)\n",
    "        \n",
    "        # BCa adjustment\n",
    "        z_alpha_lower = stats.norm.ppf(alpha / 2)\n",
    "        z_alpha_upper = stats.norm.ppf(1 - alpha / 2)\n",
    "        z_lower = z0 + (z0 + z_alpha_lower) / (1 - a * (z0 + z_alpha_lower))\n",
    "        z_upper = z0 + (z0 + z_alpha_upper) / (1 - a * (z0 + z_alpha_upper))\n",
    "        \n",
    "        ci_lower = np.percentile(bootstrap_samples, 100 * stats.norm.cdf(z_lower))\n",
    "        ci_upper = np.percentile(bootstrap_samples, 100 * stats.norm.cdf(z_upper))\n",
    "    \n",
    "    return {\n",
    "        'mean': mean,\n",
    "        'std': std,\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "\n",
    "def compute_r2_with_ci(y_true, y_pred, n_bootstrap=1000, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Compute R² with bootstrap confidence interval.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True target values\n",
    "    y_pred : array-like\n",
    "        Predicted values\n",
    "    n_bootstrap : int\n",
    "        Number of bootstrap samples\n",
    "    confidence : float\n",
    "        Confidence level\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with r2, std, ci_lower, ci_upper\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    n = len(y_true)\n",
    "    \n",
    "    # Original R²\n",
    "    r2_original = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Bootstrap R² values\n",
    "    r2_bootstrap = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        indices = np.random.choice(n, size=n, replace=True)\n",
    "        y_true_boot = y_true[indices]\n",
    "        y_pred_boot = y_pred[indices]\n",
    "        r2_boot = r2_score(y_true_boot, y_pred_boot)\n",
    "        r2_bootstrap.append(r2_boot)\n",
    "    \n",
    "    r2_bootstrap = np.array(r2_bootstrap)\n",
    "    \n",
    "    # Compute statistics\n",
    "    std = np.std(r2_bootstrap)\n",
    "    alpha = 1 - confidence\n",
    "    ci_lower = np.percentile(r2_bootstrap, 100 * alpha / 2)\n",
    "    ci_upper = np.percentile(r2_bootstrap, 100 * (1 - alpha / 2))\n",
    "    \n",
    "    return {\n",
    "        'r2': r2_original,\n",
    "        'std': std,\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "\n",
    "print(\"✅ Bootstrap confidence interval functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6022c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced evaluation function with confidence intervals\n",
    "def evaluate_model_under_attack_with_ci(model, model_name, X_val, y_val, attack_type, epsilon, \n",
    "                                        device='cpu', is_sklearn=False, num_runs=5, n_bootstrap=1000):\n",
    "    \"\"\"\n",
    "    Evaluate a model under a specific attack with bootstrap confidence intervals.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with metrics including confidence intervals for robustness\n",
    "    \"\"\"\n",
    "    # Set model to eval mode\n",
    "    if not is_sklearn:\n",
    "        model.eval()\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, nn.Dropout):\n",
    "                module.eval()\n",
    "    \n",
    "    # Make clean predictions\n",
    "    if is_sklearn:\n",
    "        y_pred_clean = model.predict(X_val)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X_val).to(device)\n",
    "            output = model(X_tensor)\n",
    "            if isinstance(output, tuple):\n",
    "                y_pred_tensor = output[0]\n",
    "            else:\n",
    "                y_pred_tensor = output\n",
    "            y_pred_clean = y_pred_tensor.cpu().numpy().flatten()\n",
    "    \n",
    "    # Calculate clean RMSE and R² with CI\n",
    "    clean_rmse = np.sqrt(mean_squared_error(y_val, y_pred_clean))\n",
    "    r2_stats = compute_r2_with_ci(y_val, y_pred_clean, n_bootstrap=n_bootstrap)\n",
    "    \n",
    "    # Run attack multiple times and collect robustness values\n",
    "    robustness_values = []\n",
    "    adv_rmses = []\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        # Apply attack\n",
    "        if attack_type == 'a1':\n",
    "            X_adv = apply_a1_attack(X_val, epsilon=epsilon)\n",
    "        elif attack_type == 'a2':\n",
    "            missing_rate = min(epsilon / 10.0, 0.8)\n",
    "            X_adv = apply_a2_attack(X_val, missing_rate=missing_rate)\n",
    "        elif attack_type == 'a3':\n",
    "            X_adv = apply_a3_attack(X_val, epsilon=epsilon)\n",
    "        elif attack_type == 'a4':\n",
    "            X_adv = apply_a4_attack(X_val, epsilon=epsilon)\n",
    "        else:\n",
    "            X_adv = X_val.copy()\n",
    "        \n",
    "        # Make adversarial predictions\n",
    "        if is_sklearn:\n",
    "            y_pred_adv = model.predict(X_adv)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                X_adv_tensor = torch.FloatTensor(X_adv).to(device)\n",
    "                output_adv = model(X_adv_tensor)\n",
    "                if isinstance(output_adv, tuple):\n",
    "                    y_pred_adv_tensor = output_adv[0]\n",
    "                else:\n",
    "                    y_pred_adv_tensor = output_adv\n",
    "                y_pred_adv = y_pred_adv_tensor.cpu().numpy().flatten()\n",
    "        \n",
    "        # Calculate adversarial RMSE\n",
    "        adv_rmse = np.sqrt(mean_squared_error(y_val, y_pred_adv))\n",
    "        adv_rmses.append(adv_rmse)\n",
    "        \n",
    "        # Calculate robustness for this run\n",
    "        delta_rmse = adv_rmse - clean_rmse\n",
    "        if clean_rmse > 0:\n",
    "            robustness = min(1.0, 1.0 - (delta_rmse / clean_rmse))\n",
    "        else:\n",
    "            robustness = 1.0\n",
    "        robustness_values.append(robustness)\n",
    "    \n",
    "    # Average across runs\n",
    "    avg_adv_rmse = np.mean(adv_rmses)\n",
    "    delta_rmse = avg_adv_rmse - clean_rmse\n",
    "    avg_robustness = np.mean(robustness_values)\n",
    "    \n",
    "    # Compute robustness confidence interval\n",
    "    robustness_ci = bootstrap_confidence_interval(\n",
    "        robustness_values, n_bootstrap=n_bootstrap, confidence=0.95\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'clean_rmse': clean_rmse,\n",
    "        'adv_rmse': avg_adv_rmse,\n",
    "        'delta_rmse': delta_rmse,\n",
    "        'robustness': avg_robustness,\n",
    "        'robustness_std': robustness_ci['std'],\n",
    "        'robustness_ci_lower': robustness_ci['ci_lower'],\n",
    "        'robustness_ci_upper': robustness_ci['ci_upper'],\n",
    "        'r2': r2_stats['r2'],\n",
    "        'r2_std': r2_stats['std'],\n",
    "        'r2_ci_lower': r2_stats['ci_lower'],\n",
    "        'r2_ci_upper': r2_stats['ci_upper']\n",
    "    }\n",
    "\n",
    "print(\"✅ Enhanced evaluation function with confidence intervals loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0fb92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Confidence Intervals for R² and Robustness Metrics\n",
    "# R² CI is computed whenever models + validation data exist (end-to-end safe).\n",
    "# Robustness CI is computed when robustness_df exists (from evaluation cell).\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPUTING CONFIDENCE INTERVALS FOR R² AND ROBUSTNESS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "r2_ci_results = {}\n",
    "robustness_ci_df = pd.DataFrame()\n",
    "robustness_by_attack_ci_df = pd.DataFrame()\n",
    "\n",
    "# --- Part 1: Always compute R² confidence intervals when models + val data exist ---\n",
    "if 'models' in locals() and 'X_val_scaled' in locals() and 'y_val' in locals() and len(models) > 0:\n",
    "    print(\"\\nComputing R² confidence intervals for standard models...\")\n",
    "    _device = device if 'device' in locals() else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    for model_name in ['OLS', 'Ridge', 'XGBoost', 'MLP', 'Single-Head', 'Multi-Head', 'Multi-Head Diversity']:\n",
    "        if model_name in models:\n",
    "            try:\n",
    "                model = models[model_name]\n",
    "                is_sklearn = model_name in ['OLS', 'Ridge', 'XGBoost']\n",
    "                if is_sklearn:\n",
    "                    y_pred = model.predict(X_val_scaled)\n",
    "                else:\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        X_tensor = torch.FloatTensor(X_val_scaled).to(_device)\n",
    "                        output = model(X_tensor)\n",
    "                        y_pred = (output[0] if isinstance(output, tuple) else output).cpu().numpy().flatten()\n",
    "                r2_stats = compute_r2_with_ci(y_val, y_pred, n_bootstrap=1000, confidence=0.95)\n",
    "                r2_ci_results[model_name] = r2_stats\n",
    "                print(f\"  {model_name}: R² = {r2_stats['r2']:.6f} [95% CI: {r2_stats['ci_lower']:.6f}, {r2_stats['ci_upper']:.6f}], SE = {r2_stats['std']:.6f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠ Error computing R² CI for {model_name}: {e}\")\n",
    "    print(f\"\\n✓ R² CIs computed for {len(r2_ci_results)} models\")\n",
    "else:\n",
    "    print(\"\\n⚠ Skipping R² CI: need models, X_val_scaled, and y_val. Run data load + training cells first.\")\n",
    "\n",
    "# --- Part 2: Compute robustness CIs when robustness_df exists (from evaluation cell) ---\n",
    "if 'robustness_df' in locals() and len(robustness_df) > 0:\n",
    "    print(\"\\nComputing robustness confidence intervals...\")\n",
    "    robustness_ci_summary = []\n",
    "    for model_name in robustness_df['model_name'].unique():\n",
    "        model_data = robustness_df[robustness_df['model_name'] == model_name]\n",
    "        if len(model_data) > 0:\n",
    "            robustness_values = model_data['robustness'].values\n",
    "            robustness_ci = bootstrap_confidence_interval(robustness_values, n_bootstrap=1000, confidence=0.95)\n",
    "            robustness_ci_summary.append({\n",
    "                'model_name': model_name, 'mean_robustness': robustness_ci['mean'], 'robustness_std': robustness_ci['std'],\n",
    "                'robustness_ci_lower': robustness_ci['ci_lower'], 'robustness_ci_upper': robustness_ci['ci_upper'], 'n_evaluations': len(model_data)\n",
    "            })\n",
    "            print(f\"  {model_name}: Robustness = {robustness_ci['mean']:.4f} [95% CI: {robustness_ci['ci_lower']:.4f}, {robustness_ci['ci_upper']:.4f}], SE = {robustness_ci['std']:.4f} (n={len(model_data)})\")\n",
    "    robustness_ci_df = pd.DataFrame(robustness_ci_summary)\n",
    "    print(\"\\nComputing robustness CIs by attack type and epsilon...\")\n",
    "    robustness_by_attack_ci = []\n",
    "    for model_name in robustness_df['model_name'].unique():\n",
    "        for attack_type in robustness_df['attack_type'].unique():\n",
    "            for epsilon in robustness_df['epsilon'].unique():\n",
    "                subset = robustness_df[(robustness_df['model_name'] == model_name) & (robustness_df['attack_type'] == attack_type) & (robustness_df['epsilon'] == epsilon)]\n",
    "                if len(subset) > 0:\n",
    "                    rci = bootstrap_confidence_interval(subset['robustness'].values, n_bootstrap=1000, confidence=0.95)\n",
    "                    robustness_by_attack_ci.append({'model_name': model_name, 'attack_type': attack_type, 'epsilon': epsilon, 'mean_robustness': rci['mean'], 'robustness_std': rci['std'], 'robustness_ci_lower': rci['ci_lower'], 'robustness_ci_upper': rci['ci_upper']})\n",
    "    robustness_by_attack_ci_df = pd.DataFrame(robustness_by_attack_ci)\n",
    "    print(f\"✓ Robustness CIs: {len(robustness_ci_summary)} models, {len(robustness_by_attack_ci)} attack/epsilon combinations\")\n",
    "else:\n",
    "    print(\"\\n⚠ No robustness_df (or empty). Run 'Evaluate Existing Models Under Adversarial Attacks' for robustness CIs.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONFIDENCE INTERVALS COMPUTED\")\n",
    "print(\"=\" * 80)\n",
    "print(\"  - r2_ci_results: R² statistics with 95% CI\")\n",
    "print(\"  - robustness_ci_df: model-level robustness CIs (if evaluation was run)\")\n",
    "print(\"  - robustness_by_attack_ci_df: attack/epsilon-level robustness CIs (if evaluation was run)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a85608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Summary Table with Confidence Intervals\n",
    "# Runs when r2_ci_results exists (from CI cell). Robustness columns N/A if evaluation was skipped.\n",
    "\n",
    "if 'r2_ci_results' in locals() and len(r2_ci_results) > 0:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"SUMMARY TABLE: R² AND ROBUSTNESS WITH CONFIDENCE INTERVALS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    summary_data = []\n",
    "    _robustness_ci_df = robustness_ci_df if 'robustness_ci_df' in locals() and len(robustness_ci_df) > 0 else pd.DataFrame()\n",
    "    \n",
    "    for model_name in ['OLS', 'Ridge', 'XGBoost', 'MLP', 'Single-Head', 'Multi-Head', 'Multi-Head Diversity']:\n",
    "        row = {'Model': model_name}\n",
    "        if model_name in r2_ci_results:\n",
    "            r2_stats = r2_ci_results[model_name]\n",
    "            row['R²'] = f\"{r2_stats['r2']:.6f}\"\n",
    "            row['R²_SE'] = f\"{r2_stats['std']:.6f}\"\n",
    "            row['R²_CI_95'] = f\"[{r2_stats['ci_lower']:.6f}, {r2_stats['ci_upper']:.6f}]\"\n",
    "        else:\n",
    "            row['R²'] = row['R²_SE'] = row['R²_CI_95'] = \"N/A\"\n",
    "        \n",
    "        if len(_robustness_ci_df) > 0:\n",
    "            model_robustness = _robustness_ci_df[_robustness_ci_df['model_name'] == model_name]\n",
    "            if len(model_robustness) > 0:\n",
    "                rob_stats = model_robustness.iloc[0]\n",
    "                row['Robustness'] = f\"{rob_stats['mean_robustness']:.4f}\"\n",
    "                row['Robustness_SE'] = f\"{rob_stats['robustness_std']:.4f}\"\n",
    "                row['Robustness_CI_95'] = f\"[{rob_stats['robustness_ci_lower']:.4f}, {rob_stats['robustness_ci_upper']:.4f}]\"\n",
    "                row['N_Evaluations'] = int(rob_stats['n_evaluations'])\n",
    "            else:\n",
    "                row['Robustness'] = row['Robustness_SE'] = row['Robustness_CI_95'] = \"N/A\"; row['N_Evaluations'] = 0\n",
    "        else:\n",
    "            row['Robustness'] = row['Robustness_SE'] = row['Robustness_CI_95'] = \"N/A\"; row['N_Evaluations'] = 0\n",
    "        summary_data.append(row)\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(\"\\n\" + summary_df.to_string(index=False))\n",
    "    \n",
    "    if 'tables_dir' in locals():\n",
    "        ci_summary_path = tables_dir / 'model_performance_with_ci.csv'\n",
    "        summary_df.to_csv(ci_summary_path, index=False)\n",
    "        print(f\"\\n✓ Summary table saved to: {ci_summary_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"LaTeX TABLE: MODEL PERFORMANCE WITH CONFIDENCE INTERVALS\")\n",
    "    print(\"=\" * 80)\n",
    "    latex_table = \"\"\"\\\\begin{table}[h]\n",
    "\\\\centering\n",
    "\\\\caption{Model Performance with 95\\\\% Confidence Intervals}\n",
    "\\\\label{tab:model_performance_ci}\n",
    "\\\\footnotesize\n",
    "\\\\begin{tabular}{lcccccc}\n",
    "\\\\toprule\n",
    "Model & R² & R² SE & R² 95\\\\% CI & Robustness & Robustness SE & Robustness 95\\\\% CI \\\\\\\\\n",
    "\\\\midrule\n",
    "\"\"\"\n",
    "    for _, row in summary_df.iterrows():\n",
    "        latex_table += f\"{row['Model']} & {row['R²']} & {row['R²_SE']} & {row['R²_CI_95']} & {row['Robustness']} & {row['Robustness_SE']} & {row['Robustness_CI_95']} \\\\\\\\\\\\n\"\n",
    "    latex_table += \"\"\"\\\\bottomrule\n",
    "\\\\end{tabular}\n",
    "\\\\vspace{0.1cm}\n",
    "\\\\footnotesize\n",
    "\\\\begin{minipage}{\\\\columnwidth}\n",
    "\\\\textit{Note: R² and Robustness metrics with 95\\\\% bootstrap confidence intervals (1000 bootstrap samples). SE = Standard Error. Robustness is averaged across all attack types (A1-A4) and epsilon values (0.25, 0.5, 1.0).}\n",
    "\\\\end{minipage}\n",
    "\\\\end{table}\n",
    "\"\"\"\n",
    "    print(latex_table)\n",
    "    if 'tables_dir' in locals():\n",
    "        latex_path = tables_dir / 'model_performance_with_ci.tex'\n",
    "        with open(latex_path, 'w') as f:\n",
    "            f.write(latex_table)\n",
    "        print(f\"\\n✓ LaTeX table saved to: {latex_path}\")\n",
    "else:\n",
    "    print(\"⚠ Run the 'Compute Confidence Intervals for R² and Robustness' cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beff86a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Economically Interpretable Metrics: Spearman IC, Portfolio Returns, Turnover\n",
    "from scipy.stats import spearmanr\n",
    "import pandas as pd\n",
    "\n",
    "def compute_spearman_ic(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute Spearman rank Information Coefficient (IC).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True returns\n",
    "    y_pred : array-like\n",
    "        Predicted returns\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with ic, p_value, ic_ir (IC/Std(IC) if time series)\n",
    "    \"\"\"\n",
    "    # Remove NaN values\n",
    "    mask = ~(np.isnan(y_true) | np.isnan(y_pred))\n",
    "    y_true_clean = np.array(y_true)[mask]\n",
    "    y_pred_clean = np.array(y_pred)[mask]\n",
    "    \n",
    "    if len(y_true_clean) < 2:\n",
    "        return {'ic': np.nan, 'p_value': np.nan, 'ic_ir': np.nan}\n",
    "    \n",
    "    # Compute Spearman correlation\n",
    "    ic, p_value = spearmanr(y_true_clean, y_pred_clean)\n",
    "    \n",
    "    return {\n",
    "        'ic': ic,\n",
    "        'p_value': p_value,\n",
    "        'ic_ir': np.nan  # Will compute IC-IR for time series\n",
    "    }\n",
    "\n",
    "def compute_ic_ir(ic_series):\n",
    "    \"\"\"\n",
    "    Compute IC Information Ratio (mean IC / std IC).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ic_series : array-like\n",
    "        Time series of IC values\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float: IC-IR\n",
    "    \"\"\"\n",
    "    ic_series = np.array(ic_series)\n",
    "    ic_series = ic_series[~np.isnan(ic_series)]\n",
    "    \n",
    "    if len(ic_series) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    mean_ic = np.mean(ic_series)\n",
    "    std_ic = np.std(ic_series)\n",
    "    \n",
    "    if std_ic == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    return mean_ic / std_ic\n",
    "\n",
    "def construct_long_short_portfolio(predictions, returns, quantile=0.2, equal_weight=True):\n",
    "    \"\"\"\n",
    "    Construct long-short portfolio based on predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : array-like\n",
    "        Model predictions\n",
    "    returns : array-like\n",
    "        Actual returns\n",
    "    quantile : float\n",
    "        Top/bottom quantile for long/short positions (default 0.2 = top/bottom 20%)\n",
    "    equal_weight : bool\n",
    "        If True, equal weight positions; if False, weight by prediction strength\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with portfolio metrics\n",
    "    \"\"\"\n",
    "    predictions = np.array(predictions)\n",
    "    returns = np.array(returns)\n",
    "    \n",
    "    # Remove NaN\n",
    "    mask = ~(np.isnan(predictions) | np.isnan(returns))\n",
    "    predictions = predictions[mask]\n",
    "    returns = returns[mask]\n",
    "    \n",
    "    if len(predictions) == 0:\n",
    "        return {\n",
    "            'long_short_return': np.nan,\n",
    "            'long_return': np.nan,\n",
    "            'short_return': np.nan,\n",
    "            'sharpe_ratio': np.nan,\n",
    "            'n_long': 0,\n",
    "            'n_short': 0\n",
    "        }\n",
    "    \n",
    "    # Determine long and short positions\n",
    "    n = len(predictions)\n",
    "    n_positions = max(1, int(n * quantile))\n",
    "    \n",
    "    # Long: top predictions\n",
    "    long_indices = np.argsort(predictions)[-n_positions:]\n",
    "    # Short: bottom predictions\n",
    "    short_indices = np.argsort(predictions)[:n_positions]\n",
    "    \n",
    "    if equal_weight:\n",
    "        # Equal weights\n",
    "        long_weights = np.ones(len(long_indices)) / len(long_indices)\n",
    "        short_weights = np.ones(len(short_indices)) / len(short_indices)\n",
    "    else:\n",
    "        # Weight by prediction strength (normalized)\n",
    "        long_preds = predictions[long_indices]\n",
    "        short_preds = predictions[short_indices]\n",
    "        long_weights = long_preds / np.sum(long_preds) if np.sum(long_preds) != 0 else np.ones(len(long_indices)) / len(long_indices)\n",
    "        short_weights = -short_preds / np.sum(np.abs(short_preds)) if np.sum(np.abs(short_preds)) != 0 else -np.ones(len(short_indices)) / len(short_indices)\n",
    "    \n",
    "    # Compute returns\n",
    "    long_return = np.sum(long_weights * returns[long_indices])\n",
    "    short_return = np.sum(short_weights * returns[short_indices])\n",
    "    long_short_return = long_return - short_return\n",
    "    \n",
    "    # Compute Sharpe ratio (annualized, assuming monthly returns)\n",
    "    if np.std(returns) > 0:\n",
    "        sharpe_ratio = (long_short_return / np.std(returns)) * np.sqrt(12)  # Annualized\n",
    "    else:\n",
    "        sharpe_ratio = np.nan\n",
    "    \n",
    "    return {\n",
    "        'long_short_return': long_short_return,\n",
    "        'long_return': long_return,\n",
    "        'short_return': short_return,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'n_long': len(long_indices),\n",
    "        'n_short': len(short_indices)\n",
    "    }\n",
    "\n",
    "def compute_turnover(current_weights, previous_weights):\n",
    "    \"\"\"\n",
    "    Compute portfolio turnover.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    current_weights : array-like\n",
    "        Current period portfolio weights\n",
    "    previous_weights : array-like or None\n",
    "        Previous period portfolio weights (None for first period)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float: Turnover (sum of absolute weight changes)\n",
    "    \"\"\"\n",
    "    if previous_weights is None:\n",
    "        # First period: turnover = sum of absolute weights (full rebalancing)\n",
    "        return np.sum(np.abs(current_weights))\n",
    "    \n",
    "    current_weights = np.array(current_weights)\n",
    "    previous_weights = np.array(previous_weights)\n",
    "    \n",
    "    # Align lengths (pad with zeros if needed)\n",
    "    max_len = max(len(current_weights), len(previous_weights))\n",
    "    if len(current_weights) < max_len:\n",
    "        current_weights = np.pad(current_weights, (0, max_len - len(current_weights)), 'constant')\n",
    "    if len(previous_weights) < max_len:\n",
    "        previous_weights = np.pad(previous_weights, (0, max_len - len(previous_weights)), 'constant')\n",
    "    \n",
    "    # Turnover = sum of absolute weight changes\n",
    "    turnover = np.sum(np.abs(current_weights - previous_weights))\n",
    "    \n",
    "    return turnover\n",
    "\n",
    "def compute_portfolio_metrics_time_series(predictions_df, returns_df, dates, quantile=0.2, \n",
    "                                         transaction_cost=0.001):\n",
    "    \"\"\"\n",
    "    Compute portfolio metrics over time series with turnover adjustment.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions_df : DataFrame\n",
    "        Predictions with columns for each model, indexed by date\n",
    "    returns_df : DataFrame\n",
    "        Returns with same index as predictions_df\n",
    "    dates : array-like\n",
    "        Date index\n",
    "    quantile : float\n",
    "        Top/bottom quantile for long/short\n",
    "    transaction_cost : float\n",
    "        Transaction cost per unit of turnover (default 0.1% = 0.001)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with time series metrics for each model\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Group by date if not already grouped\n",
    "    if isinstance(predictions_df, pd.DataFrame):\n",
    "        date_groups = predictions_df.groupby(dates)\n",
    "    else:\n",
    "        # If predictions_df is a Series, create groups manually\n",
    "        date_groups = {}\n",
    "        for date in np.unique(dates):\n",
    "            mask = dates == date\n",
    "            date_groups[date] = mask\n",
    "    \n",
    "    for model_name in predictions_df.columns if isinstance(predictions_df, pd.DataFrame) else ['Model']:\n",
    "        if isinstance(predictions_df, pd.DataFrame):\n",
    "            model_predictions = predictions_df[model_name]\n",
    "        else:\n",
    "            model_predictions = predictions_df\n",
    "        \n",
    "        # Time series metrics\n",
    "        period_returns = []\n",
    "        period_turnovers = []\n",
    "        period_ics = []\n",
    "        previous_weights = None\n",
    "        \n",
    "        for date in sorted(date_groups.keys()):\n",
    "            mask = date_groups[date]\n",
    "            if isinstance(mask, np.ndarray):\n",
    "                date_predictions = model_predictions[mask] if hasattr(model_predictions, '__getitem__') else model_predictions\n",
    "                date_returns = returns_df[mask] if hasattr(returns_df, '__getitem__') else returns_df\n",
    "            else:\n",
    "                date_predictions = model_predictions.loc[mask] if hasattr(model_predictions, 'loc') else model_predictions[mask]\n",
    "                date_returns = returns_df.loc[mask] if hasattr(returns_df, 'loc') else returns_df[mask]\n",
    "            \n",
    "            # Compute IC for this period\n",
    "            ic_result = compute_spearman_ic(date_returns, date_predictions)\n",
    "            period_ics.append(ic_result['ic'])\n",
    "            \n",
    "            # Construct portfolio\n",
    "            portfolio_result = construct_long_short_portfolio(\n",
    "                date_predictions, date_returns, quantile=quantile\n",
    "            )\n",
    "            period_return = portfolio_result['long_short_return']\n",
    "            \n",
    "            # Compute weights for turnover calculation\n",
    "            n = len(date_predictions)\n",
    "            n_positions = max(1, int(n * quantile))\n",
    "            long_indices = np.argsort(date_predictions)[-n_positions:]\n",
    "            short_indices = np.argsort(date_predictions)[:n_positions]\n",
    "            \n",
    "            current_weights = np.zeros(n)\n",
    "            current_weights[long_indices] = 1.0 / n_positions\n",
    "            current_weights[short_indices] = -1.0 / n_positions\n",
    "            \n",
    "            # Compute turnover\n",
    "            turnover = compute_turnover(current_weights, previous_weights)\n",
    "            period_turnovers.append(turnover)\n",
    "            \n",
    "            # Adjust return for transaction costs\n",
    "            net_return = period_return - (turnover * transaction_cost)\n",
    "            period_returns.append(net_return)\n",
    "            \n",
    "            previous_weights = current_weights\n",
    "        \n",
    "        # Aggregate metrics\n",
    "        period_returns = np.array(period_returns)\n",
    "        period_turnovers = np.array(period_turnovers)\n",
    "        period_ics = np.array(period_ics)\n",
    "        \n",
    "        # Total return (cumulative)\n",
    "        cumulative_return = np.prod(1 + period_returns) - 1 if len(period_returns) > 0 else 0\n",
    "        \n",
    "        # Annualized return (assuming monthly periods)\n",
    "        n_periods = len(period_returns)\n",
    "        if n_periods > 0:\n",
    "            annualized_return = (1 + cumulative_return) ** (12 / n_periods) - 1\n",
    "        else:\n",
    "            annualized_return = np.nan\n",
    "        \n",
    "        # Average turnover\n",
    "        avg_turnover = np.mean(period_turnovers) if len(period_turnovers) > 0 else np.nan\n",
    "        \n",
    "        # IC statistics\n",
    "        mean_ic = np.nanmean(period_ics)\n",
    "        ic_ir = compute_ic_ir(period_ics)\n",
    "        \n",
    "        # Sharpe ratio\n",
    "        if len(period_returns) > 0 and np.std(period_returns) > 0:\n",
    "            sharpe_ratio = (np.mean(period_returns) / np.std(period_returns)) * np.sqrt(12)\n",
    "        else:\n",
    "            sharpe_ratio = np.nan\n",
    "        \n",
    "        results[model_name] = {\n",
    "            'cumulative_return': cumulative_return,\n",
    "            'annualized_return': annualized_return,\n",
    "            'mean_ic': mean_ic,\n",
    "            'ic_ir': ic_ir,\n",
    "            'sharpe_ratio': sharpe_ratio,\n",
    "            'avg_turnover': avg_turnover,\n",
    "            'period_returns': period_returns,\n",
    "            'period_ics': period_ics,\n",
    "            'period_turnovers': period_turnovers\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✅ Economically interpretable metrics functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e764f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display business-related metrics for all baseline models (including XGBoost)\n",
    "# Requires: predictions, y_val from \"Make predictions on validation set\" cell\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BUSINESS-RELATED METRICS: IC, Long-Short Return, Sharpe Ratio\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "BASELINE_MODELS = ['OLS', 'Ridge', 'XGBoost', 'MLP', 'Single-Head', 'Multi-Head', 'Multi-Head Diversity']\n",
    "business_metrics_rows = []\n",
    "\n",
    "if 'predictions' in locals() and 'y_val' in locals():\n",
    "    y_val_arr = y_val.values if hasattr(y_val, 'values') else np.array(y_val)\n",
    "    for model_name in BASELINE_MODELS:\n",
    "        if model_name not in predictions:\n",
    "            continue\n",
    "        pred = np.array(predictions[model_name]).flatten()\n",
    "        # Spearman IC\n",
    "        ic_result = compute_spearman_ic(y_val_arr, pred)\n",
    "        # Long-short portfolio (cross-sectional)\n",
    "        port_result = construct_long_short_portfolio(pred, y_val_arr, quantile=0.2, equal_weight=True)\n",
    "        business_metrics_rows.append({\n",
    "            'Model': model_name,\n",
    "            'Spearman IC': round(ic_result['ic'], 6),\n",
    "            'IC p-value': round(ic_result['p_value'], 6),\n",
    "            'Long-Short Return': round(port_result['long_short_return'], 6),\n",
    "            'Sharpe Ratio': round(port_result['sharpe_ratio'], 4) if not np.isnan(port_result['sharpe_ratio']) else np.nan,\n",
    "        })\n",
    "    business_metrics_df = pd.DataFrame(business_metrics_rows)\n",
    "    print(business_metrics_df.to_string(index=False))\n",
    "    if 'tables_dir' in locals():\n",
    "        path = tables_dir / 'business_metrics_baselines.csv'\n",
    "        business_metrics_df.to_csv(path, index=False)\n",
    "        print(f\"\\n✓ Saved to {path}\")\n",
    "else:\n",
    "    print(\"⚠ Run 'Make predictions on validation set' first so predictions and y_val exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8b25cd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T05:33:30.444111Z",
     "iopub.status.busy": "2026-01-23T05:33:30.443940Z",
     "iopub.status.idle": "2026-01-23T05:33:30.446664Z",
     "shell.execute_reply": "2026-01-23T05:33:30.446312Z"
    },
    "papermill": {
     "duration": 0.014163,
     "end_time": "2026-01-23T05:33:30.447477",
     "exception": false,
     "start_time": "2026-01-23T05:33:30.433314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adversarial Training Configuration\n",
    "ADVERSARIAL_CONFIG = {\n",
    "    'epsilons': [0.25, 0.5, 1.0],  # Attack strengths\n",
    "    'attacks': ['a1', 'a2', 'a3', 'a4'],  # Attack types\n",
    "    'robust_weight': 0.3,  # Weight for adversarial loss (0.3 = 30% adversarial, 70% clean)\n",
    "    'learning_rate': 0.0001,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 100,\n",
    "    'patience': 20,\n",
    "    'warmup_epochs': 5  # Gradually increase adversarial weight\n",
    "}\n",
    "\n",
    "# Store adversarially trained models\n",
    "adversarial_models = {}\n",
    "adversarial_training_history = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "903542c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T05:33:30.476286Z",
     "iopub.status.busy": "2026-01-23T05:33:30.476125Z",
     "iopub.status.idle": "2026-01-23T05:33:30.482515Z",
     "shell.execute_reply": "2026-01-23T05:33:30.482162Z"
    },
    "papermill": {
     "duration": 0.018077,
     "end_time": "2026-01-23T05:33:30.483669",
     "exception": false,
     "start_time": "2026-01-23T05:33:30.465592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adversarial_training_step(model, X_batch, y_batch, attack_type, epsilon, \n",
    "                             optimizer, device='cpu', robust_weight=0.3):\n",
    "    \"\"\"\n",
    "    Perform one adversarial training step.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to train\n",
    "        X_batch: Input batch (numpy array)\n",
    "        y_batch: Target batch (numpy array)\n",
    "        attack_type: 'a1', 'a2', 'a3', or 'a4'\n",
    "        epsilon: Attack strength\n",
    "        optimizer: Optimizer\n",
    "        device: Device to use\n",
    "        robust_weight: Weight for adversarial loss\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with loss values or None if batch is invalid\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_tensor = torch.FloatTensor(X_batch).to(device)\n",
    "    y_tensor = torch.FloatTensor(y_batch).to(device)\n",
    "    \n",
    "    # Clean forward pass\n",
    "    output_clean = model(X_tensor)\n",
    "    if isinstance(output_clean, tuple):\n",
    "        y_pred_clean = output_clean[0]\n",
    "    else:\n",
    "        y_pred_clean = output_clean\n",
    "    \n",
    "    # Check for NaN/Inf in predictions\n",
    "    if torch.any(torch.isnan(y_pred_clean)) or torch.any(torch.isinf(y_pred_clean)):\n",
    "        return None\n",
    "    \n",
    "    clean_loss = nn.MSELoss()(y_pred_clean.squeeze(), y_tensor)\n",
    "    \n",
    "    # Check if clean_loss is valid\n",
    "    if torch.isnan(clean_loss) or torch.isinf(clean_loss):\n",
    "        return None\n",
    "    \n",
    "    # Generate adversarial examples\n",
    "    if attack_type == 'a1':\n",
    "        X_adv = apply_a1_attack(X_batch, epsilon=epsilon)\n",
    "    elif attack_type == 'a2':\n",
    "        # For A2, epsilon controls missing rate\n",
    "        missing_rate = min(epsilon / 10.0, 0.8)  # Convert epsilon to missing rate\n",
    "        X_adv = apply_a2_attack(X_batch, missing_rate=missing_rate)\n",
    "    elif attack_type == 'a3':\n",
    "        X_adv = apply_a3_attack(X_batch, epsilon=epsilon)\n",
    "    elif attack_type == 'a4':\n",
    "        X_adv = apply_a4_attack(X_batch, epsilon=epsilon)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown attack type: {attack_type}\")\n",
    "    \n",
    "    # Adversarial forward pass\n",
    "    X_adv_tensor = torch.FloatTensor(X_adv).to(device)\n",
    "    output_adv = model(X_adv_tensor)\n",
    "    if isinstance(output_adv, tuple):\n",
    "        y_pred_adv = output_adv[0]\n",
    "    else:\n",
    "        y_pred_adv = output_adv\n",
    "    \n",
    "    # Check for NaN/Inf in adversarial predictions\n",
    "    if torch.any(torch.isnan(y_pred_adv)) or torch.any(torch.isinf(y_pred_adv)):\n",
    "        return None\n",
    "    \n",
    "    adv_loss = nn.MSELoss()(y_pred_adv.squeeze(), y_tensor)\n",
    "    \n",
    "    # Check if adv_loss is valid\n",
    "    if torch.isnan(adv_loss) or torch.isinf(adv_loss):\n",
    "        return None\n",
    "    \n",
    "    # Combined loss\n",
    "    total_loss = (1 - robust_weight) * clean_loss + robust_weight * adv_loss\n",
    "    \n",
    "    # Check if total_loss is valid before backward pass\n",
    "    if torch.isnan(total_loss) or torch.isinf(total_loss):\n",
    "        return None\n",
    "    \n",
    "    # Ensure total_loss requires gradients\n",
    "    if not total_loss.requires_grad:\n",
    "        return None\n",
    "    \n",
    "    # Backward pass with error handling\n",
    "    try:\n",
    "        total_loss.backward()\n",
    "    except RuntimeError as e:\n",
    "        if \"does not require grad\" in str(e) or \"does not have a grad_fn\" in str(e):\n",
    "            optimizer.zero_grad()\n",
    "            return None\n",
    "        else:\n",
    "            raise\n",
    "    \n",
    "    # Gradient clipping to prevent exploding gradients\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    return {\n",
    "        'clean_loss': clean_loss.item(),\n",
    "        'adversarial_loss': adv_loss.item(),\n",
    "        'total_loss': total_loss.item()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4b3e578",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T05:33:30.505878Z",
     "iopub.status.busy": "2026-01-23T05:33:30.505718Z",
     "iopub.status.idle": "2026-01-23T05:33:30.517488Z",
     "shell.execute_reply": "2026-01-23T05:33:30.517151Z"
    },
    "papermill": {
     "duration": 0.024209,
     "end_time": "2026-01-23T05:33:30.518558",
     "exception": false,
     "start_time": "2026-01-23T05:33:30.494349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_adversarial_model(model, model_name, X_train, y_train, X_val, y_val, \n",
    "                           attack_type, epsilon, config, device='cpu'):\n",
    "    \"\"\"\n",
    "    Train model with adversarial training.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to train (will be copied)\n",
    "        model_name: Name of the model\n",
    "        X_train: Training features\n",
    "        y_train: Training targets\n",
    "        X_val: Validation features\n",
    "        y_val: Validation targets\n",
    "        attack_type: 'a1', 'a2', 'a3', or 'a4'\n",
    "        epsilon: Attack strength\n",
    "        config: Training configuration\n",
    "        device: Device to use\n",
    "    \n",
    "    Returns:\n",
    "        Trained model, predictions, and training history\n",
    "    \"\"\"\n",
    "    # Create a fresh copy of the model for adversarial training\n",
    "    import copy\n",
    "    model = copy.deepcopy(model)\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "    X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "    y_val_tensor = torch.FloatTensor(y_val).to(device)\n",
    "    \n",
    "    # Handle feature dimension mismatch\n",
    "    num_features = model.num_features if hasattr(model, 'num_features') else model.model.num_features\n",
    "    \n",
    "    if X_train.shape[1] != num_features:\n",
    "        if X_train.shape[1] < num_features:\n",
    "            # Pad\n",
    "            padding_train = np.zeros((X_train.shape[0], num_features - X_train.shape[1]))\n",
    "            padding_val = np.zeros((X_val.shape[0], num_features - X_val.shape[1]))\n",
    "            X_train_tensor = torch.FloatTensor(np.hstack([X_train, padding_train])).to(device)\n",
    "            X_val_tensor = torch.FloatTensor(np.hstack([X_val, padding_val])).to(device)\n",
    "        else:\n",
    "            # Truncate\n",
    "            X_train_tensor = torch.FloatTensor(X_train[:, :num_features]).to(device)\n",
    "            X_val_tensor = torch.FloatTensor(X_val[:, :num_features]).to(device)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_clean_loss': [],\n",
    "        'train_adv_loss': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    warmup_epochs = config.get('warmup_epochs', 5)\n",
    "    \n",
    "    batch_size = config['batch_size']\n",
    "    n_batches = (len(X_train_tensor) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        # Gradual warmup: increase robust_weight from 0.1 to target value\n",
    "        if epoch < warmup_epochs:\n",
    "            current_robust_weight = 0.1 + (config['robust_weight'] - 0.1) * (epoch / warmup_epochs)\n",
    "        else:\n",
    "            current_robust_weight = config['robust_weight']\n",
    "        \n",
    "        epoch_losses = {'clean': [], 'adv': [], 'total': []}\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        for i in range(0, len(X_train_tensor), batch_size):\n",
    "            batch_X = X_train_tensor[i:i+batch_size].cpu().numpy()\n",
    "            batch_y = y_train_tensor[i:i+batch_size].cpu().numpy()\n",
    "            \n",
    "            losses = adversarial_training_step(\n",
    "                model, batch_X, batch_y, attack_type, epsilon,\n",
    "                optimizer, device, current_robust_weight\n",
    "            )\n",
    "            \n",
    "            # Skip batch if None (invalid batch)\n",
    "            if losses is None:\n",
    "                continue\n",
    "            \n",
    "            # Check for NaN/Inf in losses\n",
    "            if (np.isnan(losses['total_loss']) or np.isinf(losses['total_loss']) or\n",
    "                np.isnan(losses['clean_loss']) or np.isinf(losses['clean_loss']) or\n",
    "                np.isnan(losses['adversarial_loss']) or np.isinf(losses['adversarial_loss'])):\n",
    "                continue\n",
    "            \n",
    "            epoch_losses['clean'].append(losses['clean_loss'])\n",
    "            epoch_losses['adv'].append(losses['adversarial_loss'])\n",
    "            epoch_losses['total'].append(losses['total_loss'])\n",
    "        \n",
    "        # Skip epoch if all losses are invalid\n",
    "        if len(epoch_losses['total']) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output_val = model(X_val_tensor)\n",
    "            if isinstance(output_val, tuple):\n",
    "                y_pred_val = output_val[0]\n",
    "            else:\n",
    "                y_pred_val = output_val\n",
    "            \n",
    "            # Check for constant predictions (model collapse detection)\n",
    "            y_pred_np = y_pred_val.squeeze().cpu().numpy()\n",
    "            pred_std = np.std(y_pred_np)\n",
    "            \n",
    "            if pred_std < 1e-8:\n",
    "                print(f\"   MODEL COLLAPSE DETECTED at epoch {epoch+1}!\")\n",
    "                break\n",
    "            \n",
    "            val_loss = nn.MSELoss()(y_pred_val.squeeze(), y_val_tensor).item()\n",
    "            \n",
    "            # Check for NaN/Inf in validation loss\n",
    "            if np.isnan(val_loss) or np.isinf(val_loss):\n",
    "                val_loss = float('inf')\n",
    "        \n",
    "        # Record history\n",
    "        avg_train_loss = np.mean(epoch_losses['total']) if epoch_losses['total'] else float('inf')\n",
    "        avg_clean_loss = np.mean(epoch_losses['clean']) if epoch_losses['clean'] else 0.0\n",
    "        avg_adv_loss = np.mean(epoch_losses['adv']) if epoch_losses['adv'] else 0.0\n",
    "        \n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_clean_loss'].append(avg_clean_loss)\n",
    "        history['train_adv_loss'].append(avg_adv_loss)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if not (np.isnan(val_loss) or np.isinf(val_loss)):\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= config['patience']:\n",
    "                    print(f\"  {model_name} ({attack_type.upper()}, ε={epsilon}): Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"  {model_name} ({attack_type.upper()}, ε={epsilon}) - Epoch {epoch+1}/{config['epochs']}: \"\n",
    "                  f\"Train Loss={avg_train_loss:.6f}, Val Loss={val_loss:.6f}, \"\n",
    "                  f\"Robust Weight={current_robust_weight:.3f}\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        final_pred = model(X_val_tensor)\n",
    "        if isinstance(final_pred, tuple):\n",
    "            final_pred = final_pred[0]\n",
    "        final_pred = final_pred.squeeze().cpu().numpy()\n",
    "    \n",
    "    return model, final_pred, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1148cf2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T05:33:30.539172Z",
     "iopub.status.busy": "2026-01-23T05:33:30.538998Z",
     "iopub.status.idle": "2026-01-23T08:18:23.366050Z",
     "shell.execute_reply": "2026-01-23T08:18:23.365641Z"
    },
    "papermill": {
     "duration": 9892.855388,
     "end_time": "2026-01-23T08:18:23.384034",
     "exception": false,
     "start_time": "2026-01-23T05:33:30.528646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ADVERSARIAL TRAINING FOR TRANSFORMER MODELS\n",
      "================================================================================\n",
      "Training on attacks: ['a1', 'a2', 'a3', 'a4']\n",
      "Epsilons: [0.25, 0.5, 1.0]\n",
      "Robust weight: 0.3\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Training Single-Head with Adversarial Training\n",
      "================================================================================\n",
      "\n",
      "Training Single-Head (A1, ε=0.25)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A1, ε=0.25) - Epoch 10/100: Train Loss=0.000482, Val Loss=0.000271, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A1, ε=0.25) - Epoch 20/100: Train Loss=0.000468, Val Loss=0.000274, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A1, ε=0.25) - Epoch 30/100: Train Loss=0.000435, Val Loss=0.000274, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A1, ε=0.25): Early stopping at epoch 37\n",
      "  Single-Head (A1, ε=0.25) trained - RMSE: 0.016419, R²: 0.114956\n",
      "\n",
      "Training Single-Head (A1, ε=0.5)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A1, ε=0.5) - Epoch 10/100: Train Loss=0.000506, Val Loss=0.000279, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A1, ε=0.5) - Epoch 20/100: Train Loss=0.000455, Val Loss=0.000271, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A1, ε=0.5) - Epoch 30/100: Train Loss=0.000443, Val Loss=0.000269, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A1, ε=0.5) - Epoch 40/100: Train Loss=0.000432, Val Loss=0.000271, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A1, ε=0.5): Early stopping at epoch 49\n",
      "  Single-Head (A1, ε=0.5) trained - RMSE: 0.016371, R²: 0.120137\n",
      "\n",
      "Training Single-Head (A1, ε=1.0)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A1, ε=1.0) - Epoch 10/100: Train Loss=0.000514, Val Loss=0.000280, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A1, ε=1.0) - Epoch 20/100: Train Loss=0.000481, Val Loss=0.000279, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A1, ε=1.0) - Epoch 30/100: Train Loss=0.000454, Val Loss=0.000273, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A1, ε=1.0) - Epoch 40/100: Train Loss=0.000448, Val Loss=0.000272, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A1, ε=1.0) - Epoch 50/100: Train Loss=0.000443, Val Loss=0.000270, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A1, ε=1.0): Early stopping at epoch 51\n",
      "  Single-Head (A1, ε=1.0) trained - RMSE: 0.016425, R²: 0.114316\n",
      "\n",
      "Training Single-Head (A2, ε=0.25)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A2, ε=0.25) - Epoch 10/100: Train Loss=0.000497, Val Loss=0.000277, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A2, ε=0.25) - Epoch 20/100: Train Loss=0.000460, Val Loss=0.000271, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A2, ε=0.25) - Epoch 30/100: Train Loss=0.000425, Val Loss=0.000275, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A2, ε=0.25): Early stopping at epoch 34\n",
      "  Single-Head (A2, ε=0.25) trained - RMSE: 0.016517, R²: 0.104367\n",
      "\n",
      "Training Single-Head (A2, ε=0.5)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A2, ε=0.5) - Epoch 10/100: Train Loss=0.000523, Val Loss=0.000282, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A2, ε=0.5) - Epoch 20/100: Train Loss=0.000454, Val Loss=0.000274, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A2, ε=0.5) - Epoch 30/100: Train Loss=0.000437, Val Loss=0.000271, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A2, ε=0.5) - Epoch 40/100: Train Loss=0.000421, Val Loss=0.000273, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A2, ε=0.5) - Epoch 50/100: Train Loss=0.000408, Val Loss=0.000273, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A2, ε=0.5): Early stopping at epoch 52\n",
      "  Single-Head (A2, ε=0.5) trained - RMSE: 0.016529, R²: 0.103143\n",
      "\n",
      "Training Single-Head (A2, ε=1.0)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A2, ε=1.0) - Epoch 10/100: Train Loss=0.000506, Val Loss=0.000278, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A2, ε=1.0) - Epoch 20/100: Train Loss=0.000508, Val Loss=0.000270, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A2, ε=1.0) - Epoch 30/100: Train Loss=0.000452, Val Loss=0.000271, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A2, ε=1.0): Early stopping at epoch 38\n",
      "  Single-Head (A2, ε=1.0) trained - RMSE: 0.016395, R²: 0.117571\n",
      "\n",
      "Training Single-Head (A3, ε=0.25)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A3, ε=0.25) - Epoch 10/100: Train Loss=0.000527, Val Loss=0.000285, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A3, ε=0.25) - Epoch 20/100: Train Loss=0.000482, Val Loss=0.000273, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A3, ε=0.25) - Epoch 30/100: Train Loss=0.000452, Val Loss=0.000272, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A3, ε=0.25): Early stopping at epoch 36\n",
      "  Single-Head (A3, ε=0.25) trained - RMSE: 0.016467, R²: 0.109760\n",
      "\n",
      "Training Single-Head (A3, ε=0.5)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A3, ε=0.5) - Epoch 10/100: Train Loss=0.000501, Val Loss=0.000280, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A3, ε=0.5) - Epoch 20/100: Train Loss=0.000478, Val Loss=0.000273, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A3, ε=0.5) - Epoch 30/100: Train Loss=0.000464, Val Loss=0.000270, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A3, ε=0.5) - Epoch 40/100: Train Loss=0.000452, Val Loss=0.000268, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A3, ε=0.5) - Epoch 50/100: Train Loss=0.000445, Val Loss=0.000268, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A3, ε=0.5): Early stopping at epoch 59\n",
      "  Single-Head (A3, ε=0.5) trained - RMSE: 0.016412, R²: 0.115756\n",
      "\n",
      "Training Single-Head (A3, ε=1.0)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A3, ε=1.0) - Epoch 10/100: Train Loss=0.000539, Val Loss=0.000290, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A3, ε=1.0) - Epoch 20/100: Train Loss=0.000494, Val Loss=0.000273, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A3, ε=1.0) - Epoch 30/100: Train Loss=0.000481, Val Loss=0.000272, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A3, ε=1.0) - Epoch 40/100: Train Loss=0.000476, Val Loss=0.000268, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A3, ε=1.0) - Epoch 50/100: Train Loss=0.000462, Val Loss=0.000268, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A3, ε=1.0) - Epoch 60/100: Train Loss=0.000452, Val Loss=0.000268, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A3, ε=1.0) - Epoch 70/100: Train Loss=0.000447, Val Loss=0.000268, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A3, ε=1.0): Early stopping at epoch 71\n",
      "  Single-Head (A3, ε=1.0) trained - RMSE: 0.016399, R²: 0.117187\n",
      "\n",
      "Training Single-Head (A4, ε=0.25)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A4, ε=0.25) - Epoch 10/100: Train Loss=0.000501, Val Loss=0.000271, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A4, ε=0.25) - Epoch 20/100: Train Loss=0.000455, Val Loss=0.000276, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A4, ε=0.25): Early stopping at epoch 30\n",
      "  Single-Head (A4, ε=0.25) trained - RMSE: 0.016517, R²: 0.104377\n",
      "\n",
      "Training Single-Head (A4, ε=0.5)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A4, ε=0.5) - Epoch 10/100: Train Loss=0.000481, Val Loss=0.000279, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A4, ε=0.5) - Epoch 20/100: Train Loss=0.000503, Val Loss=0.000278, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A4, ε=0.5) - Epoch 30/100: Train Loss=0.000456, Val Loss=0.000272, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A4, ε=0.5) - Epoch 40/100: Train Loss=0.000438, Val Loss=0.000269, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A4, ε=0.5) - Epoch 50/100: Train Loss=0.000424, Val Loss=0.000272, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A4, ε=0.5): Early stopping at epoch 56\n",
      "  Single-Head (A4, ε=0.5) trained - RMSE: 0.016497, R²: 0.106508\n",
      "\n",
      "Training Single-Head (A4, ε=1.0)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A4, ε=1.0) - Epoch 10/100: Train Loss=0.000502, Val Loss=0.000283, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A4, ε=1.0) - Epoch 20/100: Train Loss=0.000476, Val Loss=0.000269, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A4, ε=1.0) - Epoch 30/100: Train Loss=0.000456, Val Loss=0.000267, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A4, ε=1.0) - Epoch 40/100: Train Loss=0.000450, Val Loss=0.000268, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A4, ε=1.0) - Epoch 50/100: Train Loss=0.000449, Val Loss=0.000268, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Single-Head (A4, ε=1.0): Early stopping at epoch 57\n",
      "  Single-Head (A4, ε=1.0) trained - RMSE: 0.016371, R²: 0.120107\n",
      "\n",
      "================================================================================\n",
      "Training Multi-Head with Adversarial Training\n",
      "================================================================================\n",
      "\n",
      "Training Multi-Head (A1, ε=0.25)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A1, ε=0.25) - Epoch 10/100: Train Loss=0.000449, Val Loss=0.000273, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A1, ε=0.25) - Epoch 20/100: Train Loss=0.000435, Val Loss=0.000271, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A1, ε=0.25) - Epoch 30/100: Train Loss=0.000422, Val Loss=0.000271, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A1, ε=0.25) - Epoch 40/100: Train Loss=0.000377, Val Loss=0.000268, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A1, ε=0.25) - Epoch 50/100: Train Loss=0.000358, Val Loss=0.000270, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A1, ε=0.25) - Epoch 60/100: Train Loss=0.000351, Val Loss=0.000269, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A1, ε=0.25): Early stopping at epoch 62\n",
      "  Multi-Head (A1, ε=0.25) trained - RMSE: 0.016400, R²: 0.117010\n",
      "\n",
      "Training Multi-Head (A1, ε=0.5)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A1, ε=0.5) - Epoch 10/100: Train Loss=0.000458, Val Loss=0.000273, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A1, ε=0.5) - Epoch 20/100: Train Loss=0.000431, Val Loss=0.000267, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A1, ε=0.5) - Epoch 30/100: Train Loss=0.000416, Val Loss=0.000270, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A1, ε=0.5) - Epoch 40/100: Train Loss=0.000394, Val Loss=0.000261, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A1, ε=0.5) - Epoch 50/100: Train Loss=0.000387, Val Loss=0.000261, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A1, ε=0.5) - Epoch 60/100: Train Loss=0.000381, Val Loss=0.000259, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A1, ε=0.5) - Epoch 70/100: Train Loss=0.000375, Val Loss=0.000259, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A1, ε=0.5) - Epoch 80/100: Train Loss=0.000375, Val Loss=0.000259, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A1, ε=0.5) - Epoch 90/100: Train Loss=0.000374, Val Loss=0.000259, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A1, ε=0.5) - Epoch 100/100: Train Loss=0.000370, Val Loss=0.000259, Robust Weight=0.300\n",
      "  Multi-Head (A1, ε=0.5) trained - RMSE: 0.016090, R²: 0.150109\n",
      "\n",
      "Training Multi-Head (A1, ε=1.0)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A1, ε=1.0) - Epoch 10/100: Train Loss=0.000485, Val Loss=0.000277, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A1, ε=1.0) - Epoch 20/100: Train Loss=0.000454, Val Loss=0.000272, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A1, ε=1.0) - Epoch 30/100: Train Loss=0.000447, Val Loss=0.000269, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A1, ε=1.0) - Epoch 40/100: Train Loss=0.000444, Val Loss=0.000265, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A1, ε=1.0) - Epoch 50/100: Train Loss=0.000425, Val Loss=0.000266, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A1, ε=1.0) - Epoch 60/100: Train Loss=0.000411, Val Loss=0.000265, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A1, ε=1.0) - Epoch 70/100: Train Loss=0.000401, Val Loss=0.000265, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A1, ε=1.0) - Epoch 80/100: Train Loss=0.000393, Val Loss=0.000261, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A1, ε=1.0) - Epoch 90/100: Train Loss=0.000389, Val Loss=0.000259, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A1, ε=1.0) - Epoch 100/100: Train Loss=0.000380, Val Loss=0.000259, Robust Weight=0.300\n",
      "  Multi-Head (A1, ε=1.0) trained - RMSE: 0.016101, R²: 0.148894\n",
      "\n",
      "Training Multi-Head (A2, ε=0.25)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A2, ε=0.25) - Epoch 10/100: Train Loss=0.000448, Val Loss=0.000274, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A2, ε=0.25) - Epoch 20/100: Train Loss=0.000429, Val Loss=0.000274, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A2, ε=0.25) - Epoch 30/100: Train Loss=0.000400, Val Loss=0.000272, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A2, ε=0.25) - Epoch 40/100: Train Loss=0.000361, Val Loss=0.000268, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A2, ε=0.25) - Epoch 50/100: Train Loss=0.000345, Val Loss=0.000268, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A2, ε=0.25): Early stopping at epoch 59\n",
      "  Multi-Head (A2, ε=0.25) trained - RMSE: 0.016418, R²: 0.115081\n",
      "\n",
      "Training Multi-Head (A2, ε=0.5)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A2, ε=0.5) - Epoch 10/100: Train Loss=0.000457, Val Loss=0.000277, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A2, ε=0.5) - Epoch 20/100: Train Loss=0.000416, Val Loss=0.000269, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A2, ε=0.5) - Epoch 30/100: Train Loss=0.000403, Val Loss=0.000270, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A2, ε=0.5) - Epoch 40/100: Train Loss=0.000379, Val Loss=0.000267, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A2, ε=0.5) - Epoch 50/100: Train Loss=0.000360, Val Loss=0.000270, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A2, ε=0.5): Early stopping at epoch 60\n",
      "  Multi-Head (A2, ε=0.5) trained - RMSE: 0.016466, R²: 0.109904\n",
      "\n",
      "Training Multi-Head (A2, ε=1.0)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A2, ε=1.0) - Epoch 10/100: Train Loss=0.000464, Val Loss=0.000278, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A2, ε=1.0) - Epoch 20/100: Train Loss=0.000428, Val Loss=0.000271, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A2, ε=1.0) - Epoch 30/100: Train Loss=0.000400, Val Loss=0.000268, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A2, ε=1.0) - Epoch 40/100: Train Loss=0.000383, Val Loss=0.000269, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A2, ε=1.0) - Epoch 50/100: Train Loss=0.000373, Val Loss=0.000270, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A2, ε=1.0): Early stopping at epoch 52\n",
      "  Multi-Head (A2, ε=1.0) trained - RMSE: 0.016434, R²: 0.113338\n",
      "\n",
      "Training Multi-Head (A3, ε=0.25)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A3, ε=0.25) - Epoch 10/100: Train Loss=0.000492, Val Loss=0.000281, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A3, ε=0.25) - Epoch 20/100: Train Loss=0.000441, Val Loss=0.000271, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A3, ε=0.25) - Epoch 30/100: Train Loss=0.000423, Val Loss=0.000269, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A3, ε=0.25) - Epoch 40/100: Train Loss=0.000406, Val Loss=0.000264, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A3, ε=0.25) - Epoch 50/100: Train Loss=0.000389, Val Loss=0.000264, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A3, ε=0.25) - Epoch 60/100: Train Loss=0.000380, Val Loss=0.000264, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A3, ε=0.25) - Epoch 70/100: Train Loss=0.000366, Val Loss=0.000262, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A3, ε=0.25) - Epoch 80/100: Train Loss=0.000351, Val Loss=0.000262, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A3, ε=0.25) - Epoch 90/100: Train Loss=0.000354, Val Loss=0.000262, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A3, ε=0.25): Early stopping at epoch 91\n",
      "  Multi-Head (A3, ε=0.25) trained - RMSE: 0.016188, R²: 0.139762\n",
      "\n",
      "Training Multi-Head (A3, ε=0.5)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A3, ε=0.5) - Epoch 10/100: Train Loss=0.000497, Val Loss=0.000278, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A3, ε=0.5) - Epoch 20/100: Train Loss=0.000455, Val Loss=0.000269, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A3, ε=0.5) - Epoch 30/100: Train Loss=0.000447, Val Loss=0.000268, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A3, ε=0.5) - Epoch 40/100: Train Loss=0.000435, Val Loss=0.000261, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A3, ε=0.5) - Epoch 50/100: Train Loss=0.000411, Val Loss=0.000262, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A3, ε=0.5) - Epoch 60/100: Train Loss=0.000402, Val Loss=0.000262, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A3, ε=0.5) - Epoch 70/100: Train Loss=0.000398, Val Loss=0.000261, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A3, ε=0.5) - Epoch 80/100: Train Loss=0.000396, Val Loss=0.000262, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A3, ε=0.5): Early stopping at epoch 88\n",
      "  Multi-Head (A3, ε=0.5) trained - RMSE: 0.016177, R²: 0.140912\n",
      "\n",
      "Training Multi-Head (A3, ε=1.0)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A3, ε=1.0) - Epoch 10/100: Train Loss=0.000504, Val Loss=0.000279, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A3, ε=1.0) - Epoch 20/100: Train Loss=0.000488, Val Loss=0.000266, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A3, ε=1.0) - Epoch 30/100: Train Loss=0.000460, Val Loss=0.000264, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A3, ε=1.0) - Epoch 40/100: Train Loss=0.000446, Val Loss=0.000262, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A3, ε=1.0) - Epoch 50/100: Train Loss=0.000431, Val Loss=0.000260, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A3, ε=1.0) - Epoch 60/100: Train Loss=0.000429, Val Loss=0.000258, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A3, ε=1.0) - Epoch 70/100: Train Loss=0.000420, Val Loss=0.000257, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A3, ε=1.0) - Epoch 80/100: Train Loss=0.000418, Val Loss=0.000256, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A3, ε=1.0) - Epoch 90/100: Train Loss=0.000416, Val Loss=0.000256, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A3, ε=1.0) - Epoch 100/100: Train Loss=0.000407, Val Loss=0.000255, Robust Weight=0.300\n",
      "  Multi-Head (A3, ε=1.0) trained - RMSE: 0.015966, R²: 0.163185\n",
      "\n",
      "Training Multi-Head (A4, ε=0.25)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A4, ε=0.25) - Epoch 10/100: Train Loss=0.000452, Val Loss=0.000275, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A4, ε=0.25) - Epoch 20/100: Train Loss=0.000431, Val Loss=0.000271, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A4, ε=0.25) - Epoch 30/100: Train Loss=0.000403, Val Loss=0.000270, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A4, ε=0.25) - Epoch 40/100: Train Loss=0.000378, Val Loss=0.000267, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A4, ε=0.25) - Epoch 50/100: Train Loss=0.000362, Val Loss=0.000269, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A4, ε=0.25): Early stopping at epoch 57\n",
      "  Multi-Head (A4, ε=0.25) trained - RMSE: 0.016384, R²: 0.118799\n",
      "\n",
      "Training Multi-Head (A4, ε=0.5)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A4, ε=0.5) - Epoch 10/100: Train Loss=0.000473, Val Loss=0.000278, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A4, ε=0.5) - Epoch 20/100: Train Loss=0.000436, Val Loss=0.000270, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A4, ε=0.5) - Epoch 30/100: Train Loss=0.000423, Val Loss=0.000268, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A4, ε=0.5) - Epoch 40/100: Train Loss=0.000397, Val Loss=0.000264, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A4, ε=0.5) - Epoch 50/100: Train Loss=0.000390, Val Loss=0.000264, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A4, ε=0.5) - Epoch 60/100: Train Loss=0.000379, Val Loss=0.000264, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A4, ε=0.5) - Epoch 70/100: Train Loss=0.000374, Val Loss=0.000263, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A4, ε=0.5) - Epoch 80/100: Train Loss=0.000366, Val Loss=0.000263, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A4, ε=0.5) - Epoch 90/100: Train Loss=0.000367, Val Loss=0.000262, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A4, ε=0.5): Early stopping at epoch 92\n",
      "  Multi-Head (A4, ε=0.5) trained - RMSE: 0.016197, R²: 0.138753\n",
      "\n",
      "Training Multi-Head (A4, ε=1.0)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A4, ε=1.0) - Epoch 10/100: Train Loss=0.000492, Val Loss=0.000277, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A4, ε=1.0) - Epoch 20/100: Train Loss=0.000471, Val Loss=0.000275, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A4, ε=1.0) - Epoch 30/100: Train Loss=0.000453, Val Loss=0.000272, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A4, ε=1.0) - Epoch 40/100: Train Loss=0.000439, Val Loss=0.000274, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A4, ε=1.0) - Epoch 50/100: Train Loss=0.000422, Val Loss=0.000265, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A4, ε=1.0) - Epoch 60/100: Train Loss=0.000411, Val Loss=0.000261, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A4, ε=1.0) - Epoch 70/100: Train Loss=0.000402, Val Loss=0.000262, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A4, ε=1.0) - Epoch 80/100: Train Loss=0.000401, Val Loss=0.000262, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head (A4, ε=1.0): Early stopping at epoch 82\n",
      "  Multi-Head (A4, ε=1.0) trained - RMSE: 0.016182, R²: 0.140344\n",
      "\n",
      "================================================================================\n",
      "Training Multi-Head Diversity with Adversarial Training\n",
      "================================================================================\n",
      "\n",
      "Training Multi-Head Diversity (A1, ε=0.25)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A1, ε=0.25) - Epoch 10/100: Train Loss=0.000484, Val Loss=0.000276, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A1, ε=0.25) - Epoch 20/100: Train Loss=0.000432, Val Loss=0.000273, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A1, ε=0.25) - Epoch 30/100: Train Loss=0.000409, Val Loss=0.000272, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A1, ε=0.25): Early stopping at epoch 35\n",
      "  Multi-Head Diversity (A1, ε=0.25) trained - RMSE: 0.016488, R²: 0.107536\n",
      "\n",
      "Training Multi-Head Diversity (A1, ε=0.5)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A1, ε=0.5) - Epoch 10/100: Train Loss=0.000477, Val Loss=0.000270, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A1, ε=0.5) - Epoch 20/100: Train Loss=0.000439, Val Loss=0.000271, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A1, ε=0.5) - Epoch 30/100: Train Loss=0.000423, Val Loss=0.000268, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A1, ε=0.5) - Epoch 40/100: Train Loss=0.000415, Val Loss=0.000267, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A1, ε=0.5) - Epoch 50/100: Train Loss=0.000411, Val Loss=0.000268, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A1, ε=0.5): Early stopping at epoch 60\n",
      "  Multi-Head Diversity (A1, ε=0.5) trained - RMSE: 0.016355, R²: 0.121840\n",
      "\n",
      "Training Multi-Head Diversity (A1, ε=1.0)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A1, ε=1.0) - Epoch 10/100: Train Loss=0.000496, Val Loss=0.000280, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A1, ε=1.0) - Epoch 20/100: Train Loss=0.000472, Val Loss=0.000272, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A1, ε=1.0) - Epoch 30/100: Train Loss=0.000456, Val Loss=0.000272, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A1, ε=1.0) - Epoch 40/100: Train Loss=0.000443, Val Loss=0.000271, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A1, ε=1.0) - Epoch 50/100: Train Loss=0.000442, Val Loss=0.000270, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A1, ε=1.0): Early stopping at epoch 58\n",
      "  Multi-Head Diversity (A1, ε=1.0) trained - RMSE: 0.016432, R²: 0.113603\n",
      "\n",
      "Training Multi-Head Diversity (A2, ε=0.25)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A2, ε=0.25) - Epoch 10/100: Train Loss=0.000448, Val Loss=0.000273, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A2, ε=0.25) - Epoch 20/100: Train Loss=0.000418, Val Loss=0.000274, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A2, ε=0.25): Early stopping at epoch 22\n",
      "  Multi-Head Diversity (A2, ε=0.25) trained - RMSE: 0.016589, R²: 0.096617\n",
      "\n",
      "Training Multi-Head Diversity (A2, ε=0.5)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A2, ε=0.5) - Epoch 10/100: Train Loss=0.000466, Val Loss=0.000271, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A2, ε=0.5) - Epoch 20/100: Train Loss=0.000436, Val Loss=0.000275, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A2, ε=0.5): Early stopping at epoch 30\n",
      "  Multi-Head Diversity (A2, ε=0.5) trained - RMSE: 0.016561, R²: 0.099664\n",
      "\n",
      "Training Multi-Head Diversity (A2, ε=1.0)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A2, ε=1.0) - Epoch 10/100: Train Loss=0.000455, Val Loss=0.000272, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A2, ε=1.0) - Epoch 20/100: Train Loss=0.000429, Val Loss=0.000275, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A2, ε=1.0): Early stopping at epoch 22\n",
      "  Multi-Head Diversity (A2, ε=1.0) trained - RMSE: 0.016555, R²: 0.100240\n",
      "\n",
      "Training Multi-Head Diversity (A3, ε=0.25)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A3, ε=0.25) - Epoch 10/100: Train Loss=0.000463, Val Loss=0.000270, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A3, ε=0.25) - Epoch 20/100: Train Loss=0.000440, Val Loss=0.000268, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A3, ε=0.25) - Epoch 30/100: Train Loss=0.000425, Val Loss=0.000269, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A3, ε=0.25): Early stopping at epoch 40\n",
      "  Multi-Head Diversity (A3, ε=0.25) trained - RMSE: 0.016447, R²: 0.111984\n",
      "\n",
      "Training Multi-Head Diversity (A3, ε=0.5)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A3, ε=0.5) - Epoch 10/100: Train Loss=0.000496, Val Loss=0.000275, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A3, ε=0.5) - Epoch 20/100: Train Loss=0.000477, Val Loss=0.000269, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A3, ε=0.5) - Epoch 30/100: Train Loss=0.000478, Val Loss=0.000268, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A3, ε=0.5) - Epoch 40/100: Train Loss=0.000444, Val Loss=0.000270, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A3, ε=0.5): Early stopping at epoch 47\n",
      "  Multi-Head Diversity (A3, ε=0.5) trained - RMSE: 0.016417, R²: 0.115171\n",
      "\n",
      "Training Multi-Head Diversity (A3, ε=1.0)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A3, ε=1.0) - Epoch 10/100: Train Loss=0.000521, Val Loss=0.000275, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A3, ε=1.0) - Epoch 20/100: Train Loss=0.000495, Val Loss=0.000273, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A3, ε=1.0) - Epoch 30/100: Train Loss=0.000469, Val Loss=0.000276, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A3, ε=1.0) - Epoch 40/100: Train Loss=0.000458, Val Loss=0.000268, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A3, ε=1.0) - Epoch 50/100: Train Loss=0.000446, Val Loss=0.000270, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A3, ε=1.0): Early stopping at epoch 53\n",
      "  Multi-Head Diversity (A3, ε=1.0) trained - RMSE: 0.016380, R²: 0.119191\n",
      "\n",
      "Training Multi-Head Diversity (A4, ε=0.25)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A4, ε=0.25) - Epoch 10/100: Train Loss=0.000460, Val Loss=0.000274, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A4, ε=0.25) - Epoch 20/100: Train Loss=0.000437, Val Loss=0.000269, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A4, ε=0.25) - Epoch 30/100: Train Loss=0.000411, Val Loss=0.000272, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A4, ε=0.25): Early stopping at epoch 34\n",
      "  Multi-Head Diversity (A4, ε=0.25) trained - RMSE: 0.016448, R²: 0.111832\n",
      "\n",
      "Training Multi-Head Diversity (A4, ε=0.5)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A4, ε=0.5) - Epoch 10/100: Train Loss=0.000485, Val Loss=0.000268, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A4, ε=0.5) - Epoch 20/100: Train Loss=0.000447, Val Loss=0.000268, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A4, ε=0.5) - Epoch 30/100: Train Loss=0.000428, Val Loss=0.000268, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A4, ε=0.5) - Epoch 40/100: Train Loss=0.000417, Val Loss=0.000266, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A4, ε=0.5) - Epoch 50/100: Train Loss=0.000407, Val Loss=0.000267, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A4, ε=0.5): Early stopping at epoch 59\n",
      "  Multi-Head Diversity (A4, ε=0.5) trained - RMSE: 0.016321, R²: 0.125523\n",
      "\n",
      "Training Multi-Head Diversity (A4, ε=1.0)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A4, ε=1.0) - Epoch 10/100: Train Loss=0.000504, Val Loss=0.000279, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A4, ε=1.0) - Epoch 20/100: Train Loss=0.000463, Val Loss=0.000270, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A4, ε=1.0) - Epoch 30/100: Train Loss=0.000452, Val Loss=0.000270, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A4, ε=1.0) - Epoch 40/100: Train Loss=0.000439, Val Loss=0.000268, Robust Weight=0.300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multi-Head Diversity (A4, ε=1.0): Early stopping at epoch 47\n",
      "  Multi-Head Diversity (A4, ε=1.0) trained - RMSE: 0.016388, R²: 0.118332\n",
      "\n",
      "================================================================================\n",
      "ADVERSARIAL TRAINING COMPLETE\n",
      "================================================================================\n",
      "Total adversarially trained models: 36\n"
     ]
    }
   ],
   "source": [
    "# Train adversarially trained models\n",
    "print(\"=\" * 80)\n",
    "print(\"ADVERSARIAL TRAINING FOR TRANSFORMER MODELS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Training on attacks: {ADVERSARIAL_CONFIG['attacks']}\")\n",
    "print(f\"Epsilons: {ADVERSARIAL_CONFIG['epsilons']}\")\n",
    "print(f\"Robust weight: {ADVERSARIAL_CONFIG['robust_weight']}\")\n",
    "print()\n",
    "\n",
    "# Models to train adversarially\n",
    "transformer_model_names = ['Single-Head', 'Multi-Head', 'Multi-Head Diversity']\n",
    "base_models = {\n",
    "    'Single-Head': models['Single-Head'],\n",
    "    'Multi-Head': models['Multi-Head'],\n",
    "    'Multi-Head Diversity': models['Multi-Head Diversity']\n",
    "}\n",
    "\n",
    "# Train each model with each attack at each epsilon\n",
    "for model_name in transformer_model_names:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training {model_name} with Adversarial Training\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    base_model = base_models[model_name]\n",
    "    \n",
    "    for attack_type in ADVERSARIAL_CONFIG['attacks']:\n",
    "        for epsilon in ADVERSARIAL_CONFIG['epsilons']:\n",
    "            model_key = f\"{model_name} ({attack_type.upper()}, ε={epsilon})\"\n",
    "            print(f\"\\nTraining {model_key}...\")\n",
    "            \n",
    "            try:\n",
    "                adv_model, adv_pred, adv_history = train_adversarial_model(\n",
    "                    base_model, model_name, X_train_scaled, y_train, \n",
    "                    X_val_scaled, y_val, attack_type, epsilon, \n",
    "                    ADVERSARIAL_CONFIG, device\n",
    "                )\n",
    "                \n",
    "                # Evaluate\n",
    "                adv_rmse = np.sqrt(mean_squared_error(y_val, adv_pred))\n",
    "                adv_r2 = r2_score(y_val, adv_pred)\n",
    "                \n",
    "                adversarial_models[model_key] = adv_model\n",
    "                adversarial_training_history[model_key] = {\n",
    "                    'rmse': adv_rmse,\n",
    "                    'r2': adv_r2,\n",
    "                    'history': adv_history\n",
    "                }\n",
    "                \n",
    "                print(f\"  {model_key} trained - RMSE: {adv_rmse:.6f}, R²: {adv_r2:.6f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\" Error training {model_key}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ADVERSARIAL TRAINING COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total adversarially trained models: {len(adversarial_models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0537ffde",
   "metadata": {
    "papermill": {
     "duration": 0.014057,
     "end_time": "2026-01-23T08:18:23.412802",
     "exception": false,
     "start_time": "2026-01-23T08:18:23.398745",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Evaluate Existing Models Under Adversarial Attacks\n",
    "\n",
    "Evaluate already-trained models (standard and adversarially trained) under A1-A4 attacks to generate robustness results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10661.980152,
   "end_time": "2026-01-23T08:19:09.986181",
   "environment_variables": {},
   "exception": null,
   "input_path": "notebooks/Final_Models_Demo_6heads.ipynb",
   "output_path": "Final_Models_Demo_6heads_executed.ipynb",
   "parameters": {},
   "start_time": "2026-01-23T05:21:28.006029",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
