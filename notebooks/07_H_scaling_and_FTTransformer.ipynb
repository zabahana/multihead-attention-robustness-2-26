{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Head Diversity: H Scaling & FT-Transformer Baseline\n",
        "\n",
        "Experiments:\n",
        "1. **H scaling** (clean only): Multi-Head Diversity for H ∈ {1, 2, 4, 8, 16}\n",
        "2. **FT-Transformer baseline**: Modern tabular transformer for comparison\n",
        "\n",
        "Uses subset of data when `USE_SMALL_DATA=True` for faster runs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "USE_SMALL_DATA = False  # True = subset for quick runs; False = full data (matches paper)\n",
        "N_SAMPLES = 5000       # Max observations when USE_SMALL_DATA\n",
        "N_EPOCHS = 100         # Epochs when USE_SMALL_DATA=False (use 30 for small-data runs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Mount Google Drive first (required when running in Colab)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    print('Google Drive mounted.')\n",
        "except Exception:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "def _find_repo_root():\n",
        "    cwd = Path.cwd().resolve()\n",
        "    # Colab: explicit paths (run Drive mount cell first)\n",
        "    candidates = [\n",
        "        Path('/content/drive/MyDrive/multihead-attention-robustness'),\n",
        "        Path('/content/drive/My Drive/multihead-attention-robustness'),\n",
        "        Path('/content/repo_run'),\n",
        "    ]\n",
        "    for p in candidates:\n",
        "        if p.exists() and (p / 'src').exists():\n",
        "            return p\n",
        "    # Colab: search under Drive for any folder containing 'multihead-attention' with src\n",
        "    drive_root = Path('/content/drive')\n",
        "    if drive_root.exists():\n",
        "        for base in [drive_root / 'MyDrive', drive_root / 'My Drive', drive_root]:\n",
        "            if base.exists():\n",
        "                for sub in base.iterdir():\n",
        "                    if sub.is_dir() and 'multihead-attention' in sub.name.lower() and (sub / 'src').exists():\n",
        "                        return sub\n",
        "    # Local: walk up from cwd\n",
        "    p = cwd\n",
        "    for _ in range(10):\n",
        "        if (p / 'src').exists():\n",
        "            return p\n",
        "        if p.parent == p:\n",
        "            break\n",
        "        p = p.parent\n",
        "    # Fallback: if we're in notebooks/, parent is repo root\n",
        "    if cwd.name == 'notebooks' and (cwd.parent / 'src').exists():\n",
        "        return cwd.parent\n",
        "    return cwd.parent if cwd.name == 'notebooks' else cwd\n",
        "\n",
        "repo_root = _find_repo_root()\n",
        "if not (repo_root / 'src').exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Could not find repo root with 'src' folder. repo_root={repo_root}\\n\"\n",
        "        \"In Colab: run the Drive mount cell above first, then ensure your project folder \"\n",
        "        \"(multihead-attention-robustness) is in My Drive.\"\n",
        "    )\n",
        "sys.path.insert(0, str(repo_root))\n",
        "os.chdir(repo_root)  # ensure cwd is repo root for relative paths\n",
        "from src.models.feature_token_transformer import FeatureTokenTransformer\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device: {device}, Repo: {repo_root}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data_path = repo_root / 'data' / 'cross_sectional' / 'master_table.csv'\n",
        "if not data_path.exists():\n",
        "    data_path = repo_root / 'data' / 'master_table.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "if 'date' in df.columns:\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df = df.set_index('date')\n",
        "\n",
        "class CrossSectionalDataSplitter:\n",
        "    def __init__(self, train_start='2005-01-01', train_end='2017-12-31', val_start='2018-01-01', val_end='2019-12-31'):\n",
        "        self.train_start, self.train_end = train_start, train_end\n",
        "        self.val_start, self.val_end = val_start, val_end\n",
        "    def split(self, master_table):\n",
        "        master_table = master_table.copy()\n",
        "        master_table.index = pd.to_datetime(master_table.index)\n",
        "        return {'train': master_table.loc[self.train_start:self.train_end], 'val': master_table.loc[self.val_start:self.val_end]}\n",
        "    def prepare_features_labels(self, data):\n",
        "        if data.empty:\n",
        "            return pd.DataFrame(), pd.Series()\n",
        "        numeric_data = data.select_dtypes(include=[np.number])\n",
        "        if numeric_data.empty:\n",
        "            return pd.DataFrame(), pd.Series()\n",
        "        exclude_cols = ['mktcap', 'market_cap', 'date', 'year', 'month', 'ticker', 'permno', 'gvkey']\n",
        "        target_cols = ['return', 'returns', 'ret', 'target', 'y', 'next_return', 'forward_return', 'ret_1', 'ret_1m', 'ret_12m', 'future_return', 'returns_1d']\n",
        "        target_col = None\n",
        "        for tc in target_cols:\n",
        "            for col in numeric_data.columns:\n",
        "                if tc.lower() in col.lower() and col.lower() not in [ec.lower() for ec in exclude_cols]:\n",
        "                    target_col = col\n",
        "                    break\n",
        "            if target_col:\n",
        "                break\n",
        "        if target_col is None:\n",
        "            potential = [c for c in numeric_data.columns if c.lower() not in [ec.lower() for ec in exclude_cols]]\n",
        "            target_col = potential[-2] if len(potential) > 1 else (potential[-1] if potential else numeric_data.columns[-1])\n",
        "        feature_cols = [c for c in numeric_data.columns if c != target_col and c.lower() not in [ec.lower() for ec in exclude_cols]]\n",
        "        if not feature_cols:\n",
        "            feature_cols = [c for c in numeric_data.columns if c != target_col]\n",
        "        if not feature_cols:\n",
        "            feature_cols = numeric_data.columns[:-1].tolist()\n",
        "            target_col = numeric_data.columns[-1]\n",
        "        return numeric_data[feature_cols], numeric_data[target_col]\n",
        "\n",
        "splitter = CrossSectionalDataSplitter()\n",
        "data_splits = splitter.split(df)\n",
        "train_df, val_df = data_splits['train'], data_splits['val']\n",
        "X_train_df, y_train = splitter.prepare_features_labels(train_df)\n",
        "X_val_df, y_val = splitter.prepare_features_labels(val_df)\n",
        "X_train = X_train_df.fillna(0).values.astype(np.float32)\n",
        "y_train = y_train.fillna(0).values.astype(np.float32)\n",
        "X_val = X_val_df.fillna(0).values.astype(np.float32)\n",
        "y_val = y_val.fillna(0).values.astype(np.float32)\n",
        "\n",
        "if USE_SMALL_DATA and N_SAMPLES < len(X_train):\n",
        "    idx = np.random.RandomState(RANDOM_SEED).choice(len(X_train), N_SAMPLES, replace=False)\n",
        "    X_train, y_train = X_train[idx], y_train[idx]\n",
        "    print(f'Using subset: {N_SAMPLES} train samples')\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "print(f'Data: train {X_train_scaled.shape[0]}, val {X_val_scaled.shape[0]}, features {X_train_scaled.shape[1]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Training Function (with Diversity Loss)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train_mhd_transformer(model, model_name, X_train, y_train, X_val, y_val, config, device='cpu'):\n",
        "    \"\"\"Train Multi-Head Diversity transformer (includes diversity loss).\"\"\"\n",
        "    model = model.to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "    \n",
        "    X_train_t = torch.FloatTensor(X_train).to(device)\n",
        "    y_train_t = torch.FloatTensor(y_train).to(device)\n",
        "    X_val_t = torch.FloatTensor(X_val).to(device)\n",
        "    y_val_t = torch.FloatTensor(y_val).to(device)\n",
        "    \n",
        "    num_features = model.num_features\n",
        "    if X_train.shape[1] != num_features:\n",
        "        X_train_t = torch.FloatTensor(X_train[:, :num_features]).to(device)\n",
        "        X_val_t = torch.FloatTensor(X_val[:, :num_features]).to(device)\n",
        "    \n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    train_losses, val_losses = [], []\n",
        "    batch_size = config['batch_size']\n",
        "    n_batches = (len(X_train_t) + batch_size - 1) // batch_size\n",
        "    epochs = config['epochs']\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        for i in range(0, len(X_train_t), batch_size):\n",
        "            batch_X = X_train_t[i:i+batch_size]\n",
        "            batch_y = y_train_t[i:i+batch_size]\n",
        "            optimizer.zero_grad()\n",
        "            pred, attn_dict = model(batch_X)\n",
        "            mse_loss = criterion(pred.squeeze(), batch_y)\n",
        "            loss = mse_loss\n",
        "            if model.use_head_diversity and attn_dict:\n",
        "                attn_list = [attn_dict[f'layer_{j}'] for j in range(len(attn_dict))]\n",
        "                div_loss = model.compute_diversity_loss(attn_list)\n",
        "                loss = mse_loss + div_loss\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        epoch_loss /= n_batches\n",
        "        \n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_pred, _ = model(X_val_t)\n",
        "            val_loss = criterion(val_pred.squeeze(), y_val_t)\n",
        "        \n",
        "        train_losses.append(epoch_loss)\n",
        "        val_losses.append(val_loss.item())\n",
        "        scheduler.step(val_loss)\n",
        "        \n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= config['patience']:\n",
        "                print(f'  {model_name}: Early stop at epoch {epoch+1}')\n",
        "                break\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'  {model_name} Epoch {epoch+1}/{epochs}: train={epoch_loss:.6f}, val={val_loss.item():.6f}')\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        final_pred, _ = model(X_val_t)\n",
        "        final_pred = final_pred.squeeze().cpu().numpy()\n",
        "    return model, final_pred, train_losses, val_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. H Scaling Experiment (H ∈ {1, 2, 4, 8, 16})"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "H_VALUES = [1, 2, 4, 8, 12]  # 16 omitted: 72 not divisible by 16\n",
        "D_MODEL = 72  # Must be divisible by all H (matches TRAINING_CONFIG)\n",
        "assert all(D_MODEL % h == 0 for h in H_VALUES), 'd_model must be divisible by all H'\n",
        "\n",
        "tr_config = {\n",
        "    'd_model': D_MODEL,\n",
        "    'num_layers': 2,\n",
        "    'd_ff': 512,\n",
        "    'dropout': 0.1,\n",
        "    'learning_rate': 0.0001,\n",
        "    'batch_size': 32,\n",
        "    'epochs': N_EPOCHS if USE_SMALL_DATA else 100,\n",
        "    'patience': 20\n",
        "}\n",
        "\n",
        "h_scaling_results = []\n",
        "for h in H_VALUES:\n",
        "    print(f'\\n{\"=\"*60}\\nTraining Multi-Head Diversity with H={h}\\n{\"=\"*60}')\n",
        "    model = FeatureTokenTransformer(\n",
        "        num_features=X_train_scaled.shape[1],\n",
        "        d_model=D_MODEL,\n",
        "        num_heads=h,\n",
        "        num_layers=tr_config['num_layers'],\n",
        "        d_ff=tr_config['d_ff'],\n",
        "        dropout=tr_config['dropout'],\n",
        "        use_head_diversity=True,\n",
        "        diversity_weight=0.01\n",
        "    )\n",
        "    model, pred, tl, vl = train_mhd_transformer(\n",
        "        model, f'MHD-H{h}', X_train_scaled, y_train, X_val_scaled, y_val, tr_config, device\n",
        "    )\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
        "    r2 = r2_score(y_val, pred)\n",
        "    h_scaling_results.append({'H': h, 'RMSE': rmse, 'R2': r2})\n",
        "    print(f'  H={h}: RMSE={rmse:.6f}, R²={r2:.6f}')\n",
        "\n",
        "h_df = pd.DataFrame(h_scaling_results)\n",
        "print('\\nH Scaling Summary:')\n",
        "print(h_df.to_string(index=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. FT-Transformer Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "try:\n",
        "    from pytorch_tabular import TabularModel\n",
        "    from pytorch_tabular.models import FTTransformerConfig\n",
        "    from pytorch_tabular.config import DataConfig, OptimizerConfig, TrainerConfig\n",
        "    HAS_PYTORCH_TABULAR = True\n",
        "except ImportError:\n",
        "    HAS_PYTORCH_TABULAR = False\n",
        "    print('pytorch-tabular not installed. Run: pip install pytorch-tabular')\n",
        "\n",
        "if HAS_PYTORCH_TABULAR:\n",
        "    feature_names = [f'f{i}' for i in range(X_train_scaled.shape[1])]\n",
        "    train_df_pt = pd.DataFrame(X_train_scaled, columns=feature_names)\n",
        "    train_df_pt['target'] = y_train\n",
        "    val_df_pt = pd.DataFrame(X_val_scaled, columns=feature_names)\n",
        "    val_df_pt['target'] = y_val\n",
        "    \n",
        "    data_config = DataConfig(\n",
        "        target=['target'],\n",
        "        continuous_cols=feature_names,\n",
        "        categorical_cols=[],\n",
        "        normalize_continuous_features=False,  # already scaled\n",
        "    )\n",
        "    trainer_config = TrainerConfig(\n",
        "        batch_size=32,\n",
        "        max_epochs=N_EPOCHS if USE_SMALL_DATA else 100,\n",
        "        early_stopping='valid_loss',\n",
        "        early_stopping_patience=20,\n",
        "        accelerator='auto',\n",
        "        devices=1 if torch.cuda.is_available() else 0,\n",
        "    )\n",
        "    optimizer_config = OptimizerConfig()\n",
        "    \n",
        "    model_config = FTTransformerConfig(\n",
        "        task='regression',\n",
        "        learning_rate=1e-4,\n",
        "        num_heads=8,\n",
        "        num_attn_blocks=2,\n",
        "        transformer_activation='GEGLU',  # GEGLU is default; 'gelu' causes KeyError in pytorch-tabular\n",
        "        embedding_dropout=0.1,\n",
        "        attn_dropout=0.1,\n",
        "    )\n",
        "    \n",
        "    ft_model = TabularModel(\n",
        "        data_config=data_config,\n",
        "        model_config=model_config,\n",
        "        optimizer_config=optimizer_config,\n",
        "        trainer_config=trainer_config,\n",
        "    )\n",
        "    \n",
        "    print('Training FT-Transformer...')\n",
        "    ft_model.fit(train=train_df_pt, validation=val_df_pt)\n",
        "    pred_ft = ft_model.predict(val_df_pt)\n",
        "    pred_ft = pred_ft['prediction'].values if 'prediction' in pred_ft.columns else pred_ft.iloc[:, -1].values\n",
        "    ft_rmse = np.sqrt(mean_squared_error(y_val, pred_ft))\n",
        "    ft_r2 = r2_score(y_val, pred_ft)\n",
        "    print(f'FT-Transformer: RMSE={ft_rmse:.6f}, R²={ft_r2:.6f}')\n",
        "else:\n",
        "    ft_rmse, ft_r2 = np.nan, np.nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary & Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "axes[0].plot(h_df['H'], h_df['R2'], 'o-', label='MHD R²')\n",
        "axes[0].set_xlabel('Number of Heads (H)')\n",
        "axes[0].set_ylabel('R²')\n",
        "axes[0].set_title('H Scaling: Multi-Head Diversity (Clean)')\n",
        "axes[0].set_xticks(H_VALUES)\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(h_df['H'], h_df['RMSE'], 's-', color='orange', label='MHD RMSE')\n",
        "axes[1].set_xlabel('Number of Heads (H)')\n",
        "axes[1].set_ylabel('RMSE')\n",
        "axes[1].set_title('H Scaling: RMSE')\n",
        "axes[1].set_xticks(H_VALUES)\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('FINAL SUMMARY')\n",
        "print('='*60)\n",
        "print(h_df.to_string(index=False))\n",
        "if HAS_PYTORCH_TABULAR:\n",
        "    print(f'\\nFT-Transformer: RMSE={ft_rmse:.6f}, R²={ft_r2:.6f}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}