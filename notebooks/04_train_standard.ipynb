{"cells":[{"cell_type":"code","execution_count":null,"id":"c3ebde29","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c3ebde29","executionInfo":{"status":"ok","timestamp":1770399765732,"user_tz":300,"elapsed":26027,"user":{"displayName":"Zelalem Abahana","userId":"10078571423914522929"}},"outputId":"f1514af2-89dd-47b5-d1d4-e97fd2907fe8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import sys\n","from pathlib import Path\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import random\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error, r2_score\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","warnings.filterwarnings('ignore')\n","# Repo root for src imports\n","try:\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=False)\n","except Exception:\n","    pass\n","def _find_repo_root():\n","    cwd = Path.cwd().resolve()\n","    for p in [Path('/content/drive/MyDrive/multihead-attention-robustness'),\n","              Path('/content/drive/My Drive/multihead-attention-robustness'),\n","              Path('/content/repo_run')]:\n","        if (p / 'src').exists():\n","            return p\n","    drive_root = Path('/content/drive')\n","    if drive_root.exists():\n","        for base in [drive_root / 'MyDrive', drive_root / 'My Drive', drive_root]:\n","            p = base / 'multihead-attention-robustness'\n","            if p.exists() and (p / 'src').exists():\n","                return p\n","    p = cwd\n","    for _ in range(10):\n","        if (p / 'src').exists():\n","            return p\n","        if p.parent == p:\n","            break\n","        p = p.parent\n","    return cwd.parent if cwd.name == 'notebooks' else cwd\n","repo_root = _find_repo_root()\n","sys.path.insert(0, str(repo_root))\n","from src.models.feature_token_transformer import FeatureTokenTransformer, SingleHeadTransformer\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","RANDOM_SEED = 42\n","random.seed(RANDOM_SEED)\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","# Preserve models/training_history from prior notebooks (02, 03) when run in pipeline\n","if 'models' not in globals() or not isinstance(globals().get('models'), dict) or len(globals().get('models', {})) == 0:\n","    models = {}\n","if 'training_history' not in globals() or not isinstance(globals().get('training_history'), dict) or len(globals().get('training_history', {})) == 0:\n","    training_history = {}\n","TRAINING_CONFIG = {\n","    'ols': {}, 'ridge': {'alpha': 1.0},\n","    'mlp': {'hidden_dims': [128, 64], 'learning_rate': 0.001, 'batch_size': 64, 'epochs': 100, 'patience': 10},\n","    'transformer': {'d_model': 72, 'num_heads': 8, 'num_layers': 2, 'd_ff': 512, 'dropout': 0.1,\n","                   'learning_rate': 0.0001, 'batch_size': 32, 'epochs': 100, 'patience': 20}\n","}\n"]},{"cell_type":"code","execution_count":null,"id":"72155dba","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"72155dba","executionInfo":{"status":"ok","timestamp":1770399768860,"user_tz":300,"elapsed":3126,"user":{"displayName":"Zelalem Abahana","userId":"10078571423914522929"}},"outputId":"a50fdece-6084-414b-8966-2659e9595d45"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded fresh data: train 18826, val 3408\n"]}],"source":["# Load fresh data from master_table.csv (standalone: each notebook pulls its own data)\n","data_path = repo_root / 'data' / 'cross_sectional' / 'master_table.csv'\n","df = pd.read_csv(data_path)\n","if 'date' in df.columns:\n","    df['date'] = pd.to_datetime(df['date'])\n","    df = df.set_index('date')\n","class CrossSectionalDataSplitter:\n","    def __init__(self, train_start='2005-01-01', train_end='2017-12-31', val_start='2018-01-01', val_end='2019-12-31'):\n","        self.train_start, self.train_end = train_start, train_end\n","        self.val_start, self.val_end = val_start, val_end\n","    def split(self, master_table):\n","        master_table = master_table.copy()\n","        master_table.index = pd.to_datetime(master_table.index)\n","        return {'train': master_table.loc[self.train_start:self.train_end], 'val': master_table.loc[self.val_start:self.val_end]}\n","    def prepare_features_labels(self, data):\n","        if data.empty:\n","            return pd.DataFrame(), pd.Series()\n","        numeric_data = data.select_dtypes(include=[np.number])\n","        if numeric_data.empty:\n","            return pd.DataFrame(), pd.Series()\n","        exclude_cols = ['mktcap', 'market_cap', 'date', 'year', 'month', 'ticker', 'permno', 'gvkey']\n","        target_cols = ['return', 'returns', 'ret', 'target', 'y', 'next_return', 'forward_return', 'ret_1', 'ret_1m', 'ret_12m', 'future_return', 'returns_1d']\n","        target_col = None\n","        for tc in target_cols:\n","            for col in numeric_data.columns:\n","                if tc.lower() in col.lower() and col.lower() not in [ec.lower() for ec in exclude_cols]:\n","                    target_col = col\n","                    break\n","            if target_col:\n","                break\n","        if target_col is None:\n","            potential = [c for c in numeric_data.columns if c.lower() not in [ec.lower() for ec in exclude_cols]]\n","            target_col = potential[-2] if len(potential) > 1 else (potential[-1] if potential else numeric_data.columns[-1])\n","        feature_cols = [c for c in numeric_data.columns if c != target_col and c.lower() not in [ec.lower() for ec in exclude_cols]]\n","        if not feature_cols:\n","            feature_cols = [c for c in numeric_data.columns if c != target_col]\n","        if not feature_cols:\n","            feature_cols = numeric_data.columns[:-1].tolist()\n","            target_col = numeric_data.columns[-1]\n","        return numeric_data[feature_cols], numeric_data[target_col]\n","splitter = CrossSectionalDataSplitter()\n","data_splits = splitter.split(df)\n","train_df, val_df = data_splits['train'], data_splits['val']\n","X_train_df, y_train = splitter.prepare_features_labels(train_df)\n","X_val_df, y_val = splitter.prepare_features_labels(val_df)\n","X_train = X_train_df.fillna(0).values.astype(np.float32)\n","y_train = y_train.fillna(0).values.astype(np.float32)\n","X_val = X_val_df.fillna(0).values.astype(np.float32)\n","y_val = y_val.fillna(0).values.astype(np.float32)\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_val_scaled = scaler.transform(X_val)\n","print(f'Loaded fresh data: train {X_train_scaled.shape[0]}, val {X_val_scaled.shape[0]}')\n"]},{"cell_type":"code","source":["\"\"\"\n","Train baseline models when models is empty (standalone notebook execution).\n","Run with: %run -i train_baseline_if_needed.py\n","Expects in namespace: X_train_scaled, y_train, X_val_scaled, y_val, device, TRAINING_CONFIG,\n","  RANDOM_SEED, FeatureTokenTransformer, SingleHeadTransformer, nn, torch, np,\n","  mean_squared_error, r2_score.\n","Populates: models, training_history.\n","\"\"\"\n","from sklearn.linear_model import LinearRegression, Ridge\n","\n","def train_baseline_models():\n","    global models, training_history\n","    models = {}\n","    training_history = {}\n","    _device = globals().get('device', 'cpu')\n","    _cfg = globals().get('TRAINING_CONFIG', {})\n","    _seed = globals().get('RANDOM_SEED', 42)\n","    tr_cfg = _cfg.get('transformer', {'d_model': 72, 'num_heads': 8, 'num_layers': 2, 'd_ff': 512,\n","        'dropout': 0.1, 'learning_rate': 0.0001, 'batch_size': 32, 'epochs': 100, 'patience': 20})\n","\n","    def _train_transformer(model, X_train, y_train, X_val, y_val):\n","        model = model.to(_device)\n","        criterion = nn.MSELoss()\n","        optimizer = torch.optim.Adam(model.parameters(), lr=tr_cfg['learning_rate'])\n","        X_t = torch.FloatTensor(X_train).to(_device)\n","        y_t = torch.FloatTensor(y_train).to(_device)\n","        X_v = torch.FloatTensor(X_val).to(_device)\n","        y_v = torch.FloatTensor(y_val).to(_device)\n","        nf = model.num_features if hasattr(model, 'num_features') else getattr(model.model, 'num_features', X_train.shape[1])\n","        if X_train.shape[1] != nf:\n","            if X_train.shape[1] < nf:\n","                pad_t = np.zeros((X_train.shape[0], nf - X_train.shape[1]))\n","                pad_v = np.zeros((X_val.shape[0], nf - X_val.shape[1]))\n","                X_t = torch.FloatTensor(np.hstack([X_train, pad_t])).to(_device)\n","                X_v = torch.FloatTensor(np.hstack([X_val, pad_v])).to(_device)\n","            else:\n","                X_t = torch.FloatTensor(X_train[:, :nf]).to(_device)\n","                X_v = torch.FloatTensor(X_val[:, :nf]).to(_device)\n","        bs = tr_cfg['batch_size']\n","        best = float('inf')\n","        pc = 0\n","        for ep in range(tr_cfg['epochs']):\n","            model.train()\n","            for i in range(0, len(X_t), bs):\n","                optimizer.zero_grad()\n","                out = model(X_t[i:i+bs])\n","                out = out[0] if isinstance(out, tuple) else out\n","                criterion(out.squeeze(), y_t[i:i+bs]).backward()\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","                optimizer.step()\n","            model.eval()\n","            with torch.no_grad():\n","                vp = model(X_v)\n","                vp = vp[0] if isinstance(vp, tuple) else vp\n","                vl = criterion(vp.squeeze(), y_v).item()\n","            if vl < best:\n","                best, pc = vl, 0\n","            else:\n","                pc += 1\n","                if pc >= tr_cfg['patience']:\n","                    break\n","        model.eval()\n","        with torch.no_grad():\n","            pred = model(X_v)\n","            pred = (pred[0] if isinstance(pred, tuple) else pred).squeeze().cpu().numpy()\n","        return pred\n","\n","    # OLS\n","    m = LinearRegression()\n","    m.fit(X_train_scaled, y_train)\n","    p = m.predict(X_val_scaled)\n","    models['OLS'] = m\n","    training_history['OLS'] = {'rmse': np.sqrt(mean_squared_error(y_val, p)), 'r2': r2_score(y_val, p)}\n","\n","    # Ridge\n","    m = Ridge(alpha=_cfg.get('ridge', {}).get('alpha', 1.0), random_state=_seed)\n","    m.fit(X_train_scaled, y_train)\n","    p = m.predict(X_val_scaled)\n","    models['Ridge'] = m\n","    training_history['Ridge'] = {'rmse': np.sqrt(mean_squared_error(y_val, p)), 'r2': r2_score(y_val, p)}\n","\n","    # XGBoost\n","    try:\n","        import xgboost as xgb\n","        m = xgb.XGBRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, subsample=0.8,\n","            colsample_bytree=0.8, random_state=_seed, objective='reg:squarederror', eval_metric='rmse')\n","        m.fit(X_train_scaled, y_train, eval_set=[(X_val_scaled, y_val)], verbose=False)\n","        p = m.predict(X_val_scaled)\n","        models['XGBoost'] = m\n","        training_history['XGBoost'] = {'rmse': np.sqrt(mean_squared_error(y_val, p)), 'r2': r2_score(y_val, p)}\n","    except ImportError:\n","        pass\n","\n","    # MLP\n","    class MLP(nn.Module):\n","        def __init__(self):\n","            super().__init__()\n","            h = _cfg.get('mlp', {}).get('hidden_dims', [128, 64])\n","            layers = []\n","            prev = X_train_scaled.shape[1]\n","            for d in h:\n","                layers += [nn.Linear(prev, d), nn.ReLU(), nn.Dropout(0.1)]\n","                prev = d\n","            layers.append(nn.Linear(prev, 1))\n","            self.net = nn.Sequential(*layers)\n","        def forward(self, x):\n","            return self.net(x).squeeze(-1)\n","    m = MLP().to(_device)\n","    opt = torch.optim.Adam(m.parameters(), lr=_cfg.get('mlp', {}).get('learning_rate', 0.001))\n","    Xt = torch.FloatTensor(X_train_scaled).to(_device)\n","    yt = torch.FloatTensor(y_train).to(_device)\n","    Xv = torch.FloatTensor(X_val_scaled).to(_device)\n","    for _ in range(min(_cfg.get('mlp', {}).get('epochs', 100), 50)):\n","        m.train()\n","        opt.zero_grad()\n","        torch.nn.functional.mse_loss(m(Xt), yt).backward()\n","        opt.step()\n","    m.eval()\n","    with torch.no_grad():\n","        p = m(Xv).cpu().numpy()\n","    models['MLP'] = m\n","    training_history['MLP'] = {'rmse': np.sqrt(mean_squared_error(y_val, p)), 'r2': r2_score(y_val, p)}\n","\n","    # Transformers\n","    nf = X_train_scaled.shape[1]\n","    for name, cls, kw in [\n","        ('Single-Head', SingleHeadTransformer, {'num_features': nf, 'd_model': tr_cfg['d_model'], 'num_layers': tr_cfg['num_layers']}),\n","        ('Multi-Head', FeatureTokenTransformer, {'num_features': nf, 'd_model': tr_cfg['d_model'], 'num_heads': tr_cfg['num_heads'],\n","            'num_layers': tr_cfg['num_layers'], 'd_ff': tr_cfg['d_ff'], 'dropout': tr_cfg['dropout'], 'use_head_diversity': False}),\n","        ('Multi-Head Diversity', FeatureTokenTransformer, {'num_features': nf, 'd_model': tr_cfg['d_model'], 'num_heads': tr_cfg['num_heads'],\n","            'num_layers': tr_cfg['num_layers'], 'd_ff': tr_cfg['d_ff'], 'dropout': tr_cfg['dropout'],\n","            'use_head_diversity': True, 'diversity_weight': 0.01}),\n","    ]:\n","        mdl = cls(**kw)\n","        pred = _train_transformer(mdl, X_train_scaled, y_train, X_val_scaled, y_val)\n","        models[name] = mdl\n","        training_history[name] = {'rmse': np.sqrt(mean_squared_error(y_val, pred)), 'r2': r2_score(y_val, pred)}\n","\n","    print(f\"✓ Trained {len(models)} baseline models (standalone mode)\")\n","\n","if __name__ == '__main__':\n","    g = globals()\n","    if 'models' not in g or not isinstance(g.get('models'), dict) or len(g.get('models', {})) == 0:\n","        train_baseline_models()\n","    else:\n","        print(\"✓ Baseline models already loaded, skipping training\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"44w8t_yvDOYg","executionInfo":{"status":"ok","timestamp":1770405933726,"user_tz":300,"elapsed":6144124,"user":{"displayName":"Zelalem Abahana","userId":"10078571423914522929"}},"outputId":"3902f4e7-3ba6-4965-a829-620f248a4346"},"id":"44w8t_yvDOYg","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✓ Trained 7 baseline models (standalone mode)\n"]}]},{"cell_type":"markdown","id":"8f81e6c4","metadata":{"id":"8f81e6c4"},"source":["## Standard Training Summary\n","\n","This notebook summarizes standard (clean) model performance and exports results to CSV.\n","Adversarial training is done in notebook 03."]},{"cell_type":"code","execution_count":null,"id":"6948855d","metadata":{"execution":{"iopub.execute_input":"2026-01-23T08:18:23.449160Z","iopub.status.busy":"2026-01-23T08:18:23.448955Z","iopub.status.idle":"2026-01-23T08:18:55.897546Z","shell.execute_reply":"2026-01-23T08:18:55.897152Z"},"papermill":{"duration":32.4719,"end_time":"2026-01-23T08:18:55.898546","exception":false,"start_time":"2026-01-23T08:18:23.426646","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"6948855d","executionInfo":{"status":"ok","timestamp":1770406244934,"user_tz":300,"elapsed":311208,"user":{"displayName":"Zelalem Abahana","userId":"10078571423914522929"}},"outputId":"4fec2b9b-b2e3-4161-fe81-4201d5fb04d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","EVALUATING EXISTING MODELS UNDER ADVERSARIAL ATTACKS\n","================================================================================\n","\n","Available models:\n","  Standard models: ['OLS', 'Ridge', 'XGBoost', 'MLP', 'Single-Head', 'Multi-Head', 'Multi-Head Diversity']\n","  ⚠ No adversarially trained models found\n","\n","Validation set: 3408 samples, 22 features\n","Using device: cpu\n","\n","Evaluating standard models...\n","  OLS...\n","  Ridge...\n","  XGBoost...\n","  MLP...\n","  Single-Head...\n","  Multi-Head...\n","  Multi-Head Diversity...\n","\n","================================================================================\n","EVALUATION COMPLETE\n","================================================================================\n","\n","✓ Generated 84 robustness evaluations\n","✓ Created robustness_df with shape: (84, 8)\n","\n","Summary by model:\n","  OLS: Average Robustness = 0.8115\n","  Ridge: Average Robustness = 0.8508\n","  XGBoost: Average Robustness = 0.8717\n","  MLP: Average Robustness = 0.7476\n","  Single-Head: Average Robustness = 0.9288\n","  Multi-Head: Average Robustness = 0.9016\n","  Multi-Head Diversity: Average Robustness = 0.8848\n"]}],"source":["# Evaluate existing models under adversarial attacks\n","# This generates robustness_results and robustness_df without retraining\n","\n","print(\"=\" * 80)\n","print(\"EVALUATING EXISTING MODELS UNDER ADVERSARIAL ATTACKS\")\n","print(\"=\" * 80)\n","\n","# Define attack epsilons and types\n","ATTACK_EPSILONS = [0.25, 0.5, 1.0]\n","ATTACK_TYPES = ['a1', 'a2', 'a3', 'a4']\n","\n","# Attack functions (reuse from adversarial training section)\n","def apply_a1_attack(X, epsilon=0.01):\n","    \"\"\"A1: Measurement Error - bounded perturbations.\"\"\"\n","    noise = np.random.normal(0, epsilon, X.shape)\n","    return X + noise\n","\n","def apply_a2_attack(X, missing_rate=0.1):\n","    \"\"\"A2: Missingness/Staleness - set random features to zero.\"\"\"\n","    X_adv = X.copy()\n","    n_samples, n_features = X.shape\n","    n_missing = max(1, int(n_features * missing_rate))\n","    for i in range(n_samples):\n","        missing_indices = np.random.choice(n_features, n_missing, replace=False)\n","        X_adv[i, missing_indices] = 0.0\n","    return X_adv\n","\n","def apply_a3_attack(X, epsilon=0.01):\n","    \"\"\"A3: Rank Manipulation - cross-sectional perturbation preserving ranks.\"\"\"\n","    X_adv = X.copy()\n","    n_samples = X.shape[0]\n","    for i in range(n_samples):\n","        perturbation = np.random.normal(0, epsilon, X.shape[1])\n","        X_adv[i] = X[i] + perturbation\n","    return X_adv\n","\n","def apply_a4_attack(X, epsilon=1.0):\n","    \"\"\"A4: Regime Shift - distribution shift attack.\"\"\"\n","    X_adv = X.copy()\n","    feature_std = np.std(X, axis=0, keepdims=True) + 1e-8\n","    noise = np.random.normal(0, epsilon, X.shape) * feature_std\n","    X_adv = X + noise\n","    return X_adv\n","\n","# Function to evaluate a model under attack\n","def evaluate_model_under_attack(model, model_name, X_val, y_val, attack_type, epsilon,\n","                                device='cpu', is_sklearn=False, num_runs=5):\n","    \"\"\"Evaluate a model under a specific attack.\"\"\"\n","    # Set model to eval mode\n","    if not is_sklearn:\n","        model.eval()\n","        for module in model.modules():\n","            if isinstance(module, nn.Dropout):\n","                module.eval()\n","\n","    # Make clean predictions\n","    if is_sklearn:\n","        y_pred_clean = model.predict(X_val)\n","    else:\n","        with torch.no_grad():\n","            X_tensor = torch.FloatTensor(X_val).to(device)\n","            output = model(X_tensor)\n","            # Handle tuple returns (some models return (predictions, attention_weights))\n","            if isinstance(output, tuple):\n","                y_pred_tensor = output[0]\n","            else:\n","                y_pred_tensor = output\n","            y_pred_clean = y_pred_tensor.cpu().numpy().flatten()\n","\n","    # Calculate clean RMSE\n","    clean_rmse = np.sqrt(mean_squared_error(y_val, y_pred_clean))\n","\n","    # Run attack multiple times and average\n","    adv_rmses = []\n","    for run in range(num_runs):\n","        # Apply attack\n","        if attack_type == 'a1':\n","            X_adv = apply_a1_attack(X_val, epsilon=epsilon)\n","        elif attack_type == 'a2':\n","            # Convert epsilon to missing rate\n","            missing_rate = min(epsilon / 10.0, 0.8)\n","            X_adv = apply_a2_attack(X_val, missing_rate=missing_rate)\n","        elif attack_type == 'a3':\n","            X_adv = apply_a3_attack(X_val, epsilon=epsilon)\n","        elif attack_type == 'a4':\n","            X_adv = apply_a4_attack(X_val, epsilon=epsilon)\n","        else:\n","            X_adv = X_val.copy()\n","\n","        # Make adversarial predictions\n","        if is_sklearn:\n","            y_pred_adv = model.predict(X_adv)\n","        else:\n","            with torch.no_grad():\n","                X_adv_tensor = torch.FloatTensor(X_adv).to(device)\n","                output_adv = model(X_adv_tensor)\n","                # Handle tuple returns (some models return (predictions, attention_weights))\n","                if isinstance(output_adv, tuple):\n","                    y_pred_adv_tensor = output_adv[0]\n","                else:\n","                    y_pred_adv_tensor = output_adv\n","                y_pred_adv = y_pred_adv_tensor.cpu().numpy().flatten()\n","\n","        # Calculate adversarial RMSE\n","        adv_rmse = np.sqrt(mean_squared_error(y_val, y_pred_adv))\n","        adv_rmses.append(adv_rmse)\n","\n","    # Average across runs\n","    avg_adv_rmse = np.mean(adv_rmses)\n","    delta_rmse = avg_adv_rmse - clean_rmse\n","\n","    # Calculate robustness: min(1.0, 1 - (ΔRMSE / RMSE_clean))\n","    if clean_rmse > 0:\n","        robustness = min(1.0, 1.0 - (delta_rmse / clean_rmse))\n","    else:\n","        robustness = 1.0\n","\n","    return {\n","        'clean_rmse': clean_rmse,\n","        'adv_rmse': avg_adv_rmse,\n","        'delta_rmse': delta_rmse,\n","        'robustness': robustness\n","    }\n","\n","# Initialize results (always set so downstream CI/summary cells never fail)\n","robustness_results = []\n","robustness_df = pd.DataFrame()\n","\n","# Check what models are available\n","print(\"\\nAvailable models:\")\n","if 'models' in locals():\n","    print(f\"  Standard models: {list(models.keys())}\")\n","else:\n","    print(\"  ⚠ No standard models found\")\n","    models = {}\n","\n","if 'adversarial_models' in locals():\n","    print(f\"  Adversarially trained models: {len(adversarial_models)} models\")\n","else:\n","    print(\"  ⚠ No adversarially trained models found\")\n","    adversarial_models = {}\n","\n","# Ensure we have validation data\n","if 'X_val_scaled' not in locals() or 'y_val' not in locals():\n","    print(\"\\n⚠ X_val_scaled or y_val not found. Please run data loading and splitting cells first.\")\n","    robustness_df = pd.DataFrame()\n","else:\n","    print(f\"\\nValidation set: {len(X_val_scaled)} samples, {X_val_scaled.shape[1]} features\")\n","\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    print(f\"Using device: {device}\\n\")\n","\n","    # Evaluate standard models\n","    print(\"Evaluating standard models...\")\n","    for model_name, model in models.items():\n","        print(f\"  {model_name}...\")\n","        is_sklearn = model_name in ['OLS', 'Ridge', 'XGBoost']\n","\n","        for attack_type in ATTACK_TYPES:\n","            for epsilon in ATTACK_EPSILONS:\n","                try:\n","                    result = evaluate_model_under_attack(\n","                        model, model_name, X_val_scaled, y_val.values if hasattr(y_val, 'values') else y_val,\n","                        attack_type, epsilon, device=device, is_sklearn=is_sklearn, num_runs=5\n","                    )\n","                    robustness_results.append({\n","                        'model_name': model_name,\n","                        'attack_type': attack_type,\n","                        'epsilon': epsilon,\n","                        'clean_rmse': result['clean_rmse'],\n","                        'adv_rmse': result['adv_rmse'],\n","                        'delta_rmse': result['delta_rmse'],\n","                        'robustness': result['robustness'],\n","                        'training_type': 'standard'\n","                    })\n","                except Exception as e:\n","                    print(f\"    ⚠ Error evaluating {model_name} under {attack_type} (ε={epsilon}): {e}\")\n","\n","    # Evaluate adversarially trained models\n","    if len(adversarial_models) > 0:\n","        print(\"\\nEvaluating adversarially trained models...\")\n","        for model_key, adv_model in adversarial_models.items():\n","            # Extract base model name and attack info from key\n","            # Format: \"Multi-Head Diversity (A1, ε=0.25)\"\n","            base_model = model_key.split('(')[0].strip()\n","            print(f\"  {model_key}...\")\n","\n","            for attack_type in ATTACK_TYPES:\n","                for epsilon in ATTACK_EPSILONS:\n","                    try:\n","                        result = evaluate_model_under_attack(\n","                            adv_model, model_key, X_val_scaled, y_val.values if hasattr(y_val, 'values') else y_val,\n","                            attack_type, epsilon, device=device, is_sklearn=False, num_runs=5\n","                        )\n","                        robustness_results.append({\n","                            'model_name': model_key,\n","                            'attack_type': attack_type,\n","                            'epsilon': epsilon,\n","                            'clean_rmse': result['clean_rmse'],\n","                            'adv_rmse': result['adv_rmse'],\n","                            'delta_rmse': result['delta_rmse'],\n","                            'robustness': result['robustness'],\n","                            'training_type': 'adversarial'\n","                        })\n","                    except Exception as e:\n","                        print(f\"    ⚠ Error evaluating {model_key} under {attack_type} (ε={epsilon}): {e}\")\n","\n","    # Create DataFrame\n","    if len(robustness_results) > 0:\n","        robustness_df = pd.DataFrame(robustness_results)\n","        print(\"\\n\" + \"=\" * 80)\n","        print(\"EVALUATION COMPLETE\")\n","        print(\"=\" * 80)\n","        print(f\"\\n✓ Generated {len(robustness_results)} robustness evaluations\")\n","        print(f\"✓ Created robustness_df with shape: {robustness_df.shape}\")\n","        print(f\"\\nSummary by model:\")\n","        for model_name in robustness_df['model_name'].unique():\n","            model_data = robustness_df[robustness_df['model_name'] == model_name]\n","            avg_robustness = model_data['robustness'].mean()\n","            print(f\"  {model_name}: Average Robustness = {avg_robustness:.4f}\")\n","    else:\n","        print(\"\\n⚠ No robustness results generated. Check that models are available.\")\n","        robustness_df = pd.DataFrame()"]},{"cell_type":"code","execution_count":null,"id":"5fb93a7b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5fb93a7b","executionInfo":{"status":"ok","timestamp":1770406244956,"user_tz":300,"elapsed":25,"user":{"displayName":"Zelalem Abahana","userId":"10078571423914522929"}},"outputId":"f16be712-2b05-450b-e3f9-f68e9b2baced"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","ROBUSTNESS RESULTS (standard + adversarially trained)\n","================================================================================\n","          model_name attack_type  epsilon  clean_rmse  adv_rmse  delta_rmse  robustness training_type\n","                 OLS          a1     0.25    0.017520  0.018411    0.000891    0.949139      standard\n","                 OLS          a1     0.50    0.017520  0.020571    0.003051    0.825859      standard\n","                 OLS          a1     1.00    0.017520  0.028058    0.010538    0.398483      standard\n","                 OLS          a2     0.25    0.017520  0.017858    0.000338    0.980703      standard\n","                 OLS          a2     0.50    0.017520  0.017807    0.000287    0.983601      standard\n","                 OLS          a2     1.00    0.017520  0.018252    0.000732    0.958226      standard\n","                 OLS          a3     0.25    0.017520  0.018314    0.000795    0.954648      standard\n","                 OLS          a3     0.50    0.017520  0.020570    0.003050    0.825925      standard\n","                 OLS          a3     1.00    0.017520  0.028235    0.010715    0.388416      standard\n","                 OLS          a4     0.25    0.017520  0.017998    0.000479    0.972687      standard\n","                 OLS          a4     0.50    0.017520  0.019495    0.001975    0.887244      standard\n","                 OLS          a4     1.00    0.017520  0.024298    0.006778    0.613136      standard\n","               Ridge          a1     0.25    0.017519  0.018274    0.000755    0.956924      standard\n","               Ridge          a1     0.50    0.017519  0.019878    0.002359    0.865340      standard\n","               Ridge          a1     1.00    0.017519  0.025875    0.008356    0.523028      standard\n","               Ridge          a2     0.25    0.017519  0.017824    0.000305    0.982616      standard\n","               Ridge          a2     0.50    0.017519  0.017747    0.000228    0.987013      standard\n","               Ridge          a2     1.00    0.017519  0.018023    0.000504    0.971212      standard\n","               Ridge          a3     0.25    0.017519  0.018205    0.000686    0.960864      standard\n","               Ridge          a3     0.50    0.017519  0.019947    0.002427    0.861439      standard\n","               Ridge          a3     1.00    0.017519  0.025965    0.008446    0.517923      standard\n","               Ridge          a4     0.25    0.017519  0.017923    0.000404    0.976927      standard\n","               Ridge          a4     0.50    0.017519  0.018962    0.001443    0.917636      standard\n","               Ridge          a4     1.00    0.017519  0.022976    0.005457    0.688536      standard\n","             XGBoost          a1     0.25    0.016591  0.017077    0.000486    0.970702      standard\n","             XGBoost          a1     0.50    0.016591  0.018522    0.001931    0.883603      standard\n","             XGBoost          a1     1.00    0.016591  0.023437    0.006845    0.587410      standard\n","             XGBoost          a2     0.25    0.016591  0.016768    0.000177    0.989356      standard\n","             XGBoost          a2     0.50    0.016591  0.016779    0.000188    0.988662      standard\n","             XGBoost          a2     1.00    0.016591  0.016862    0.000271    0.983657      standard\n","             XGBoost          a3     0.25    0.016591  0.017006    0.000415    0.975003      standard\n","             XGBoost          a3     0.50    0.016591  0.018633    0.002041    0.876970      standard\n","             XGBoost          a3     1.00    0.016591  0.023119    0.006528    0.606568      standard\n","             XGBoost          a4     0.25    0.016591  0.017038    0.000446    0.973089      standard\n","             XGBoost          a4     0.50    0.016591  0.017987    0.001395    0.915899      standard\n","             XGBoost          a4     1.00    0.016591  0.021419    0.004828    0.709007      standard\n","                 MLP          a1     0.25    0.021139  0.022390    0.001251    0.940832      standard\n","                 MLP          a1     0.50    0.021139  0.025775    0.004636    0.780664      standard\n","                 MLP          a1     1.00    0.021139  0.037349    0.016211    0.233133      standard\n","                 MLP          a2     0.25    0.021139  0.021545    0.000406    0.980770      standard\n","                 MLP          a2     0.50    0.021139  0.021499    0.000360    0.982953      standard\n","                 MLP          a2     1.00    0.021139  0.021759    0.000620    0.970674      standard\n","                 MLP          a3     0.25    0.021139  0.022425    0.001286    0.939152      standard\n","                 MLP          a3     0.50    0.021139  0.025990    0.004852    0.770484      standard\n","                 MLP          a3     1.00    0.021139  0.037455    0.016317    0.228119      standard\n","                 MLP          a4     0.25    0.021139  0.022131    0.000992    0.953055      standard\n","                 MLP          a4     0.50    0.021139  0.024819    0.003680    0.825889      standard\n","                 MLP          a4     1.00    0.021139  0.034555    0.013416    0.365315      standard\n","         Single-Head          a1     0.25    0.016711  0.016854    0.000143    0.991431      standard\n","         Single-Head          a1     0.50    0.016711  0.017518    0.000807    0.951690      standard\n","         Single-Head          a1     1.00    0.016711  0.020868    0.004157    0.751255      standard\n","         Single-Head          a2     0.25    0.016711  0.016883    0.000172    0.989685      standard\n","         Single-Head          a2     0.50    0.016711  0.016781    0.000070    0.995817      standard\n","         Single-Head          a2     1.00    0.016711  0.016946    0.000235    0.985945      standard\n","         Single-Head          a3     0.25    0.016711  0.016924    0.000213    0.987278      standard\n","         Single-Head          a3     0.50    0.016711  0.017276    0.000565    0.966193      standard\n","         Single-Head          a3     1.00    0.016711  0.021412    0.004701    0.718669      standard\n","         Single-Head          a4     0.25    0.016711  0.016813    0.000102    0.993891      standard\n","         Single-Head          a4     0.50    0.016711  0.017257    0.000546    0.967317      standard\n","         Single-Head          a4     1.00    0.016711  0.019286    0.002575    0.845889      standard\n","          Multi-Head          a1     0.25    0.016503  0.016804    0.000301    0.981737      standard\n","          Multi-Head          a1     0.50    0.016503  0.017660    0.001157    0.929899      standard\n","          Multi-Head          a1     1.00    0.016503  0.022270    0.005768    0.650510      standard\n","          Multi-Head          a2     0.25    0.016503  0.016595    0.000092    0.994417      standard\n","          Multi-Head          a2     0.50    0.016503  0.016576    0.000074    0.995539      standard\n","          Multi-Head          a2     1.00    0.016503  0.016791    0.000288    0.982543      standard\n","          Multi-Head          a3     0.25    0.016503  0.016699    0.000196    0.988121      standard\n","          Multi-Head          a3     0.50    0.016503  0.017743    0.001240    0.924834      standard\n","          Multi-Head          a3     1.00    0.016503  0.022370    0.005867    0.644481      standard\n","          Multi-Head          a4     0.25    0.016503  0.016680    0.000177    0.989252      standard\n","          Multi-Head          a4     0.50    0.016503  0.017160    0.000657    0.960161      standard\n","          Multi-Head          a4     1.00    0.016503  0.020181    0.003678    0.777137      standard\n","Multi-Head Diversity          a1     0.25    0.016396  0.016722    0.000326    0.980118      standard\n","Multi-Head Diversity          a1     0.50    0.016396  0.017574    0.001178    0.928143      standard\n","Multi-Head Diversity          a1     1.00    0.016396  0.023276    0.006880    0.580369      standard\n","Multi-Head Diversity          a2     0.25    0.016396  0.016494    0.000099    0.993983      standard\n","Multi-Head Diversity          a2     0.50    0.016396  0.016508    0.000112    0.993143      standard\n","Multi-Head Diversity          a2     1.00    0.016396  0.016653    0.000257    0.984297      standard\n","Multi-Head Diversity          a3     0.25    0.016396  0.016763    0.000368    0.977582      standard\n","Multi-Head Diversity          a3     0.50    0.016396  0.017688    0.001292    0.921170      standard\n","Multi-Head Diversity          a3     1.00    0.016396  0.023217    0.006821    0.583965      standard\n","Multi-Head Diversity          a4     0.25    0.016396  0.016549    0.000153    0.990668      standard\n","Multi-Head Diversity          a4     0.50    0.016396  0.017397    0.001002    0.938907      standard\n","Multi-Head Diversity          a4     1.00    0.016396  0.020571    0.004176    0.745319      standard\n","\n","✓ Exported to /content/drive/MyDrive/multihead-attention-robustness/outputs/robustness_results.csv\n"]}],"source":["# Print results (run this cell only to view results without re-running evaluation)\n","if 'robustness_df' in globals() and len(robustness_df) > 0:\n","    print(\"=\" * 80)\n","    print(\"ROBUSTNESS RESULTS (standard + adversarially trained)\")\n","    print(\"=\" * 80)\n","    print(robustness_df.to_string(index=False))\n","    # Export to CSV\n","    output_dir = repo_root / 'outputs'\n","    output_dir.mkdir(parents=True, exist_ok=True)\n","    csv_path = output_dir / 'robustness_results.csv'\n","    robustness_df.to_csv(csv_path, index=False)\n","    print(f\"\\n✓ Exported to {csv_path}\")\n","else:\n","    print(\"⚠ robustness_df not found or empty. Run the evaluation cell above first.\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"papermill":{"default_parameters":{},"duration":10661.980152,"end_time":"2026-01-23T08:19:09.986181","environment_variables":{},"exception":null,"input_path":"notebooks/Final_Models_Demo_6heads.ipynb","output_path":"Final_Models_Demo_6heads_executed.ipynb","parameters":{},"start_time":"2026-01-23T05:21:28.006029","version":"2.6.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}